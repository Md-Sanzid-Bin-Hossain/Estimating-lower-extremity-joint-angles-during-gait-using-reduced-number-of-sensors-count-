# -*- coding: utf-8 -*-
"""Conference_Subject _1_Estimation_Gait_ensemble_Net_subject_1_REAL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BZ4xt65NOT9faH4atNYUDq67ZuodiPFe
"""

# Let`s import all packages that we may need:
import numpy
import tensorflow as tf
import statistics 
from numpy import loadtxt
import matplotlib.pyplot as plt
import pandas
import math
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import GRU,LSTM
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from statistics import stdev 
import math

 
import numpy as np
from scipy.signal import butter,filtfilt
 
import sys 
import numpy as np # linear algebra
from scipy.stats import randint
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL
import matplotlib.pyplot as plt # this is used for the plot the graph 
import seaborn as sns # used for plot interactive graph. 
import pandas
import matplotlib.pyplot as plt
 
## for Deep-learing:
import tensorflow.keras
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
to_categorical([0, 1, 2, 3], num_classes=4)
from tensorflow.keras.optimizers import SGD 
from tensorflow.keras.callbacks import EarlyStopping
# from tensorflow.keras.utils import np_utils
import itertools
from tensorflow.keras.layers import LSTM
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.layers import Dropout
from keras.layers import TimeDistributed
from keras.layers import Flatten
from keras.layers import Bidirectional
#import constraint
 
from sklearn.model_selection import train_test_split
from keras.regularizers import l2
 
 
###  Library for attention layers 
 
import pandas as pd
#import pyarrow.parquet as pq # Used to read the data
import os 
import numpy as np
from tensorflow.keras.layers import * # Keras is the most friendly Neural Network library, this Kernel use a lot of layers classes
from tensorflow.keras.models import Model
from tqdm import tqdm # Processing time measurement
from sklearn.model_selection import train_test_split 
from tensorflow.keras import backend as K # The backend give us access to tensorflow operations and allow us to create the Attention class
from tensorflow.keras import optimizers # Allow us to access the Adam class to modify some parameters
from sklearn.model_selection import GridSearchCV, StratifiedKFold # Used to use Kfold to train our model
from tensorflow.keras.callbacks import * # This object helps the model to train in a smarter way, avoiding overfitting
 
from tensorflow.keras.layers import Layer
import tensorflow.keras.backend as K
from tensorflow.keras import initializers
from tensorflow.keras import regularizers
import statistics
import gc
 
### Early stopping 
 
from tensorflow.keras.callbacks import EarlyStopping
 

####### Subject 1 #######
 
 
Features_1= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_treadmill_fast_features.csv', delimiter=',')
Features_2= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_treadmill_normal_features.csv', delimiter=',')
Features_3= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_treadmill_slow_1_features.csv', delimiter=',')
Features_4= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_treadmill_slow_2_features.csv', delimiter=',')
Features_5= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_treadmill_vfast_features.csv', delimiter=',')

Features_6= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_overground_fast_features.csv', delimiter=',')
Features_7= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_overground_normal_features.csv', delimiter=',')
Features_8= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_overground_slow_features.csv', delimiter=',')
Features_9= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_overground_vfast_features.csv', delimiter=',')

Features_10= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_slope_1_features_ascent.csv', delimiter=',')
Features_11= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_slope_1_features_decent.csv', delimiter=',')
Features_12= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_slope_2_features_ascent.csv', delimiter=',')
Features_13= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_slope_2_features_decent.csv', delimiter=',')
Features_14= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_stair_1_features_ascent.csv', delimiter=',')
Features_15= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_stair_1_features_decent.csv', delimiter=',')
Features_16= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_stair_2_features_ascent.csv', delimiter=',')
Features_17= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_stair_2_features_decent.csv', delimiter=',')




## Output ### 
 
Output_1= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_treadmill_fast_v6.csv', delimiter=',')
Output_2= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_treadmill_normal_v6.csv', delimiter=',')
Output_3= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_treadmill_slow_1_v6.csv', delimiter=',')
Output_4= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_treadmill_slow_2_v6.csv', delimiter=',')
Output_5= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_treadmill_vfast_v6.csv', delimiter=',')

Output_6= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_overground_fast.csv', delimiter=',')
Output_7= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_overground_normal.csv', delimiter=',')
Output_8= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_overground_slow.csv', delimiter=',')
Output_9= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_overground_vfast.csv', delimiter=',')

Output_10= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_slope_1_ascent.csv', delimiter=',')
Output_11= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_slope_1_decent.csv', delimiter=',')
Output_12= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_slope_2_ascent.csv', delimiter=',')
Output_13= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_slope_2_decent.csv', delimiter=',')
Output_14= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_stair_1_ascent.csv', delimiter=',')
Output_15= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_stair_1_decent.csv', delimiter=',')
Output_16= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_stair_2_ascent.csv', delimiter=',')
Output_17= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T001_stair_2_decent.csv', delimiter=',')
 

#### Slope and stair ###

Features_18=pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T001_slope_1_features.csv', header=None)
Features_19= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T001_slope_2_features.csv', header=None)
Features_20= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T001_stair_1_features.csv', header=None)
Features_21=pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T001_stair_2_features.csv', header=None)


Output_18= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T001_slope_1.csv',skiprows = 1,header = None)
Output_19= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T001_slope_2.csv', skiprows = 1,header = None)
Output_20= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T001_stair_1.csv', skiprows = 1,header = None)
Output_21= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T001_stair_2.csv', skiprows = 1,header = None)

slope_1=np.concatenate((Features_18,Output_18),axis=1)
slope_2=np.concatenate((Features_19,Output_19),axis=1)
stair_1=np.concatenate((Features_20,Output_20),axis=1)
stair_2=np.concatenate((Features_21,Output_21),axis=1)


###          ###

fast_dataset=np.concatenate((Features_1,Output_1),axis=1)
normal_dataset=np.concatenate((Features_2,Output_2),axis=1)
slow_1_dataset=np.concatenate((Features_3,Output_3),axis=1)
slow_2_dataset=np.concatenate((Features_4,Output_4),axis=1)
vfast_dataset=np.concatenate((Features_5,Output_5),axis=1)


fast_dataset_o=np.concatenate((Features_6,Output_6),axis=1)
normal_dataset_o=np.concatenate((Features_7,Output_7),axis=1)
slow_dataset_o=np.concatenate((Features_8,Output_8),axis=1)
vfast_dataset_o=np.concatenate((Features_9,Output_9),axis=1)


slope_1_ascent=np.concatenate((Features_10,Output_10),axis=1)
slope_1_decent=np.concatenate((Features_11,Output_11),axis=1)
slope_2_ascent=np.concatenate((Features_12,Output_12),axis=1)
slope_2_decent=np.concatenate((Features_13,Output_13),axis=1)
stair_1_ascent=np.concatenate((Features_14,Output_14),axis=1)
stair_1_decent=np.concatenate((Features_15,Output_15),axis=1)
stair_2_ascent=np.concatenate((Features_16,Output_16),axis=1)
stair_2_decent=np.concatenate((Features_17,Output_17),axis=1)



 
 
 
subject_1_treadmill=np.concatenate((fast_dataset,normal_dataset,slow_2_dataset,vfast_dataset),axis=0)
subject_1_overground=np.concatenate((fast_dataset_o,normal_dataset_o,slow_dataset_o,vfast_dataset_o),axis=0)
subject_1_stair_slope=np.concatenate((slope_1_ascent,slope_1_decent,slope_2_ascent,slope_2_decent,stair_1_ascent,stair_1_decent,stair_2_ascent,stair_2_decent),axis=0)
#subject_1_stair_slope=np.concatenate((stair_1_ascent,stair_1_decent),axis=0)


subject_1=np.concatenate((subject_1_treadmill,subject_1_overground,subject_1_stair_slope),axis=0)
# subject_1=np.concatenate((subject_1_treadmill,subject_1_overground),axis=0)

# subject_1=subject_1_overground


#subject_1_stair_slope=np.concatenate((slope_1,slope_2,stair_1,stair_2),axis=0)
# subject_1=subject_1_stair_slope

              ### Extra features ###

features_E_1=np.array([1,60.18,1.7,0.297,0.143,0.264,0.077,0.186,0.338,0.412,0.273,0.121,0.075,0.040898,0.0309,0.468])
features_E_1=features_E_1.repeat(subject_1.shape[0])
features_E_1=features_E_1.reshape(16,subject_1.shape[0])
features_E_1=features_E_1.transpose()

subject_1=np.concatenate((subject_1,features_E_1),axis=1)



### treadmill ###

features_E_1=np.array([1,60.18,1.7,0.297,0.143,0.264,0.077,0.186,0.338,0.412,0.273,0.121,0.075,0.040898,0.0309,0.468])
features_E_1=features_E_1.repeat(subject_1_treadmill.shape[0])
features_E_1=features_E_1.reshape(16,subject_1_treadmill.shape[0])
features_E_1=features_E_1.transpose()

subject_1_treadmill=np.concatenate((subject_1_treadmill,features_E_1),axis=1)

### overground ###

features_E_1=np.array([1,60.18,1.7,0.297,0.143,0.264,0.077,0.186,0.338,0.412,0.273,0.121,0.075,0.040898,0.0309,0.468])
features_E_1=features_E_1.repeat(subject_1_overground.shape[0])
features_E_1=features_E_1.reshape(16,subject_1_overground.shape[0])
features_E_1=features_E_1.transpose()

subject_1_overground=np.concatenate((subject_1_overground,features_E_1),axis=1)


### stair_slope ###

features_E_1=np.array([1,60.18,1.7,0.297,0.143,0.264,0.077,0.186,0.338,0.412,0.273,0.121,0.075,0.040898,0.0309,0.468])
features_E_1=features_E_1.repeat(subject_1_stair_slope.shape[0])
features_E_1=features_E_1.reshape(16,subject_1_stair_slope.shape[0])
features_E_1=features_E_1.transpose()

subject_1_stair_slope=np.concatenate((subject_1_stair_slope,features_E_1),axis=1)


 
 
print(subject_1.shape)

####### Subject 2 #######
 
 
 
Features_1= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_treadmill_fast_features.csv', delimiter=',')
Features_2= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_treadmill_normal_features.csv', delimiter=',')
Features_3= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_treadmill_slow_1_features.csv', delimiter=',')
Features_4= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_treadmill_slow_2_features.csv', delimiter=',')
Features_5= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_treadmill_vfast_features.csv', delimiter=',')

Features_6= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_overground_fast_features.csv', delimiter=',')
Features_7= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_overground_normal_features.csv', delimiter=',')
Features_8= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_overground_slow_features.csv', delimiter=',')
Features_9= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_overground_vfast_features.csv', delimiter=',')

Features_10= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_slope_1_features_ascent.csv', delimiter=',')
Features_11= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_slope_1_features_decent.csv', delimiter=',')
Features_12= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_slope_2_features_ascent.csv', delimiter=',')
Features_13= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_slope_2_features_decent.csv', delimiter=',')
Features_14= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_stair_1_features_ascent.csv', delimiter=',')
Features_15= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_stair_1_features_decent.csv', delimiter=',')
Features_16= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_stair_2_features_ascent.csv', delimiter=',')
Features_17= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_stair_2_features_decent.csv', delimiter=',')


 ## output ##
 
Output_1= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_treadmill_fast_v4.csv', delimiter=',')
Output_2= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_treadmill_normal_v4.csv', delimiter=',')
Output_3= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_treadmill_slow_1_v4.csv', delimiter=',')
Output_4= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_treadmill_slow_2_v4.csv', delimiter=',')
Output_5= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_treadmill_vfast_v4.csv', delimiter=',')
 
Output_6= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_overground_fast.csv', delimiter=',')
Output_7= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_overground_normal.csv', delimiter=',')
Output_8= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_overground_slow.csv', delimiter=',')
Output_9= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_overground_vfast.csv', delimiter=',')

Output_10= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_slope_1_ascent.csv', delimiter=',')
Output_11= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_slope_1_decent.csv', delimiter=',')
Output_12= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_slope_2_ascent.csv', delimiter=',')
Output_13= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_slope_2_decent.csv', delimiter=',')
Output_14= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_stair_1_ascent.csv', delimiter=',')
Output_15= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_stair_1_decent.csv', delimiter=',')
Output_16= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_stair_2_ascent.csv', delimiter=',')
Output_17= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T002_stair_2_decent.csv', delimiter=',')

#### Slope and stair ###

Features_18=pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T002_slope_1_features.csv', header=None)
Features_19= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T002_slope_2_features.csv', header=None)
Features_20= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T002_stair_1_features.csv', header=None)
Features_21=pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T002_stair_2_features.csv', header=None)


Output_18= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T002_slope_1.csv',skiprows = 1,header = None)
Output_19= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T002_slope_2.csv', skiprows = 1,header = None)
Output_20= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T002_stair_1.csv', skiprows = 1,header = None)
Output_21= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T002_stair_2.csv', skiprows = 1,header = None)

slope_1=np.concatenate((Features_18,Output_18),axis=1)
slope_2=np.concatenate((Features_19,Output_19),axis=1)
stair_1=np.concatenate((Features_20,Output_20),axis=1)
stair_2=np.concatenate((Features_21,Output_21),axis=1)


###          ###
 
fast_dataset=np.concatenate((Features_1,Output_1),axis=1)
normal_dataset=np.concatenate((Features_2,Output_2),axis=1)
slow_1_dataset=np.concatenate((Features_3,Output_3),axis=1)
slow_2_dataset=np.concatenate((Features_4,Output_4),axis=1)
vfast_dataset=np.concatenate((Features_5,Output_5),axis=1)


fast_dataset_o=np.concatenate((Features_6,Output_6),axis=1)
normal_dataset_o=np.concatenate((Features_7,Output_7),axis=1)
slow_dataset_o=np.concatenate((Features_8,Output_8),axis=1)
vfast_dataset_o=np.concatenate((Features_9,Output_9),axis=1)


slope_1_ascent=np.concatenate((Features_10,Output_10),axis=1)
slope_1_decent=np.concatenate((Features_11,Output_11),axis=1)
slope_2_ascent=np.concatenate((Features_12,Output_12),axis=1)
slope_2_decent=np.concatenate((Features_13,Output_13),axis=1)
stair_1_ascent=np.concatenate((Features_14,Output_14),axis=1)
stair_1_decent=np.concatenate((Features_15,Output_15),axis=1)
stair_2_ascent=np.concatenate((Features_16,Output_16),axis=1)
stair_2_decent=np.concatenate((Features_17,Output_17),axis=1)



 
 
 
subject_2_treadmill=np.concatenate((fast_dataset,normal_dataset,slow_2_dataset,vfast_dataset),axis=0)
subject_2_overground=np.concatenate((fast_dataset_o,normal_dataset_o,slow_dataset_o,vfast_dataset_o),axis=0)
subject_2_stair_slope=np.concatenate((slope_1_ascent,slope_1_decent,slope_2_ascent,slope_2_decent,stair_1_ascent,stair_1_decent,stair_2_ascent,stair_2_decent),axis=0)
# subject_2_stair_slope=np.concatenate((stair_1_ascent,stair_1_decent,stair_2_ascent,stair_2_decent),axis=0)

 
 
subject_2=np.concatenate((subject_2_treadmill,subject_2_overground,subject_2_stair_slope),axis=0)
 
# subject_2=np.concatenate((subject_2_treadmill,subject_2_overground),axis=0)

#subject_2_stair_slope=np.concatenate((slope_1,slope_2,stair_1,stair_2),axis=0)

# subject_2=subject_2_stair_slope
 
               ### Extra features ###
               
              ### Extra features ###

features_E_1=np.array([0,53.47,1.548,0.292,0.153,0.231,0.07559,0.195,0.386,0.325,0.214,0.0967,0.0723,0.0373262,0.0234975,0.426])
features_E_1=features_E_1.repeat(subject_2.shape[0])
features_E_1=features_E_1.reshape(16,subject_2.shape[0])
features_E_1=features_E_1.transpose()

subject_2=np.concatenate((subject_2,features_E_1),axis=1)               


### treadmill ###

features_E_1=np.array([0,53.47,1.548,0.292,0.153,0.231,0.07559,0.195,0.386,0.325,0.214,0.0967,0.0723,0.0373262,0.0234975,0.426])
features_E_1=features_E_1.repeat(subject_2_treadmill.shape[0])
features_E_1=features_E_1.reshape(16,subject_2_treadmill.shape[0])
features_E_1=features_E_1.transpose()

subject_2_treadmill=np.concatenate((subject_2_treadmill,features_E_1),axis=1)

### overground ###

features_E_1=np.array([0,53.47,1.548,0.292,0.153,0.231,0.07559,0.195,0.386,0.325,0.214,0.0967,0.0723,0.0373262,0.0234975,0.426])
features_E_1=features_E_1.repeat(subject_2_overground.shape[0])
features_E_1=features_E_1.reshape(16,subject_2_overground.shape[0])
features_E_1=features_E_1.transpose()

subject_2_overground=np.concatenate((subject_2_overground,features_E_1),axis=1)


### stair_slope ###

features_E_1=np.array([0,53.47,1.548,0.292,0.153,0.231,0.07559,0.195,0.386,0.325,0.214,0.0967,0.0723,0.0373262,0.0234975,0.426])
features_E_1=features_E_1.repeat(subject_2_stair_slope.shape[0])
features_E_1=features_E_1.reshape(16,subject_2_stair_slope.shape[0])
features_E_1=features_E_1.transpose()

subject_2_stair_slope=np.concatenate((subject_2_stair_slope,features_E_1),axis=1)






 
print(subject_2.shape)

####### Subject 3 #######
 
 
Features_1= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_treadmill_fast_features.csv', delimiter=',')
Features_2= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_treadmill_normal_features.csv', delimiter=',')
Features_3= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_treadmill_slow_1_features.csv', delimiter=',')
Features_4= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_treadmill_slow_2_features.csv', delimiter=',')
Features_5= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_treadmill_vfast_features.csv', delimiter=',')

Features_6= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_overground_fast_features.csv', delimiter=',')
Features_7= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_overground_normal_features.csv', delimiter=',')
Features_8= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_overground_slow_features.csv', delimiter=',')
Features_9= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_overground_vfast_features.csv', delimiter=',')

Features_10= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_slope_1_features_ascent.csv', delimiter=',')
Features_11= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_slope_1_features_decent.csv', delimiter=',')
Features_12= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_slope_2_features_ascent.csv', delimiter=',')
Features_13= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_slope_2_features_decent.csv', delimiter=',')
Features_14= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_stair_1_features_ascent.csv', delimiter=',')
Features_15= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_stair_1_features_decent.csv', delimiter=',')
Features_16= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_stair_2_features_ascent.csv', delimiter=',')
Features_17= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_stair_2_features_decent.csv', delimiter=',')


 
Output_1= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_treadmill_fast_v2.csv', delimiter=',')
Output_2= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_treadmill_normal_v2.csv', delimiter=',')
Output_3= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_treadmill_slow_1_v2.csv', delimiter=',')
Output_4= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_treadmill_slow_2_v2.csv', delimiter=',')
Output_5= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_treadmill_vfast_v2.csv', delimiter=',')
 



## output ##

Output_6= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_overground_fast.csv', delimiter=',')
Output_7= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_overground_normal.csv', delimiter=',')
Output_8= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_overground_slow.csv', delimiter=',')
Output_9= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_overground_vfast.csv', delimiter=',')

Output_10= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_slope_1_ascent.csv', delimiter=',')
Output_11= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_slope_1_decent.csv', delimiter=',')
Output_12= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_slope_2_ascent.csv', delimiter=',')
Output_13= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_slope_2_decent.csv', delimiter=',')
Output_14= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_stair_1_ascent.csv', delimiter=',')
Output_15= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_stair_1_decent.csv', delimiter=',')
Output_16= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_stair_2_ascent.csv', delimiter=',')
Output_17= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T003_stair_2_decent.csv', delimiter=',')


#### Slope and stair ###

Features_18=pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T003_slope_1_features.csv', header=None)
Features_19= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T003_slope_2_features.csv', header=None)
Features_20= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T003_stair_1_features.csv', header=None)
Features_21=pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T003_stair_2_features.csv', header=None)


Output_18= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T003_slope_1.csv',skiprows = 1,header = None)
Output_19= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T003_slope_2.csv', skiprows = 1,header = None)
Output_20= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T003_stair_1.csv', skiprows = 1,header = None)
Output_21= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T003_stair_2.csv', skiprows = 1,header = None)

slope_1=np.concatenate((Features_18,Output_18),axis=1)
slope_2=np.concatenate((Features_19,Output_19),axis=1)
stair_1=np.concatenate((Features_20,Output_20),axis=1)
stair_2=np.concatenate((Features_21,Output_21),axis=1)


###          ###
 
fast_dataset=np.concatenate((Features_1,Output_1),axis=1)
normal_dataset=np.concatenate((Features_2,Output_2),axis=1)
slow_1_dataset=np.concatenate((Features_3,Output_3),axis=1)
slow_2_dataset=np.concatenate((Features_4,Output_4),axis=1)
vfast_dataset=np.concatenate((Features_5,Output_5),axis=1)


fast_dataset_o=np.concatenate((Features_6,Output_6),axis=1)
normal_dataset_o=np.concatenate((Features_7,Output_7),axis=1)
slow_dataset_o=np.concatenate((Features_8,Output_8),axis=1)
vfast_dataset_o=np.concatenate((Features_9,Output_9),axis=1)


slope_1_ascent=np.concatenate((Features_10,Output_10),axis=1)
slope_1_decent=np.concatenate((Features_11,Output_11),axis=1)
slope_2_ascent=np.concatenate((Features_12,Output_12),axis=1)
slope_2_decent=np.concatenate((Features_13,Output_13),axis=1)
stair_1_ascent=np.concatenate((Features_14,Output_14),axis=1)
stair_1_decent=np.concatenate((Features_15,Output_15),axis=1)
stair_2_ascent=np.concatenate((Features_16,Output_16),axis=1)
stair_2_decent=np.concatenate((Features_17,Output_17),axis=1)



 
 
 
subject_3_treadmill=np.concatenate((fast_dataset,normal_dataset,slow_2_dataset,vfast_dataset),axis=0)
subject_3_overground=np.concatenate((fast_dataset_o,normal_dataset_o,slow_dataset_o,vfast_dataset_o),axis=0)
subject_3_stair_slope=np.concatenate((slope_1_ascent,slope_1_decent,slope_2_ascent,slope_2_decent,stair_1_ascent,stair_1_decent,stair_2_ascent,stair_2_decent),axis=0)

#subject_3_stair_slope=np.concatenate((stair_1_decent,stair_2_decent),axis=0)

 
 
subject_3=np.concatenate((subject_3_treadmill,subject_3_overground,subject_3_stair_slope),axis=0)
 
# subject_3=np.concatenate((subject_3_treadmill,subject_3_overground),axis=0)
 
#subject_3_stair_slope=np.concatenate((slope_1,slope_2,stair_1,stair_2),axis=0)

# subject_3=subject_3_stair_slope

   ### Extra Features ###

features_E_1=np.array([1,65.54,1.73,0.252,0.126,0.211,0.04,0.191,0.367,0.367,0.271,0.118,0.084,0.043306,0.030828,0.457])
features_E_1=features_E_1.repeat(subject_3.shape[0])
features_E_1=features_E_1.reshape(16,subject_3.shape[0])
features_E_1=features_E_1.transpose()

subject_3=np.concatenate((subject_3,features_E_1),axis=1)


### treadmill ###

features_E_1=np.array([1,65.54,1.73,0.252,0.126,0.211,0.04,0.191,0.367,0.367,0.271,0.118,0.084,0.043306,0.030828,0.457])
features_E_1=features_E_1.repeat(subject_3_treadmill.shape[0])
features_E_1=features_E_1.reshape(16,subject_3_treadmill.shape[0])
features_E_1=features_E_1.transpose()

subject_3_treadmill=np.concatenate((subject_3_treadmill,features_E_1),axis=1)

### overground ###

features_E_1=np.array([1,65.54,1.73,0.252,0.126,0.211,0.04,0.191,0.367,0.367,0.271,0.118,0.084,0.043306,0.030828,0.457])
features_E_1=features_E_1.repeat(subject_3_overground.shape[0])
features_E_1=features_E_1.reshape(16,subject_3_overground.shape[0])
features_E_1=features_E_1.transpose()

subject_3_overground=np.concatenate((subject_3_overground,features_E_1),axis=1)


### stair_slope ###

features_E_1=np.array([1,65.54,1.73,0.252,0.126,0.211,0.04,0.191,0.367,0.367,0.271,0.118,0.084,0.043306,0.030828,0.457])
features_E_1=features_E_1.repeat(subject_3_stair_slope.shape[0])
features_E_1=features_E_1.reshape(16,subject_3_stair_slope.shape[0])
features_E_1=features_E_1.transpose()

subject_3_stair_slope=np.concatenate((subject_3_stair_slope,features_E_1),axis=1)

 
 
print(subject_3.shape)

####### Subject 4 #######
 
 
Features_1= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_treadmill_fast_features.csv', delimiter=',')
Features_2= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_treadmill_normal_features.csv', delimiter=',')
Features_3= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_treadmill_slow_1_features.csv', delimiter=',')
Features_4= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_treadmill_slow_2_features.csv', delimiter=',')
Features_5= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_treadmill_vfast_features.csv', delimiter=',')

Features_6= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_overground_fast_features.csv', delimiter=',')
Features_7= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_overground_normal_features.csv', delimiter=',')
Features_8= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_overground_slow_features.csv', delimiter=',')
Features_9= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_overground_vfast_features.csv', delimiter=',')

Features_10= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_slope_1_features_ascent.csv', delimiter=',')
Features_11= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_slope_1_features_decent.csv', delimiter=',')
Features_12= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_slope_2_features_ascent.csv', delimiter=',')
Features_13= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_slope_2_features_decent.csv', delimiter=',')
Features_14= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_stair_1_features_ascent.csv', delimiter=',')
Features_15= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_stair_1_features_decent.csv', delimiter=',')
Features_16= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_stair_2_features_ascent.csv', delimiter=',')
Features_17= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_stair_2_features_decent.csv', delimiter=',')



 
Output_1= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_treadmill_fast_v2.csv', delimiter=',')
Output_2= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_treadmill_normal_v2.csv', delimiter=',')
Output_3= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_treadmill_slow_1_v2.csv', delimiter=',')
Output_4= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_treadmill_slow_2_v2.csv', delimiter=',')
Output_5= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_treadmill_vfast_v2.csv', delimiter=',')


## output ##

Output_6= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_overground_fast.csv', delimiter=',')
Output_7= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_overground_normal.csv', delimiter=',')
Output_8= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_overground_slow.csv', delimiter=',')
Output_9= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_overground_vfast.csv', delimiter=',')

Output_10= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_slope_1_ascent.csv', delimiter=',')
Output_11= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_slope_1_decent.csv', delimiter=',')
Output_12= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_slope_2_ascent.csv', delimiter=',')
Output_13= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_slope_2_decent.csv', delimiter=',')
Output_14= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_stair_1_ascent.csv', delimiter=',')
Output_15= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_stair_1_decent.csv', delimiter=',')
Output_16= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_stair_2_ascent.csv', delimiter=',')
Output_17= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T004_stair_2_decent.csv', delimiter=',')


#### Slope and stair ###

Features_18=pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T004_slope_1_features.csv', header=None)
Features_19= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T004_slope_2_features.csv', header=None)
Features_20= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T004_stair_1_features.csv', header=None)
Features_21=pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T004_stair_2_features.csv', header=None)


Output_18= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T004_slope_1.csv',skiprows = 1,header = None)
Output_19= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T004_slope_2.csv', skiprows = 1,header = None)
Output_20= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T004_stair_1.csv', skiprows = 1,header = None)
Output_21= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T004_stair_2.csv', skiprows = 1,header = None)

slope_1=np.concatenate((Features_18,Output_18),axis=1)
slope_2_1=np.concatenate((Features_19,Output_19),axis=1)
stair_1=np.concatenate((Features_20,Output_20),axis=1)
stair_2=np.concatenate((Features_21,Output_21),axis=1)

###          ###
 
fast_dataset=np.concatenate((Features_1,Output_1),axis=1)
normal_dataset=np.concatenate((Features_2,Output_2),axis=1)
slow_1_dataset=np.concatenate((Features_3,Output_3),axis=1)
slow_2_dataset=np.concatenate((Features_4,Output_4),axis=1)
vfast_dataset=np.concatenate((Features_5,Output_5),axis=1)


fast_dataset_o=np.concatenate((Features_6,Output_6),axis=1)
normal_dataset_o=np.concatenate((Features_7,Output_7),axis=1)
slow_dataset_o=np.concatenate((Features_8,Output_8),axis=1)
vfast_dataset_o=np.concatenate((Features_9,Output_9),axis=1)


slope_1_ascent=np.concatenate((Features_10,Output_10),axis=1)
slope_1_decent=np.concatenate((Features_11,Output_11),axis=1)
slope_2_ascent=np.concatenate((Features_12,Output_12),axis=1)
slope_2_decent=np.concatenate((Features_13,Output_13),axis=1)
stair_1_ascent=np.concatenate((Features_14,Output_14),axis=1)
stair_1_decent=np.concatenate((Features_15,Output_15),axis=1)
stair_2_ascent=np.concatenate((Features_16,Output_16),axis=1)
stair_2_decent=np.concatenate((Features_17,Output_17),axis=1)


 
 
subject_4_treadmill=np.concatenate((fast_dataset,normal_dataset,slow_2_dataset,vfast_dataset),axis=0)
subject_4_overground=np.concatenate((fast_dataset_o,normal_dataset_o,slow_dataset_o,vfast_dataset_o),axis=0)
subject_4_stair_slope=np.concatenate((slope_1_ascent,slope_1_decent,slope_2_ascent,slope_2_decent,stair_1_ascent,stair_1_decent,stair_2_ascent,stair_2_decent),axis=0)
 
 
# subject_4=subject_4_stair_slope
# subject_4=np.concatenate((subject_4_treadmill,subject_4_overground),axis=0)
 
subject_4=np.concatenate((subject_4_treadmill,subject_4_overground,subject_4_stair_slope),axis=0)
 
              ### Extra features ###

features_E_1=np.array([1,55.62,1.62,0.294,0.124,0.21,0.09,0.18489,0.366,0.331,0.257,0.111,0.08,0.040626,0.02648,0.479])
features_E_1=features_E_1.repeat(subject_4.shape[0])
features_E_1=features_E_1.reshape(16,subject_4.shape[0])
features_E_1=features_E_1.transpose()

subject_4=np.concatenate((subject_4,features_E_1),axis=1)


### treadmill ###

features_E_1=np.array([1,55.62,1.62,0.294,0.124,0.21,0.09,0.18489,0.366,0.331,0.257,0.111,0.08,0.040626,0.02648,0.479])
features_E_1=features_E_1.repeat(subject_4_treadmill.shape[0])
features_E_1=features_E_1.reshape(16,subject_4_treadmill.shape[0])
features_E_1=features_E_1.transpose()

subject_4_treadmill=np.concatenate((subject_4_treadmill,features_E_1),axis=1)

### overground ###

features_E_1=np.array([1,55.62,1.62,0.294,0.124,0.21,0.09,0.18489,0.366,0.331,0.257,0.111,0.08,0.040626,0.02648,0.479])
features_E_1=features_E_1.repeat(subject_4_overground.shape[0])
features_E_1=features_E_1.reshape(16,subject_4_overground.shape[0])
features_E_1=features_E_1.transpose()

subject_4_overground=np.concatenate((subject_4_overground,features_E_1),axis=1)


### stair_slope ###

features_E_1=np.array([1,55.62,1.62,0.294,0.124,0.21,0.09,0.18489,0.366,0.331,0.257,0.111,0.08,0.040626,0.02648,0.479])
features_E_1=features_E_1.repeat(subject_4_stair_slope.shape[0])
features_E_1=features_E_1.reshape(16,subject_4_stair_slope.shape[0])
features_E_1=features_E_1.transpose()

subject_4_stair_slope=np.concatenate((subject_4_stair_slope,features_E_1),axis=1)


print(subject_4.shape)

####### Subject 5 #######
 
 
Features_1= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_treadmill_fast_features.csv', delimiter=',')
Features_2= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_treadmill_normal_features.csv', delimiter=',')
Features_3= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_treadmill_slow_1_features.csv', delimiter=',')
Features_4= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_treadmill_slow_2_features.csv', delimiter=',')
Features_5= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_treadmill_vfast_1_features.csv', delimiter=',')
Features_6= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_treadmill_vfast_2_features.csv', delimiter=',')

Features_7= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_overground_fast_features.csv', delimiter=',')
Features_8= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_overground_normal_features.csv', delimiter=',')
Features_9= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_overground_slow_features.csv', delimiter=',')
Features_10= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_overground_vfast_features.csv', delimiter=',')

Features_11= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_slope_1_features_ascent.csv', delimiter=',')
Features_12= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_slope_1_features_decent.csv', delimiter=',')
Features_13= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_slope_2_features_ascent.csv', delimiter=',')
Features_14= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_slope_2_features_decent.csv', delimiter=',')
Features_15= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_stair_1_features_ascent.csv', delimiter=',')
Features_16= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_stair_1_features_decent.csv', delimiter=',')
Features_17= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_stair_2_features_ascent.csv', delimiter=',')
Features_18= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_stair_2_features_decent.csv', delimiter=',')

 
 
Output_1= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_treadmill_fast_v9.csv', delimiter=',')
Output_2= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_treadmill_normal_v9.csv', delimiter=',')
Output_3= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_treadmill_slow_1_v9.csv', delimiter=',')
Output_4= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_treadmill_slow_2_v9.csv', delimiter=',')
Output_5= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_treadmill_vfast_1_v9.csv', delimiter=',')
Output_6= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_treadmill_vfast_2_v9.csv', delimiter=',')

## output ##

Output_7= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_overground_fast.csv', delimiter=',')
Output_8= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_overground_normal.csv', delimiter=',')
Output_9= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_overground_slow.csv', delimiter=',')
Output_10= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_overground_vfast.csv', delimiter=',')

Output_11= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_slope_1_ascent.csv', delimiter=',')
Output_12= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_slope_1_decent.csv', delimiter=',')
Output_13= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_slope_2_ascent.csv', delimiter=',')
Output_14= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_slope_2_decent.csv', delimiter=',')
Output_15= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_stair_1_ascent.csv', delimiter=',')
Output_16= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_stair_1_decent.csv', delimiter=',')
Output_17= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_stair_2_ascent.csv', delimiter=',')
Output_18= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T005_stair_2_decent.csv', delimiter=',')


#### Slope and stair ###

Features_19=pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T005_slope_1_features.csv', header=None)
Features_20= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T005_slope_2_features.csv', header=None)
Features_21= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T005_stair_1_features.csv', header=None)
Features_22=pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T005_stair_2_features.csv', header=None)


Output_19= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T005_slope_1.csv',skiprows = 1,header = None)
Output_20= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T005_slope_2.csv', skiprows = 1,header = None)
Output_21= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T005_stair_1.csv', skiprows = 1,header = None)
Output_22= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T005_stair_2.csv', skiprows = 1,header = None)

slope_1=np.concatenate((Features_18,Output_18),axis=1)
slope_2=np.concatenate((Features_19,Output_19),axis=1)
stair_1=np.concatenate((Features_20,Output_20),axis=1)
stair_2=np.concatenate((Features_21,Output_21),axis=1)


###          ###

 
fast_dataset=np.concatenate((Features_1,Output_1),axis=1)
normal_dataset=np.concatenate((Features_2,Output_2),axis=1)
slow_1_dataset=np.concatenate((Features_3,Output_3),axis=1)
slow_2_dataset=np.concatenate((Features_4,Output_4),axis=1)
vfast_dataset_1=np.concatenate((Features_5,Output_5),axis=1)
vfast_dataset_2=np.concatenate((Features_6,Output_6),axis=1)


fast_dataset_o=np.concatenate((Features_7,Output_7),axis=1)
normal_dataset_o=np.concatenate((Features_8,Output_8),axis=1)
slow_dataset_o=np.concatenate((Features_9,Output_9),axis=1)
vfast_dataset_o=np.concatenate((Features_10,Output_10),axis=1)


slope_1_ascent=np.concatenate((Features_11,Output_11),axis=1)
slope_1_decent=np.concatenate((Features_12,Output_12),axis=1)
slope_2_ascent=np.concatenate((Features_13,Output_13),axis=1)
slope_2_decent=np.concatenate((Features_14,Output_14),axis=1)
stair_1_ascent=np.concatenate((Features_15,Output_15),axis=1)
stair_1_decent=np.concatenate((Features_16,Output_16),axis=1)
stair_2_ascent=np.concatenate((Features_17,Output_17),axis=1)
stair_2_decent=np.concatenate((Features_18,Output_18),axis=1)



 
 
 
subject_5_treadmill=np.concatenate((fast_dataset,normal_dataset,slow_2_dataset,vfast_dataset_1,vfast_dataset_2),axis=0)
subject_5_overground=np.concatenate((fast_dataset_o,normal_dataset_o,slow_dataset_o,vfast_dataset_o),axis=0)
subject_5_stair_slope=np.concatenate((slope_1_ascent,slope_1_decent,slope_2_ascent,slope_2_decent,stair_1_ascent,stair_1_decent,stair_2_ascent,stair_2_decent),axis=0)
 



#subject_5_stair_slope=np.concatenate((stair_1_decent,stair_2_decent),axis=0)

 
subject_5=np.concatenate((subject_5_treadmill,subject_5_overground,subject_5_stair_slope),axis=0)
 
# subject_5=np.concatenate((subject_5_treadmill,subject_5_overground),axis=0)
 
#subject_5_stair_slope=np.concatenate((slope_1,slope_2,stair_1,stair_2),axis=0)
# subject_5=subject_5_stair_slope

              ### Extra features ###

features_E_1=np.array([1,72.9,1.74,0.236,0.17,0.215,0.1,0.187,0.444,0.363,0.281,0.108,0.088,0.047952,0.031944,0.517])

# features_E_1=np.array([0,0,0,0,0,0])
features_E_1=features_E_1.repeat(subject_5.shape[0])
features_E_1=features_E_1.reshape(16,subject_5.shape[0])
features_E_1=features_E_1.transpose()

subject_5=np.concatenate((subject_5,features_E_1),axis=1)

### treadmill ###

features_E_1=np.array([1,72.9,1.74,0.236,0.17,0.215,0.1,0.187,0.444,0.363,0.281,0.108,0.088,0.047952,0.031944,0.517])

# features_E_1=np.array([0,0,0,0,0,0])
features_E_1=features_E_1.repeat(subject_5_treadmill.shape[0])
features_E_1=features_E_1.reshape(16,subject_5_treadmill.shape[0])
features_E_1=features_E_1.transpose()

subject_5_treadmill=np.concatenate((subject_5_treadmill,features_E_1),axis=1)

### overground ###

features_E_1=np.array([1,72.9,1.74,0.236,0.17,0.215,0.1,0.187,0.444,0.363,0.281,0.108,0.088,0.047952,0.031944,0.517])

# features_E_1=np.array([0,0,0,0,0,0])
features_E_1=features_E_1.repeat(subject_5_overground.shape[0])
features_E_1=features_E_1.reshape(16,subject_5_overground.shape[0])
features_E_1=features_E_1.transpose()

subject_5_overground=np.concatenate((subject_5_overground,features_E_1),axis=1)


### stair_slope ###

features_E_1=np.array([1,72.9,1.74,0.236,0.17,0.215,0.1,0.187,0.444,0.363,0.281,0.108,0.088,0.047952,0.031944,0.517])

# features_E_1=np.array([0,0,0,0,0,0])
features_E_1=features_E_1.repeat(subject_5_stair_slope.shape[0])
features_E_1=features_E_1.reshape(16,subject_5_stair_slope.shape[0])
features_E_1=features_E_1.transpose()

subject_5_stair_slope=np.concatenate((subject_5_stair_slope,features_E_1),axis=1)
 

print(subject_5.shape)

#### Subject 6 ###

Features_1= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_treadmill_fast_features.csv', delimiter=',')
Features_2= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_treadmill_normal_features.csv', delimiter=',')
Features_3= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_treadmill_slow_1_features.csv', delimiter=',')
Features_4= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_treadmill_slow_2_features.csv', delimiter=',')
Features_5= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_treadmill_vfast_features.csv', delimiter=',')

Features_6= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_overground_fast_features.csv', delimiter=',')
Features_7= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_overground_normal_features.csv', delimiter=',')
Features_8= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_overground_slow_features.csv', delimiter=',')
Features_9= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_overground_vfast_features.csv', delimiter=',')

Features_10= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_slope_1_features_ascent.csv', delimiter=',')
Features_11= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_slope_1_features_decent.csv', delimiter=',')
Features_12= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_slope_2_features_ascent.csv', delimiter=',')
Features_13= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_slope_2_features_decent.csv', delimiter=',')
Features_14= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_stair_1_features_ascent.csv', delimiter=',')
Features_15= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_stair_1_features_decent.csv', delimiter=',')
Features_16= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_stair_2_features_ascent.csv', delimiter=',')
Features_17= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_stair_2_features_decent.csv', delimiter=',')


 
Output_1= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_treadmill_fast_v5.csv', delimiter=',')
Output_2= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_treadmill_normal_v5.csv', delimiter=',')
Output_3= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_treadmill_slow_1_v5.csv', delimiter=',')
Output_4= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_treadmill_slow_2_v5.csv', delimiter=',')
Output_5= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_treadmill_vfast_v5.csv', delimiter=',')
 
 
 
 
## output ##

Output_6= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_overground_fast.csv', delimiter=',')
Output_7= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_overground_normal.csv', delimiter=',')
Output_8= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_overground_slow.csv', delimiter=',')
Output_9= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_overground_vfast.csv', delimiter=',')

Output_10= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_slope_1_ascent.csv', delimiter=',')
Output_11= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_slope_1_decent.csv', delimiter=',')
Output_12= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_slope_2_ascent.csv', delimiter=',')
Output_13= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_slope_2_decent.csv', delimiter=',')
Output_14= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_stair_1_ascent.csv', delimiter=',')
Output_15= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_stair_1_decent.csv', delimiter=',')
Output_16= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_stair_2_ascent.csv', delimiter=',')
Output_17= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T006_stair_2_decent.csv', delimiter=',')



#### Slope and stair ###

Features_18=pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T006_slope_1_features.csv', header=None)
Features_19= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T006_slope_2_features.csv', header=None)
Features_20= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T006_stair_1_features.csv', header=None)
Features_21=pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T006_stair_2_features.csv', header=None)


Output_18= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T006_slope_1.csv',skiprows = 1,header = None)
Output_19= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T006_slope_2.csv', skiprows = 1,header = None)
Output_20= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T006_stair_1.csv', skiprows = 1,header = None)
Output_21= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T006_stair_2.csv', skiprows = 1,header = None)

slope_1=np.concatenate((Features_18,Output_18),axis=1)
slope_2=np.concatenate((Features_19,Output_19),axis=1)
stair_1=np.concatenate((Features_20,Output_20),axis=1)
stair_2=np.concatenate((Features_21,Output_21),axis=1)


###          ###
 
 
fast_dataset=np.concatenate((Features_1,Output_1),axis=1)
normal_dataset=np.concatenate((Features_2,Output_2),axis=1)
slow_1_dataset=np.concatenate((Features_3,Output_3),axis=1)
slow_2_dataset=np.concatenate((Features_4,Output_4),axis=1)
vfast_dataset=np.concatenate((Features_5,Output_5),axis=1)


fast_dataset_o=np.concatenate((Features_6,Output_6),axis=1)
normal_dataset_o=np.concatenate((Features_7,Output_7),axis=1)
slow_dataset_o=np.concatenate((Features_8,Output_8),axis=1)
vfast_dataset_o=np.concatenate((Features_9,Output_9),axis=1)


slope_1_ascent=np.concatenate((Features_10,Output_10),axis=1)
slope_1_decent=np.concatenate((Features_11,Output_11),axis=1)
slope_2_ascent=np.concatenate((Features_12,Output_12),axis=1)
slope_2_decent=np.concatenate((Features_13,Output_13),axis=1)
stair_1_ascent=np.concatenate((Features_14,Output_14),axis=1)
stair_1_decent=np.concatenate((Features_15,Output_15),axis=1)
stair_2_ascent=np.concatenate((Features_16,Output_16),axis=1)
stair_2_decent=np.concatenate((Features_17,Output_17),axis=1)



 
 
 
subject_6_treadmill=np.concatenate((fast_dataset,normal_dataset,slow_1_dataset,vfast_dataset),axis=0)
subject_6_overground=np.concatenate((fast_dataset_o,normal_dataset_o,slow_dataset_o,vfast_dataset_o),axis=0)
subject_6_stair_slope=np.concatenate((slope_1_ascent,slope_1_decent,slope_2_ascent,slope_2_decent,stair_1_ascent,stair_1_decent,stair_2_ascent,stair_2_decent),axis=0)
 
#subject_6_stair_slope=np.concatenate((stair_1_decent,stair_2_decent),axis=0)
 
# subject_6=subject_6_stair_slope
subject_6=np.concatenate((subject_6_treadmill,subject_6_overground,subject_6_stair_slope),axis=0)

# subject_6=np.concatenate((subject_6_treadmill,subject_6_overground),axis=0)

#subject_6_stair_slope=np.concatenate((slope_1,slope_2,stair_1,stair_2),axis=0)
 
              ### Extra features ###

features_E_1=np.array([1,74.48,1.68,0.263,0.151,0.262,0.07,0.22,0.434,0.338,0.262,0.121,0.08,0.052514,0.02704,0.497])
features_E_1=features_E_1.repeat(subject_6.shape[0])
features_E_1=features_E_1.reshape(16,subject_6.shape[0])
features_E_1=features_E_1.transpose()

subject_6=np.concatenate((subject_6,features_E_1),axis=1)

### treadmill ###

features_E_1=np.array([1,74.48,1.68,0.263,0.151,0.262,0.07,0.22,0.434,0.338,0.262,0.121,0.08,0.052514,0.02704,0.497])
features_E_1=features_E_1.repeat(subject_6_treadmill.shape[0])
features_E_1=features_E_1.reshape(16,subject_6_treadmill.shape[0])
features_E_1=features_E_1.transpose()

subject_6_treadmill=np.concatenate((subject_6_treadmill,features_E_1),axis=1)

### overground ###

features_E_1=np.array([1,74.48,1.68,0.263,0.151,0.262,0.07,0.22,0.434,0.338,0.262,0.121,0.08,0.052514,0.02704,0.497])
features_E_1=features_E_1.repeat(subject_6_overground.shape[0])
features_E_1=features_E_1.reshape(16,subject_6_overground.shape[0])
features_E_1=features_E_1.transpose()

subject_6_overground=np.concatenate((subject_6_overground,features_E_1),axis=1)


### stair_slope ###

features_E_1=np.array([1,74.48,1.68,0.263,0.151,0.262,0.07,0.22,0.434,0.338,0.262,0.121,0.08,0.052514,0.02704,0.497])
features_E_1=features_E_1.repeat(subject_6_stair_slope.shape[0])
features_E_1=features_E_1.reshape(16,subject_6_stair_slope.shape[0])
features_E_1=features_E_1.transpose()

subject_6_stair_slope=np.concatenate((subject_6_stair_slope,features_E_1),axis=1)
 
 
 

print(subject_6.shape)

#### Subject 7 ###

Features_1= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_treadmill_fast_features.csv', delimiter=',')
Features_2= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_treadmill_normal_features.csv', delimiter=',')
Features_3= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_treadmill_slow_1_features.csv', delimiter=',')
Features_4= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_treadmill_slow_2_features.csv', delimiter=',')
Features_5= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_treadmill_vfast_features.csv', delimiter=',')

Features_6= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_overground_fast_features.csv', delimiter=',')
Features_7= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_overground_normal_features.csv', delimiter=',')
Features_8= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_overground_slow_features.csv', delimiter=',')
Features_9= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_overground_vfast_features.csv', delimiter=',')

Features_10= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_slope_1_features_ascent.csv', delimiter=',')
Features_11= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_slope_1_features_decent.csv', delimiter=',')
Features_12= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_slope_2_features_ascent.csv', delimiter=',')
Features_13= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_slope_2_features_decent.csv', delimiter=',')
Features_14= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_stair_1_features_ascent.csv', delimiter=',')
Features_15= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_stair_1_features_decent.csv', delimiter=',')
Features_16= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_stair_2_features_ascent.csv', delimiter=',')
Features_17= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_stair_2_features_decent.csv', delimiter=',')

## features ##
 
Output_1= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_treadmill_fast_v3.csv', delimiter=',')
Output_2= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_treadmill_normal_v3.csv', delimiter=',')
Output_3= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_treadmill_slow_1_v3.csv', delimiter=',')
Output_4= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_treadmill_slow_2_v3.csv', delimiter=',')
Output_5= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_treadmill_vfast_v3.csv', delimiter=',')
 
## output ##

Output_6= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_overground_fast.csv', delimiter=',')
Output_7= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_overground_normal.csv', delimiter=',')
Output_8= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_overground_slow.csv', delimiter=',')
Output_9= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_overground_vfast.csv', delimiter=',')

Output_10= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_slope_1_ascent.csv', delimiter=',')
Output_11= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_slope_1_decent.csv', delimiter=',')
Output_12= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_slope_2_ascent.csv', delimiter=',')
Output_13= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_slope_2_decent.csv', delimiter=',')
Output_14= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_stair_1_ascent.csv', delimiter=',')
Output_15= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_stair_1_decent.csv', delimiter=',')
Output_16= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_stair_2_ascent.csv', delimiter=',')
Output_17= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T007_stair_2_decent.csv', delimiter=',')


#### Slope and stair ###

Features_18=pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T007_slope_1_features.csv', header=None)
Features_19= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T007_slope_2_features.csv', header=None)
Features_20= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T007_stair_1_features.csv', header=None)
Features_21=pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T007_stair_2_features.csv', header=None)


Output_18= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T007_slope_1.csv',skiprows = 1,header = None)
Output_19= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T007_slope_2.csv', skiprows = 1,header = None)
Output_20= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T007_stair_1.csv', skiprows = 1,header = None)
Output_21= pd.read_csv('/home/sanzidpr/DeepBBWAE_Net/P002_T007_stair_2.csv', skiprows = 1,header = None)

slope_1=np.concatenate((Features_18,Output_18),axis=1)
slope_2=np.concatenate((Features_19,Output_19),axis=1)
stair_1=np.concatenate((Features_20,Output_20),axis=1)
stair_2=np.concatenate((Features_21,Output_21),axis=1)


###          ###
 
 
fast_dataset=np.concatenate((Features_1,Output_1),axis=1)
normal_dataset=np.concatenate((Features_2,Output_2),axis=1)
slow_1_dataset=np.concatenate((Features_3,Output_3),axis=1)
slow_2_dataset=np.concatenate((Features_4,Output_4),axis=1)
vfast_dataset=np.concatenate((Features_5,Output_5),axis=1)


fast_dataset_o=np.concatenate((Features_6,Output_6),axis=1)
normal_dataset_o=np.concatenate((Features_7,Output_7),axis=1)
slow_dataset_o=np.concatenate((Features_8,Output_8),axis=1)
vfast_dataset_o=np.concatenate((Features_9,Output_9),axis=1)


slope_1_ascent=np.concatenate((Features_10,Output_10),axis=1)
slope_1_decent=np.concatenate((Features_11,Output_11),axis=1)
slope_2_ascent=np.concatenate((Features_12,Output_12),axis=1)
slope_2_decent=np.concatenate((Features_13,Output_13),axis=1)
stair_1_ascent=np.concatenate((Features_14,Output_14),axis=1)
stair_1_decent=np.concatenate((Features_15,Output_15),axis=1)
stair_2_ascent=np.concatenate((Features_16,Output_16),axis=1)
stair_2_decent=np.concatenate((Features_17,Output_17),axis=1)



 
 
 
subject_7_treadmill=np.concatenate((fast_dataset,normal_dataset,slow_1_dataset,slow_2_dataset,vfast_dataset),axis=0)
subject_7_overground=np.concatenate((fast_dataset_o,normal_dataset_o,slow_dataset_o,vfast_dataset_o),axis=0)
subject_7_stair_slope=np.concatenate((slope_1_ascent,slope_1_decent,slope_2_ascent,slope_2_decent,stair_1_ascent,stair_1_decent,stair_2_ascent,stair_2_decent),axis=0)

#subject_7_stair_slope=np.concatenate((stair_1_decent,stair_2_decent),axis=0)

 
 
subject_7=np.concatenate((subject_7_treadmill,subject_7_overground,subject_7_stair_slope),axis=0)
#  
# subject_7=np.concatenate((subject_7_treadmill,subject_7_overground),axis=0)
 
# subject_7_stair_slope=np.concatenate((slope_1,slope_2,stair_1,stair_2),axis=0)
# subject_7=subject_7_stair_slope

              ### Extra features ###

features_E_1=np.array([1,68.9,1.67,0.291,0.135,0.251,0.105,0.2,0.353,0.349,0.273,0.108,0.076,0.038124,0.026524,0.465])
features_E_1=features_E_1.repeat(subject_7.shape[0])
features_E_1=features_E_1.reshape(16,subject_7.shape[0])
features_E_1=features_E_1.transpose()

subject_7=np.concatenate((subject_7,features_E_1),axis=1)

### treadmill ###

features_E_1=np.array([1,68.9,1.67,0.291,0.135,0.251,0.105,0.2,0.353,0.349,0.273,0.108,0.076,0.038124,0.026524,0.465])
features_E_1=features_E_1.repeat(subject_7_treadmill.shape[0])
features_E_1=features_E_1.reshape(16,subject_7_treadmill.shape[0])
features_E_1=features_E_1.transpose()

subject_7_treadmill=np.concatenate((subject_7_treadmill,features_E_1),axis=1)

### overground ###

features_E_1=np.array([1,68.9,1.67,0.291,0.135,0.251,0.105,0.2,0.353,0.349,0.273,0.108,0.076,0.038124,0.026524,0.465])
features_E_1=features_E_1.repeat(subject_7_overground.shape[0])
features_E_1=features_E_1.reshape(16,subject_7_overground.shape[0])
features_E_1=features_E_1.transpose()

subject_7_overground=np.concatenate((subject_7_overground,features_E_1),axis=1)


### stair_slope ###

features_E_1=np.array([1,68.9,1.67,0.291,0.135,0.251,0.105,0.2,0.353,0.349,0.273,0.108,0.076,0.038124,0.026524,0.465])
features_E_1=features_E_1.repeat(subject_7_stair_slope.shape[0])
features_E_1=features_E_1.reshape(16,subject_7_stair_slope.shape[0])
features_E_1=features_E_1.transpose()

subject_7_stair_slope=np.concatenate((subject_7_stair_slope,features_E_1),axis=1)
 
 
 
 
print(subject_7.shape)

#### Subject 8 ###

Features_1= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_treadmill_fast_features.csv', delimiter=',')
Features_2= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_treadmill_normal_features.csv', delimiter=',')
Features_3= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_treadmill_slow_1_features.csv', delimiter=',')
Features_4= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_treadmill_slow_2_features.csv', delimiter=',')
Features_5= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_treadmill_vfast_features.csv', delimiter=',')


Features_6= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_overground_fast_features.csv', delimiter=',')
Features_7= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_overground_normal_features.csv', delimiter=',')
Features_8= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_overground_slow_features.csv', delimiter=',')
Features_9= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_overground_vfast_features.csv', delimiter=',')

Features_10= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_slope_1_features_ascent.csv', delimiter=',')
Features_11= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_slope_1_features_decent.csv', delimiter=',')
Features_12= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_slope_2_features_ascent.csv', delimiter=',')
Features_13= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_slope_2_features_decent.csv', delimiter=',')
Features_14= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_stair_1_features_ascent.csv', delimiter=',')
Features_15= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_stair_1_features_decent.csv', delimiter=',')
Features_16= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_stair_2_features_ascent.csv', delimiter=',')
Features_17= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_stair_2_features_decent.csv', delimiter=',')


 
Output_1= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_treadmill_fast_v5.csv', delimiter=',')
Output_2= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_treadmill_normal_v5.csv', delimiter=',')
Output_3= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_treadmill_slow_1_v5.csv', delimiter=',')
Output_4= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_treadmill_slow_2_v5.csv', delimiter=',')
Output_5= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_treadmill_vfast_v5.csv', delimiter=',')

## output ##

Output_6= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_overground_fast.csv', delimiter=',')
Output_7= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_overground_normal.csv', delimiter=',')
Output_8= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_overground_slow.csv', delimiter=',')
Output_9= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_overground_vfast.csv', delimiter=',')

Output_10= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_slope_1_ascent.csv', delimiter=',')
Output_11= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_slope_1_decent.csv', delimiter=',')
Output_12= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_slope_2_ascent.csv', delimiter=',')
Output_13= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_slope_2_decent.csv', delimiter=',')
Output_14= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_stair_1_ascent.csv', delimiter=',')
Output_15= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_stair_1_decent.csv', delimiter=',')
Output_16= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_stair_2_ascent.csv', delimiter=',')
Output_17= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T008_stair_2_decent.csv', delimiter=',')


 
fast_dataset=np.concatenate((Features_1,Output_1),axis=1)
normal_dataset_1=np.concatenate((Features_2,Output_2),axis=1)
slow_1_dataset=np.concatenate((Features_3,Output_3),axis=1)
slow_2_dataset=np.concatenate((Features_4,Output_4),axis=1)
vfast_dataset=np.concatenate((Features_5,Output_5),axis=1)
 
normal_dataset=normal_dataset_1[0:12000,:]


fast_dataset_o=np.concatenate((Features_6,Output_6),axis=1)
normal_dataset_o=np.concatenate((Features_7,Output_7),axis=1)
slow_dataset_o=np.concatenate((Features_8,Output_8),axis=1)
vfast_dataset_o=np.concatenate((Features_9,Output_9),axis=1)


slope_1_ascent=np.concatenate((Features_10,Output_10),axis=1)
slope_1_decent=np.concatenate((Features_11,Output_11),axis=1)
slope_2_ascent=np.concatenate((Features_12,Output_12),axis=1)
slope_2_decent=np.concatenate((Features_13,Output_13),axis=1)
stair_1_ascent=np.concatenate((Features_14,Output_14),axis=1)
stair_1_decent=np.concatenate((Features_15,Output_15),axis=1)
stair_2_ascent=np.concatenate((Features_16,Output_16),axis=1)
stair_2_decent=np.concatenate((Features_17,Output_17),axis=1)

 
subject_8_treadmill=np.concatenate((fast_dataset,normal_dataset,slow_1_dataset,vfast_dataset),axis=0)
subject_8_overground=np.concatenate((fast_dataset_o,slow_dataset_o,normal_dataset_o,vfast_dataset_o),axis=0)
subject_8_stair_slope=np.concatenate((slope_1_ascent,slope_1_decent,slope_2_ascent,slope_2_decent,stair_1_ascent,stair_1_decent,stair_2_ascent,stair_2_decent),axis=0)
 
 
 
subject_8=np.concatenate((subject_8_treadmill,subject_8_overground,subject_8_stair_slope),axis=0)
 
# subject_8=np.concatenate((subject_8_treadmill,subject_8_overground),axis=0)
 
# subject_8=subject_8_stair_slope

              ### Extra features ###

features_E_1=np.array([0,65.3,1.61,0.345,0.155,0.224,0.133,0.206,0.44,0.362,0.241,0.116,0.086,0.05104,0.031132,0.426])
features_E_1=features_E_1.repeat(subject_8.shape[0])
features_E_1=features_E_1.reshape(16,subject_8.shape[0])
features_E_1=features_E_1.transpose()

subject_8=np.concatenate((subject_8,features_E_1),axis=1)



### treadmill ###

### Original ###
features_E_1=np.array([0,65.3,1.61,0.345,0.155,0.224,0.133,0.206,0.44,0.362,0.241,0.116,0.086,0.05104,0.031132,0.426])
features_E_1=features_E_1.repeat(subject_8_treadmill.shape[0])
features_E_1=features_E_1.reshape(16,subject_8_treadmill.shape[0])
features_E_1=features_E_1.transpose()

subject_8_treadmill=np.concatenate((subject_8_treadmill,features_E_1),axis=1)

### overground ###

features_E_1=np.array([0,65.3,1.61,0.345,0.155,0.224,0.133,0.206,0.44,0.362,0.241,0.116,0.086,0.05104,0.031132,0.426])
features_E_1=features_E_1.repeat(subject_8_overground.shape[0])
features_E_1=features_E_1.reshape(16,subject_8_overground.shape[0])
features_E_1=features_E_1.transpose()

subject_8_overground=np.concatenate((subject_8_overground,features_E_1),axis=1)


### stair_slope ###

features_E_1=np.array([0,65.3,1.61,0.345,0.155,0.224,0.133,0.206,0.44,0.362,0.241,0.116,0.086,0.05104,0.031132,0.426])
features_E_1=features_E_1.repeat(subject_8_stair_slope.shape[0])
features_E_1=features_E_1.reshape(16,subject_8_stair_slope.shape[0])
features_E_1=features_E_1.transpose()

subject_8_stair_slope=np.concatenate((subject_8_stair_slope,features_E_1),axis=1)
 
 
print(subject_8.shape)

#### Subject 9 ###

Features_1= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_treadmill_fast_features.csv', delimiter=',')
Features_2= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_treadmill_normal_features.csv', delimiter=',')
Features_3= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_treadmill_slow_1_features.csv', delimiter=',')
Features_4= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_treadmill_slow_2_features.csv', delimiter=',')
Features_5= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_treadmill_vfast_features.csv', delimiter=',')

Features_6= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_overground_fast_features.csv', delimiter=',')
Features_7= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_overground_normal_features.csv', delimiter=',')
Features_8= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_overground_slow_features.csv', delimiter=',')
Features_9= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_overground_vfast_features.csv', delimiter=',')

Features_10= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_slope_1_features_ascent.csv', delimiter=',')
Features_11= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_slope_1_features_decent.csv', delimiter=',')
Features_12= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_slope_2_features_ascent.csv', delimiter=',')
Features_13= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_slope_2_features_decent.csv', delimiter=',')
Features_14= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_stair_1_features_ascent.csv', delimiter=',')
Features_15= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_stair_1_features_decent.csv', delimiter=',')
Features_16= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_stair_2_features_ascent.csv', delimiter=',')
Features_17= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_stair_2_features_decent.csv', delimiter=',')


 
Output_1= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_treadmill_fast_v3.csv', delimiter=',')
Output_2= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_treadmill_normal_v3.csv', delimiter=',')
Output_3= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_treadmill_slow_1_v3.csv', delimiter=',')
Output_4= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_treadmill_slow_2_v3.csv', delimiter=',')
Output_5= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_treadmill_vfast_v3.csv', delimiter=',')

## output ##

Output_6= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_overground_fast.csv', delimiter=',')
Output_7= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_overground_normal.csv', delimiter=',')
Output_8= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_overground_slow.csv', delimiter=',')
Output_9= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_overground_vfast.csv', delimiter=',')

Output_10= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_slope_1_ascent.csv', delimiter=',')
Output_11= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_slope_1_decent.csv', delimiter=',')
Output_12= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_slope_2_ascent.csv', delimiter=',')
Output_13= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_slope_2_decent.csv', delimiter=',')
Output_14= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_stair_1_ascent.csv', delimiter=',')
Output_15= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_stair_1_decent.csv', delimiter=',')
Output_16= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_stair_2_ascent.csv', delimiter=',')
Output_17= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T009_stair_2_decent.csv', delimiter=',')


 
fast_dataset=np.concatenate((Features_1,Output_1),axis=1)
normal_dataset=np.concatenate((Features_2,Output_2),axis=1)
slow_1_dataset=np.concatenate((Features_3,Output_3),axis=1)
slow_2_dataset=np.concatenate((Features_4,Output_4),axis=1)
vfast_dataset_1=np.concatenate((Features_5,Output_5),axis=1)
vfast_dataset=vfast_dataset_1[0:3000,:]
 


fast_dataset_o=np.concatenate((Features_6,Output_6),axis=1)
normal_dataset_o=np.concatenate((Features_7,Output_7),axis=1)
slow_dataset_o=np.concatenate((Features_8,Output_8),axis=1)
vfast_dataset_o=np.concatenate((Features_9,Output_9),axis=1)


slope_1_ascent=np.concatenate((Features_10,Output_10),axis=1)
slope_1_decent=np.concatenate((Features_11,Output_11),axis=1)
slope_2_ascent=np.concatenate((Features_12,Output_12),axis=1)
slope_2_decent=np.concatenate((Features_13,Output_13),axis=1)
stair_1_ascent=np.concatenate((Features_14,Output_14),axis=1)
stair_1_decent=np.concatenate((Features_15,Output_15),axis=1)
stair_2_ascent=np.concatenate((Features_16,Output_16),axis=1)
stair_2_decent=np.concatenate((Features_17,Output_17),axis=1)



 
 
 
subject_9_treadmill=np.concatenate((fast_dataset,normal_dataset,slow_1_dataset,vfast_dataset),axis=0)
subject_9_overground=np.concatenate((fast_dataset_o,slow_dataset_o,normal_dataset_o,vfast_dataset_o),axis=0)
subject_9_stair_slope=np.concatenate((slope_1_ascent,slope_1_decent,slope_2_ascent,slope_2_decent,stair_1_ascent,stair_1_decent,stair_2_ascent,stair_2_decent),axis=0)
 
#subject_9_stair_slope=np.concatenate((stair_1_ascent,stair_1_decent,stair_2_ascent,stair_2_decent),axis=0)
 
subject_9=np.concatenate((subject_9_treadmill,subject_9_overground,subject_9_stair_slope),axis=0)
 
# subject_9=np.concatenate((subject_9_treadmill,subject_9_overground),axis=0)

# subject_9=subject_9_stair_slope

              ### Extra features ###

features_E_1=np.array([0,58,1.6,0.262,0.165,0.222,0.125,0.19,0.433,0.337,0.24,0.118,0.072,0.051094,0.024264,0.409])
features_E_1=features_E_1.repeat(subject_9.shape[0])
features_E_1=features_E_1.reshape(16,subject_9.shape[0])
features_E_1=features_E_1.transpose()

subject_9=np.concatenate((subject_9,features_E_1),axis=1)


### treadmill ###

features_E_1=np.array([0,58,1.6,0.262,0.165,0.222,0.125,0.19,0.433,0.337,0.24,0.118,0.072,0.051094,0.024264,0.409])
features_E_1=features_E_1.repeat(subject_9_treadmill.shape[0])
features_E_1=features_E_1.reshape(16,subject_9_treadmill.shape[0])
features_E_1=features_E_1.transpose()

subject_9_treadmill=np.concatenate((subject_9_treadmill,features_E_1),axis=1)

### overground ###

features_E_1=np.array([0,58,1.6,0.262,0.165,0.222,0.125,0.19,0.433,0.337,0.24,0.118,0.072,0.051094,0.024264,0.409])
features_E_1=features_E_1.repeat(subject_9_overground.shape[0])
features_E_1=features_E_1.reshape(16,subject_9_overground.shape[0])
features_E_1=features_E_1.transpose()

subject_9_overground=np.concatenate((subject_9_overground,features_E_1),axis=1)


### stair_slope ###

features_E_1=np.array([0,58,1.6,0.262,0.165,0.222,0.125,0.19,0.433,0.337,0.24,0.118,0.072,0.051094,0.024264,0.409])
features_E_1=features_E_1.repeat(subject_9_stair_slope.shape[0])
features_E_1=features_E_1.reshape(16,subject_9_stair_slope.shape[0])
features_E_1=features_E_1.transpose()

subject_9_stair_slope=np.concatenate((subject_9_stair_slope,features_E_1),axis=1)
 
print(subject_9.shape)

#### Subject 10 ###

Features_1= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_treadmill_fast_features_v1.csv', delimiter=',')
Features_2= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_treadmill_normal_features_v1.csv', delimiter=',')
Features_3= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_treadmill_slow_1_features_v1.csv', delimiter=',')
Features_4= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_treadmill_slow_2_features_v1.csv', delimiter=',')
Features_5= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_treadmill_vfast_features_v1.csv', delimiter=',')

Features_6= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_overground_fast_features.csv', delimiter=',')
Features_7= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_overground_normal_features.csv', delimiter=',')
Features_8= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_overground_slow_features.csv', delimiter=',')
Features_9= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_overground_vfast_features.csv', delimiter=',')

Features_10= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_slope_1_features_ascent.csv', delimiter=',')
Features_11= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_slope_1_features_decent.csv', delimiter=',')
Features_12= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_slope_2_features_ascent.csv', delimiter=',')
Features_13= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_slope_2_features_decent.csv', delimiter=',')
Features_14= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_stair_1_features_ascent.csv', delimiter=',')
Features_15= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_stair_1_features_decent.csv', delimiter=',')
Features_16= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_stair_2_features_ascent.csv', delimiter=',')
Features_17= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_stair_2_features_decent.csv', delimiter=',')


 
Output_1= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_treadmill_fast_v6.csv', delimiter=',')
Output_2= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_treadmill_normal_v6.csv', delimiter=',')
Output_3= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_treadmill_slow_1_v6.csv', delimiter=',')
Output_4= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_treadmill_slow_2_v6.csv', delimiter=',')
Output_5= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_treadmill_vfast_v6.csv', delimiter=',')

## output ##

Output_6= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_overground_fast.csv', delimiter=',')
Output_7= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_overground_normal.csv', delimiter=',')
Output_8= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_overground_slow.csv', delimiter=',')
Output_9= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_overground_vfast.csv', delimiter=',')

Output_10= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_slope_1_ascent.csv', delimiter=',')
Output_11= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_slope_1_decent.csv', delimiter=',')
Output_12= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_slope_2_ascent.csv', delimiter=',')
Output_13= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_slope_2_decent.csv', delimiter=',')
Output_14= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_stair_1_ascent.csv', delimiter=',')
Output_15= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_stair_1_decent.csv', delimiter=',')
Output_16= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_stair_2_ascent.csv', delimiter=',')
Output_17= loadtxt('/home/sanzidpr/DeepBBWAE_Net/P002_T010_stair_2_decent.csv', delimiter=',')


 
fast_dataset=np.concatenate((Features_1,Output_1),axis=1)
normal_dataset=np.concatenate((Features_2,Output_2),axis=1)
slow_1_dataset=np.concatenate((Features_3,Output_3),axis=1)
slow_2_dataset=np.concatenate((Features_4,Output_4),axis=1)
vfast_dataset=np.concatenate((Features_5,Output_5),axis=1)
 


fast_dataset_o=np.concatenate((Features_6,Output_6),axis=1)
normal_dataset_o=np.concatenate((Features_7,Output_7),axis=1)
slow_dataset_o=np.concatenate((Features_8,Output_8),axis=1)
vfast_dataset_o=np.concatenate((Features_9,Output_9),axis=1)


slope_1_ascent=np.concatenate((Features_10,Output_10),axis=1)
slope_1_decent=np.concatenate((Features_11,Output_11),axis=1)
slope_2_ascent=np.concatenate((Features_12,Output_12),axis=1)
slope_2_decent=np.concatenate((Features_13,Output_13),axis=1)
stair_1_ascent=np.concatenate((Features_14,Output_14),axis=1)
stair_1_decent=np.concatenate((Features_15,Output_15),axis=1)
stair_2_ascent=np.concatenate((Features_16,Output_16),axis=1)
stair_2_decent=np.concatenate((Features_17,Output_17),axis=1)



 
 
 
subject_10_treadmill=np.concatenate((fast_dataset,normal_dataset,slow_2_dataset,vfast_dataset),axis=0)
subject_10_overground=np.concatenate((fast_dataset_o,slow_dataset_o,normal_dataset_o,vfast_dataset_o),axis=0)
subject_10_stair_slope=np.concatenate((slope_1_ascent,slope_1_decent,slope_2_ascent,slope_2_decent,stair_1_ascent,stair_1_decent,stair_2_ascent,stair_2_decent),axis=0)
 
 
subject_10=np.concatenate((subject_10_treadmill,subject_10_overground,subject_10_stair_slope),axis=0)
 
# subject_10=np.concatenate((subject_10_treadmill,subject_10_overground),axis=0)
# subject_10=subject_10_stair_slope


features_E_1=np.array([0,59.69,1.6,0.281,0.155,0.251,0.152,0.194,0.412,0.357,0.236,0.116,0.076,0.047792,0.027132,0.403])
features_E_1=features_E_1.repeat(subject_10.shape[0])
features_E_1=features_E_1.reshape(16,subject_10.shape[0])
features_E_1=features_E_1.transpose()

subject_10=np.concatenate((subject_10,features_E_1),axis=1)


### treadmill ###

features_E_1=np.array([0,59.69,1.6,0.281,0.155,0.251,0.152,0.194,0.412,0.357,0.236,0.116,0.076,0.047792,0.027132,0.403])
features_E_1=features_E_1.repeat(subject_10_treadmill.shape[0])
features_E_1=features_E_1.reshape(16,subject_10_treadmill.shape[0])
features_E_1=features_E_1.transpose()

subject_10_treadmill=np.concatenate((subject_10_treadmill,features_E_1),axis=1)

### overground ###

features_E_1=np.array([0,59.69,1.6,0.281,0.155,0.251,0.152,0.194,0.412,0.357,0.236,0.116,0.076,0.047792,0.027132,0.403])
features_E_1=features_E_1.repeat(subject_10_overground.shape[0])
features_E_1=features_E_1.reshape(16,subject_10_overground.shape[0])
features_E_1=features_E_1.transpose()

subject_10_overground=np.concatenate((subject_10_overground,features_E_1),axis=1)


### stair_slope ###

features_E_1=np.array([0,59.69,1.6,0.281,0.155,0.251,0.152,0.194,0.412,0.357,0.236,0.116,0.076,0.047792,0.027132,0.403])
features_E_1=features_E_1.repeat(subject_10_stair_slope.shape[0])
features_E_1=features_E_1.reshape(16,subject_10_stair_slope.shape[0])
features_E_1=features_E_1.transpose()

subject_10_stair_slope=np.concatenate((subject_10_stair_slope,features_E_1),axis=1)

 
print(subject_10.shape)

import gc
gc.collect()

treadmill_dataset=np.concatenate((subject_2_treadmill,subject_3_treadmill,subject_4_treadmill,subject_5_treadmill,subject_6_treadmill,subject_7_treadmill,subject_8_treadmill,subject_9_treadmill,subject_10_treadmill,),axis=0)
overground_dataset=np.concatenate((subject_2_overground,subject_3_overground,subject_4_overground,subject_5_overground,subject_6_overground,subject_7_overground,subject_8_overground,subject_9_overground,subject_10_overground,),axis=0)
stair_slope_dataset=np.concatenate((subject_2_stair_slope,subject_3_stair_slope,subject_4_stair_slope,subject_5_stair_slope,subject_6_stair_slope,subject_7_stair_slope,subject_8_stair_slope,subject_9_stair_slope,subject_10_stair_slope,),axis=0)

# train_dataset=np.concatenate((treadmill_dataset,overground_dataset,overground_dataset,overground_dataset,stair_slope_dataset,stair_slope_dataset,stair_slope_dataset,stair_slope_dataset),axis=0)
train_dataset=np.concatenate((treadmill_dataset,overground_dataset,stair_slope_dataset),axis=0)

test_dataset=subject_1

print(test_dataset.shape)

# Sensor 1- Sternum
# Sensor 2-Sacrum
# Sensor 3-R_thigh
# Sensor 4-L_thigh
# Sensor 5-R_shank
# Sensor 6-L_shank
# Sensor 7-R_dorsal
# Sensor 8-L_dorsal
 
 
# Train features #
train_1=train_dataset[:,0:8]
train_2=train_dataset[:,17:25]
train_3=train_dataset[:,34:42]
train_4=train_dataset[:,51:59]
train_5=train_dataset[:,68:76]
train_6=train_dataset[:,85:93]
train_7=train_dataset[:,102:110]
train_8=train_dataset[:,119:127]




train_7_1=train_dataset[:,102:105]
train_7_2=train_dataset[:,106:109]

train_8_1=train_dataset[:,119:122]
train_8_2=train_dataset[:,123:126]


## Extra Features

train_extra=train_dataset[:,160:176]
 
#x_1=train_4
 
x_train=np.concatenate((train_7_1,train_7_2,train_8_1,train_8_2),axis=1)
 
# x_train=np.concatenate((train_7,train_8,train_extra),axis=1)
# x_train=np.concatenate((train_2,train_3,train_4,train_5,train_6,train_7,train_8,train_extra),axis=1)
 
#x_train=train_extra
 
 
 
scaler = MinMaxScaler(feature_range=(0, 1))
 
 
#train_X_1=scaler.fit_transform(x_train)
 
 
train_X_1=x_train
 
 
 
 
# Test features #
 
test_1=test_dataset[:,0:8]
test_2=test_dataset[:,17:25]
test_3=test_dataset[:,34:42]
test_4=test_dataset[:,51:59]
test_5=test_dataset[:,68:76]
test_6=test_dataset[:,85:93]
test_7=test_dataset[:,102:110]
test_8=test_dataset[:,119:127]
 



test_7_1=test_dataset[:,102:105]
test_7_2=test_dataset[:,106:109]

test_8_1=test_dataset[:,119:122]
test_8_2=test_dataset[:,123:126]

   ### Extra features  ###
  
test_extra=test_dataset[:,160:176]
 
x_test=np.concatenate((test_7_1,test_7_2,test_8_1,test_8_2),axis=1)

 
# x_test=np.concatenate((test_7,test_8,test_extra),axis=1)
# x_test=np.concatenate((test_2,test_3,test_4,test_5,test_6,test_7,test_8,test_extra),axis=1)
 
#x_test=test_extra
 
 
#test_X_1=scaler.fit_transform(x_test)
 
test_X_1=x_test
 
 
 
    
f=88
 
y_1_1=train_dataset[:,(f+55):(f+56)]
y_1_2=train_dataset[:,(f+58):(f+60)]
y_1_3=train_dataset[:,(f+62):(f+63)]
y_1_4=train_dataset[:,(f+65):(f+67)]
 
 
 
#train_y_1= np.concatenate((y_1_R,y_1_R_1,y_1_L,y_1_L_1),axis=1)     # 1-horizontal, 0-vertical---main
 

train_y_1=np.concatenate((y_1_1,y_1_2,y_1_3,y_1_4),axis=1)
 

 
y_2_1=test_dataset[:,(f+55):(f+56)]
y_2_2=test_dataset[:,(f+58):(f+60)]
y_2_3=test_dataset[:,(f+62):(f+63)]
y_2_4=test_dataset[:,(f+65):(f+67)]


 
test_y_1= np.concatenate((y_2_1,y_2_2,y_2_3,y_2_4),axis=1)

#test_y_1= np.concatenate((y_2_1,y_2_2),axis=1)
# test_y_1=y_2_1
 
 
print(train_X_1.shape,test_X_1.shape,train_y_1.shape,test_y_1.shape)

print(test_extra.shape)

"""# Data creation"""

# # convert an array of values into a dataset matrix
def create_dataset_present(dataset_1, window=100):
  dataX= []
  k=0
  w=100
  shift=50
  for i in range(int(len(dataset_1)/shift-8)):
    j=shift*k
    a = dataset_1[j:j+window,:]
    # print(a.shape)
    dataX.append(a)
    k=k+1
  return np.array(dataX)

# # convert an array of values into a dataset matrix
def create_dataset_future(dataset_1, window=100):
  dataX= []
  k=0
  w=100
  shift=50
  for i in range(int(len(dataset_1)/shift-9)):
      j=shift*k
      a = dataset_1[j+window:j+window+shift,:]
      dataX.append(a)
      k=k+1
  return np.array(dataX)

import gc
gc.collect()

### Reconstruction/Present Dataset ###
w=100

train_X_3=create_dataset_present(train_X_1,w)
train_y_3=create_dataset_present(train_y_1,w)

test_X_1D=create_dataset_present(test_X_1,w)
test_y=create_dataset_present(test_y_1,w)

print(test_y.shape)

### Future Prediction Dataset ###


train_y_3_f=create_dataset_future(train_y_1,w)
test_y_f=create_dataset_future(test_y_1,w)

print(train_y_3_f.shape)

train_y_3_f=np.concatenate((train_y_3_f[0:len(train_y_3_f),:,:],train_y_3_f[len(train_y_3_f)-1:len(train_y_3_f),:,:]),axis=0)
test_y_f=np.concatenate((test_y_f[0:len(test_y_f),:,:],test_y_f[len(test_y_f)-1:len(test_y_f),:,:]),axis=0)

print(train_y_3.shape,train_y_3_f.shape)

w=100
w_f=50

train_y_3=train_y_3.reshape(train_y_3.shape[0],w*6)
test_y=test_y.reshape(test_y.shape[0],w*6)


train_y_3_f=train_y_3_f.reshape(train_y_3_f.shape[0],w_f*6)
test_y_f=test_y_f.reshape(test_y_f.shape[0],w_f*6)

print(train_X_1.shape,test_X_1.shape,train_y_1.shape,test_y_1.shape)

# train_X_1D, X_validation_1D, train_y_5, Y_validation = train_test_split(train_X_3,train_y_3, test_size=0.20, random_state=123)
# train_y_5_f, Y_validation_f = train_test_split(train_y_3_f, test_size=0.20, random_state=123)

# print(train_X_1D.shape,train_y_5.shape,X_validation_1D.shape,Y_validation.shape)

train_X_1D, X_validation_1D, train_y_5, Y_validation = train_test_split(train_X_3,train_y_3, test_size=0.20, random_state=True)

# train_X_extra=train_X_1D[:,0,12:28]
# X_validation_extra=X_validation_1D[:,0,12:28]
# test_X_extra=test_X_1D[:,0,12:28]

train_X_1D=train_X_1D[:,:,0:12]
X_validation_1D=X_validation_1D[:,:,0:12]
test_X_1D=test_X_1D[:,:,0:12]
train_y_5_f, Y_validation_f = train_test_split(train_y_3_f, test_size=0.20, random_state=True)

print(train_X_1D.shape,train_y_5.shape,X_validation_1D.shape,Y_validation.shape)
# print(train_X_extra.shape,X_validation_extra.shape,test_X_extra.shape)

# train_X_1D, X_validation_1D, train_y_5, Y_validation = train_test_split(train_X_3,train_y_3, test_size=0.20, random_state=123)

# train_X_extra=train_X_1D[:,0,56:62]
# X_validation_extra=X_validation_1D[:,0,56:62]
# test_X_extra=test_X_1D[:,0,56:62]

# train_X_1D=train_X_1D[:,:,0:56]
# X_validation_1D=X_validation_1D[:,:,0:56]
# test_X_1D=test_X_1D[:,:,0:56]
# train_y_5_f, Y_validation_f = train_test_split(train_y_3_f, test_size=0.20, random_state=123)

# print(train_X_1D.shape,train_y_5.shape,X_validation_1D.shape,Y_validation.shape)
# print(train_X_extra.shape,X_validation_extra.shape,test_X_extra.shape)

features=6

train_X_2D=train_X_1D[:,:,0:12].reshape(train_X_1D.shape[0],train_X_1D.shape[1],features,2)
test_X_2D=test_X_1D[:,:,0:12].reshape(test_X_1D.shape[0],test_X_1D.shape[1],features,2)
X_validation_2D= X_validation_1D[:,:,0:12].reshape(X_validation_1D.shape[0],
                                                   X_validation_1D.shape[1],features,2)
#X_validation_2D_ridge= X_validation_1D_ridge.reshape(X_validation_1D_ridge.shape[0],X_validation_1D_ridge.shape[1],8,2)


print(train_X_2D.shape,test_X_2D.shape,X_validation_2D.shape)

# features=6

# train_X_2D=train_X_1D.reshape(train_X_1D.shape[0],train_X_1D.shape[1],features,2)
# test_X_2D=test_X_1D.reshape(test_X_1D.shape[0],test_X_1D.shape[1],features,2)
# X_validation_2D= X_validation_1D.reshape(X_validation_1D.shape[0],X_validation_1D.shape[1],features,2)
# #X_validation_2D_ridge= X_validation_1D_ridge.reshape(X_validation_1D_ridge.shape[0],X_validation_1D_ridge.shape[1],8,2)


# print(train_X_2D.shape,test_X_2D.shape,X_validation_2D.shape)

Bag_samples=train_X_2D.shape[0]
print(Bag_samples)

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

Bag_samples=train_X_2D.shape[0]
print(Bag_samples)

import gc
gc.collect()
gc.collect()
gc.collect()

"""# Different Function for models"""

from sklearn.model_selection import KFold
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.multioutput import MultiOutputRegressor
import pickle
from sklearn.linear_model import Ridge
from sklearn.utils import resample

def prediction_test(yhat,test_y_up):

    test_o=test_y_up
    yhat=yhat
    
    y_1_no=yhat[:,0]
    y_2_no=yhat[:,1]
    y_3_no=yhat[:,2]
    y_4_no=yhat[:,3]
    y_5_no=yhat[:,4]
    y_6_no=yhat[:,5]

    
    y_test_1=test_o[:,0]
    y_test_2=test_o[:,1]
    y_test_3=test_o[:,2]
    y_test_4=test_o[:,3]
    y_test_5=test_o[:,4]
    y_test_6=test_o[:,5]

    
    cutoff=6
    fs=100
    order=2
    
    nyq = 0.5 * fs
    ## filtering data ##
    def butter_lowpass_filter(data, cutoff, fs, order):
        normal_cutoff = cutoff / nyq
        # Get the filter coefficients 
        b, a = butter(order, normal_cutoff, btype='low', analog=False)
        y = filtfilt(b, a, data)
        return y
    
    
    
    y_1= butter_lowpass_filter(y_1_no, cutoff, fs, order)
    y_2= butter_lowpass_filter(y_2_no, cutoff, fs, order)
    y_3= butter_lowpass_filter(y_3_no, cutoff, fs, order)
    y_4= butter_lowpass_filter(y_4_no, cutoff, fs, order)
    y_5= butter_lowpass_filter(y_5_no, cutoff, fs, order)
    y_6= butter_lowpass_filter(y_6_no, cutoff, fs, order)


    
    ###calculate RMSE
    
    rmse_1 =np.sqrt(mean_squared_error(y_test_1,y_1))
    rmse_2 =np.sqrt(mean_squared_error(y_test_2,y_2))
    rmse_3 =np.sqrt(mean_squared_error(y_test_3,y_3))
    rmse_4 =np.sqrt(mean_squared_error(y_test_4,y_4))
    rmse_5 =np.sqrt(mean_squared_error(y_test_5,y_5))
    rmse_6 =np.sqrt(mean_squared_error(y_test_6,y_6))

    
    p_1=np.corrcoef(y_1, y_test_1)[0, 1]
    p_2=np.corrcoef(y_2, y_test_2)[0, 1]
    p_3=np.corrcoef(y_3, y_test_3)[0, 1]
    p_4=np.corrcoef(y_4, y_test_4)[0, 1]
    p_5=np.corrcoef(y_5, y_test_5)[0, 1]
    p_6=np.corrcoef(y_6, y_test_6)[0, 1]
 

    p=np.array([(p_1+p_4)/2,(p_2+p_5)/2,(p_3+p_6)/2])

    rmse=np.array([(rmse_1+rmse_4)/2,(rmse_2+rmse_5)/2,(rmse_3+rmse_6)/2])

   
    
    return rmse,p

# # convert an array of values into a dataset matrix
def unpack_dataset_present(dataset_1, window=80):
  dataX= []
  k=1
  w=100
  l=0
  shift=100
  for i in range(10000):
    j=shift*k
    a = dataset_1[l:j,:]
    # print(a.shape)
    l=0
    l=j+50
    dataX=np.append(dataX,a)
    k=k+1
    j=0
  return np.array(dataX)

"""#1. GRU-Net"""

def GRU_Net(inputs_1D_N,inputs_2D_N):

  model_1=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_1=Dropout(0.5)(model_1)
  model_1=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
  model_1=Dropout(0.5)(model_1)
  model_1=Flatten()(model_1)

  model_2=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_2=Dropout(0.4)(model_2)
  model_2=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_2)
  model_2=Dropout(0.4)(model_2)
  model_2=Flatten()(model_2)

  X=Conv2D(64, (3, 3), activation='gelu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(128, (3, 3), activation='gelu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)


  X=Dense(64, activation='relu')(X)
  
  X=Dropout(0.2)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.2)(X)

  X_1=Flatten()(X)
  X= concatenate([model_2,X_1])


  model_3=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_3=Dropout(0.4)(model_3)
  model_3=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_3)
  model_3=Dropout(0.4)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)


  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.2)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.2)(CNN)

  CNN_1=Flatten()(CNN)


  CNN= concatenate([model_3,CNN_1])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)


  
  output = Average()([output_GRU,output_C2,output_C1])
  
  Feat_f= concatenate([model_1,X,CNN])

  return output_GRU

"""# 2. Conv1D-Net"""

def Conv1D_Net(inputs_1D_N,inputs_2D_N):

  model_1=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_1=Dropout(0.4)(model_1)
  model_1=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
  model_1=Dropout(0.4)(model_1)
  model_1=Flatten()(model_1)

  model_2=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_2=Dropout(0.4)(model_2)
  model_2=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_2)
  model_2=Dropout(0.4)(model_2)
  model_2=Flatten()(model_2)

  X=Conv2D(64, (3, 3), activation='gelu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(128, (3, 3), activation='gelu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)


  X=Dense(128, activation='relu')(X)
  
  X=Dropout(0.5)(X)
  X=Dense(64,activation='relu')(X)
  X=Dropout(0.5)(X)

  X_1=Flatten()(X)
  X= concatenate([model_2,X_1])


  model_3=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_3=Dropout(0.5)(model_3)
  model_3=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_3)
  model_3=Dropout(0.5)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)


  CNN=Dense(128, activation='relu')(CNN)
  CNN=Dropout(0.5)(CNN)
  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.5)(CNN)

  CNN=Flatten()(CNN)


  #CNN= concatenate([model_3,CNN_1])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)


  
  output = Average()([output_GRU,output_C2,output_C1])
  
  Feat_f= concatenate([model_1,X,CNN])

  return output_C1

"""# 3. Conv2D-Net"""

def Conv2D_Net(inputs_1D_N,inputs_2D_N):

  model_1=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_1=Dropout(0.4)(model_1)
  model_1=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
  model_1=Dropout(0.4)(model_1)
  model_1=Flatten()(model_1)

  model_2=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_2=Dropout(0.5)(model_2)
  model_2=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_2)
  model_2=Dropout(0.5)(model_2)
  model_2=Flatten()(model_2)

  X=Conv2D(64, (3, 3), activation='gelu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(128, (3, 3), activation='gelu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)


  X=Dense(128, activation='relu')(X)
  
  X=Dropout(0.5)(X)
  X=Dense(64,activation='relu')(X)
  X=Dropout(0.5)(X)

  X=Flatten()(X)
  #X= concatenate([model_2,X_1])


  model_3=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_3=Dropout(0.4)(model_3)
  model_3=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_3)
  model_3=Dropout(0.4)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)


  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.2)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.2)(CNN)

  CNN_1=Flatten()(CNN)


  CNN= concatenate([model_3,CNN_1])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)


  
  output = Average()([output_GRU,output_C2,output_C1])
  
  Feat_f= concatenate([model_1,X,CNN])

  return output_C2

"""#4. GRU-Conv1D-Net"""

def GRU_Conv1D_Net(inputs_1D_N,inputs_2D_N):

  model_1=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_1=Dropout(0.4)(model_1)
  model_1=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
  model_1=Dropout(0.4)(model_1)
  model_1=Flatten()(model_1)

  model_2=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_2=Dropout(0.4)(model_2)
  model_2=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_2)
  model_2=Dropout(0.4)(model_2)
  model_2=Flatten()(model_2)

  X=Conv2D(64, (3, 3), activation='gelu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(128, (3, 3), activation='gelu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)


  X=Dense(64, activation='relu')(X)
  
  X=Dropout(0.2)(X)
  X=Dense(32,activation='relu')(X)
  X=Dropout(0.2)(X)

  X_1=Flatten()(X)
  X= concatenate([model_2,X_1])


  model_3=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_3=Dropout(0.5)(model_3)
  model_3=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_3)
  model_3=Dropout(0.5)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)


  CNN=Dense(128, activation='relu')(CNN)
  CNN=Dropout(0.5)(CNN)
  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.5)(CNN)

  CNN_1=Flatten()(CNN)


  CNN= concatenate([model_3,CNN_1])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)


  
  output = Average()([output_GRU,output_C2,output_C1])
  
  Feat_f= concatenate([model_1,X,CNN])

  return output_C1

"""#5. GRU-Conv2D-Net"""

def GRU_Conv2D_Net(inputs_1D_N,inputs_2D_N):

  model_1=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_1=Dropout(0.4)(model_1)
  model_1=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
  model_1=Dropout(0.4)(model_1)
  model_1=Flatten()(model_1)

  model_2=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_2=Dropout(0.5)(model_2)
  model_2=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_2)
  model_2=Dropout(0.5)(model_2)
  model_2=Flatten()(model_2)

  X=Conv2D(64, (3, 3), activation='gelu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(128, (3, 3), activation='gelu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)


  X=Dense(128, activation='relu')(X)
  
  X=Dropout(0.5)(X)
  X=Dense(64,activation='relu')(X)
  X=Dropout(0.5)(X)

  X_1=Flatten()(X)
  X= concatenate([model_2,X_1])


  model_3=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_3=Dropout(0.4)(model_3)
  model_3=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_3)
  model_3=Dropout(0.4)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)


  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.2)(CNN)
  CNN=Dense(32, activation='relu')(CNN)
  CNN=Dropout(0.2)(CNN)

  CNN_1=Flatten()(CNN)


  CNN= concatenate([model_3,CNN_1])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)


  
  output = Average()([output_GRU,output_C2,output_C1])
  
  Feat_f= concatenate([model_1,X,CNN])

  return output_C2

"""#6. Conv1D-Conv2D-Net"""

def Conv1D_Conv2D_Net(inputs_1D_N,inputs_2D_N):

  model_1=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_1=Dropout(0.4)(model_1)
  model_1=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
  model_1=Dropout(0.4)(model_1)
  model_1=Flatten()(model_1)

  model_2=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_2=Dropout(0.4)(model_2)
  model_2=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_2)
  model_2=Dropout(0.4)(model_2)
  model_2=Flatten()(model_2)

  X=Conv2D(64, (3, 3), activation='gelu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(128, (3, 3), activation='gelu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)


  X=Dense(128, activation='relu')(X)
  
  X=Dropout(0.5)(X)
  X=Dense(64,activation='relu')(X)
  X=Dropout(0.5)(X)

  X_1=Flatten()(X)
  #X= concatenate([model_2,X_1])


  model_3=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_3=Dropout(0.4)(model_3)
  model_3=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_3)
  model_3=Dropout(0.4)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)


  CNN=Dense(128, activation='relu')(CNN)
  CNN=Dropout(0.5)(CNN)
  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.5)(CNN)

  CNN_1=Flatten()(CNN)


  CNN= concatenate([X_1,CNN_1])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)


  
  output = Average()([output_GRU,output_C2,output_C1])
  

  return output_C1

"""#7. Gait-SubNet-1"""

def Gait_SubNet_1(inputs_1D_N,inputs_2D_N):

  model_1=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_1=Dropout(0.5)(model_1)
  model_1=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
  model_1=Dropout(0.5)(model_1)
  model_1=Flatten()(model_1)

  model_2=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_2=Dropout(0.4)(model_2)
  model_2=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_2)
  model_2=Dropout(0.4)(model_2)
  model_2=Flatten()(model_2)

  X=Conv2D(64, (3, 3), activation='gelu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(128, (3, 3), activation='gelu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)


  X=Dense(128, activation='relu')(X)
  
  X=Dropout(0.5)(X)
  X=Dense(64,activation='relu')(X)
  X=Dropout(0.5)(X)

  X_1=Flatten()(X)
  X= concatenate([model_2,X_1])


  model_3=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_3=Dropout(0.5)(model_3)
  model_3=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_3)
  model_3=Dropout(0.5)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)


  CNN=Dense(128, activation='relu')(CNN)
  CNN=Dropout(0.5)(CNN)
  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.5)(CNN)

  CNN_1=Flatten()(CNN)


  CNN= concatenate([model_3,CNN_1])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)


  
  output = Average()([output_GRU,output_C1])
  

  return (output_GRU,output_C1,output)

"""#8. Gait-SubNet-2"""

def Gait_SubNet_2(inputs_1D_N,inputs_2D_N):

  model_1=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_1=Dropout(0.5)(model_1)
  model_1=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
  model_1=Dropout(0.5)(model_1)
  model_1=Flatten()(model_1)

  model_2=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_2=Dropout(0.5)(model_2)
  model_2=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_2)
  model_2=Dropout(0.5)(model_2)
  model_2=Flatten()(model_2)

  X=Conv2D(64, (3, 3), activation='gelu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(128, (3, 3), activation='gelu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)


  X=Dense(128, activation='relu')(X)
  
  X=Dropout(0.5)(X)
  X=Dense(64,activation='relu')(X)
  X=Dropout(0.5)(X)

  X_1=Flatten()(X)
  X= concatenate([model_2,X_1])


  model_3=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_3=Dropout(0.4)(model_3)
  model_3=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_3)
  model_3=Dropout(0.4)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)


  CNN=Dense(128, activation='relu')(CNN)
  CNN=Dropout(0.5)(CNN)
  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.5)(CNN)

  CNN_1=Flatten()(CNN)


  CNN= concatenate([model_3,CNN_1])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)


  
  output = Average()([output_GRU,output_C2])
  

  return (output_GRU,output_C2,output)

"""#9. Gait-SubNet-3"""

def Gait_SubNet_3(inputs_1D_N,inputs_2D_N):

  model_1=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_1=Dropout(0.4)(model_1)
  model_1=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
  model_1=Dropout(0.4)(model_1)
  model_1=Flatten()(model_1)

  model_2=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_2=Dropout(0.5)(model_2)
  model_2=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_2)
  model_2=Dropout(0.5)(model_2)
  model_2=Flatten()(model_2)

  X=Conv2D(64, (3, 3), activation='gelu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(128, (3, 3), activation='gelu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)


  X=Dense(128, activation='relu')(X)
  
  X=Dropout(0.5)(X)
  X=Dense(64,activation='relu')(X)
  X=Dropout(0.5)(X)

  X_1=Flatten()(X)
  X= concatenate([model_2,X_1])


  model_3=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_3=Dropout(0.5)(model_3)
  model_3=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_3)
  model_3=Dropout(0.5)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)


  CNN=Dense(128, activation='relu')(CNN)
  CNN=Dropout(0.5)(CNN)
  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.5)(CNN)

  CNN_1=Flatten()(CNN)


  CNN= concatenate([model_3,CNN_1])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)


  
  output = Average()([output_C2,output_C1])
  

  return (output_C1,output_C2,output)

"""#10. Gait-Net"""

def Gait_Net(inputs_1D_N,inputs_2D_N):

  model_1=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_1=Dropout(0.5)(model_1)
  model_1=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_1)
  model_1=Dropout(0.5)(model_1)
  model_1=Flatten()(model_1)

  model_2=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_2=Dropout(0.5)(model_2)
  model_2=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_2)
  model_2=Dropout(0.5)(model_2)
  model_2=Flatten()(model_2)

  X=Conv2D(64, (3, 3), activation='gelu',padding='same')(inputs_2D_N)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)
  X=Conv2D(128, (3, 3), activation='gelu', padding='same')(X)
  X=BatchNormalization()(X)
  X=MaxPooling2D((2, 2))(X)


  X=Dense(128, activation='relu')(X)
  
  X=Dropout(0.5)(X)
  X=Dense(64,activation='relu')(X)
  X=Dropout(0.5)(X)

  X_1=Flatten()(X)
  X= concatenate([model_2,X_1])


  model_3=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(inputs_1D_N)
  model_3=Dropout(0.5)(model_3)
  model_3=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_3)
  model_3=Dropout(0.5)(model_3)
  model_3=Flatten()(model_3)

  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)
  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)
  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)
  CNN=BatchNormalization()(CNN)
  CNN=MaxPooling1D(pool_size=2)(CNN)


  CNN=Dense(128, activation='relu')(CNN)
  CNN=Dropout(0.5)(CNN)
  CNN=Dense(64, activation='relu')(CNN)
  CNN=Dropout(0.5)(CNN)

  CNN_1=Flatten()(CNN)


  CNN= concatenate([model_3,CNN_1])

  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)
  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)
  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)


  
  output = Average()([output_GRU,output_C2,output_C1])
  
  Feat_f= concatenate([model_1,X,CNN])

  return (output_GRU,output_C2,output_C1,output)

"""# Loss Function"""

from keras import backend as K
def correlation_coefficient_loss(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(tf.multiply(xm,ym))
    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
    r = r_num / r_den

    r = K.maximum(K.minimum(r, 1.0), -1.0)

    l1=K.sqrt(K.mean(K.square(y - x))) 
    l2=1-K.square(r)

    l=l1
    return l

from keras import backend as K
def correlation_coefficient_loss_1(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(tf.multiply(xm,ym))
    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
    r = r_num / r_den

    r = K.maximum(K.minimum(r, 1.0), -1.0)

    l1=K.sqrt(K.mean(K.square(y - x))) 
    l2=1-K.square(r)

    l=l2
    return l

from keras import backend as K
def correlation_coefficient_loss_joint(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(tf.multiply(xm,ym))
    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
    r = r_num / r_den

    r = K.maximum(K.minimum(r, 1.0), -1.0)

    l1=K.sqrt(K.mean(K.square(y - x))) 
    l2=1-K.square(r)

    l=l1+l2
    return l

"""# 1. GRU-Net"""

#### Main Model ####
#
#inputs_1D = tf.keras.layers.Input( shape=(w,12) )
#inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )
#
#
#inputs_1D_N=BatchNormalization()(inputs_1D)
#inputs_2D_N=BatchNormalization()(inputs_2D)
#
#
#output=GRU_Net(inputs_1D_N,inputs_2D_N)
#
#model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)
#model_2.compile(loss=correlation_coefficient_loss, optimizer='Adam', metrics=[correlation_coefficient_loss])
#
#
#history=model_2.fit([np.array(train_X_1D),np.array(train_X_2D)], train_y_5, epochs=30, batch_size=64, validation_data=([X_validation_1D,\
#                                                                      X_validation_2D], Y_validation), verbose=2, shuffle=False)
#
#filename = '/home/sanzidpr/DeepBBWAE_Net/subject 1/model_GRU.h5'
#model_2.save(filename)
#print('>Saved %s' % filename)
#
## # # summarize history for loss
#plt.plot(history.history['loss'])
#plt.plot(history.history['val_loss'])
#plt.title('model loss')
#plt.ylabel('loss')
#plt.xlabel('epoch')
#plt.legend(['train', 'test'], loc='upper right')
#plt.show()
#model_2.summary()

gc.collect()
gc.collect()
gc.collect()
gc.collect()

filename = '/home/sanzidpr/DeepBBWAE_Net/subject 1/model_Gait_JL_Net.h5'
model= tf.keras.models.load_model(filename, custom_objects={'correlation_coefficient_loss_joint': correlation_coefficient_loss_joint})

#yhat_4=model_2.predict([test_X_1D,test_X_2D])

[yhat_1,yhat_2,yhat_3,yhat_4]=model.predict([test_X_1D,test_X_2D])
 ### Present ###
yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))
test_y_r=test_y.reshape((test_y.shape[0]*w,6))

print(yhat_4.shape)


### Unpack ###
yhat_up=unpack_dataset_present(np.array(yhat_5),80)
test_y_up=unpack_dataset_present(np.array(test_y_r),80)

print(yhat_up.shape,test_y_up.shape)

### Present ###

yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)
test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)

print(yhat_up.shape,test_y_up.shape)

### Present ###

rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))

print(rmse[0])
print(rmse[1])
print(rmse[2])

m=np.mean(rmse)

print('\n')
print(m)

print('\n')

print(p[0])
print(p[1])
print(p[2])
print('\n')

print(np.mean(p))

import sys
sys.exit

RMSE_GRU=rmse
PCC_GRU=p

Ablation_1=np.hstack([RMSE_GRU,PCC_GRU])

"""# 2. Conv1D-Net"""

#### Main Model ####

inputs_1D = tf.keras.layers.Input( shape=(w,12) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output=Conv1D_Net(inputs_1D_N,inputs_2D_N)

model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)
model_2.compile(loss=correlation_coefficient_loss, optimizer='Adam', metrics=[correlation_coefficient_loss])


history=model_2.fit([np.array(train_X_1D),np.array(train_X_2D)], train_y_5, epochs=30, batch_size=64, validation_data=([X_validation_1D,\
                                                                      X_validation_2D], Y_validation), verbose=2, shuffle=False)

filename = '/home/sanzidpr/DeepBBWAE_Net/subject 1/model_Conv1D.h5'
model_2.save(filename)
print('>Saved %s' % filename)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()

gc.collect()
gc.collect()
gc.collect()
gc.collect()

yhat_4=model_2.predict([test_X_1D,test_X_2D])


 ### Present ###
yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))
test_y_r=test_y.reshape((test_y.shape[0]*w,6))

print(yhat_4.shape)


### Unpack ###
yhat_up=unpack_dataset_present(np.array(yhat_5),80)
test_y_up=unpack_dataset_present(np.array(test_y_r),80)

print(yhat_up.shape,test_y_up.shape)

### Present ###

yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)
test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)

print(yhat_up.shape,test_y_up.shape)

### Present ###

rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))

print(rmse[0])
print(rmse[1])
print(rmse[2])

m=np.mean(rmse)

print('\n')
print(m)

print('\n')

print(p[0])
print(p[1])
print(p[2])
print('\n')

print(np.mean(p))

RMSE_Conv1D=rmse
PCC_Conv1D=p

Ablation_2=np.hstack([RMSE_Conv1D,PCC_Conv1D])

"""# 3. Conv2D-Net"""

#### Main Model ####

inputs_1D = tf.keras.layers.Input( shape=(w,12) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output=Conv2D_Net(inputs_1D_N,inputs_2D_N)

model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)
model_2.compile(loss=correlation_coefficient_loss, optimizer='Adam', metrics=[correlation_coefficient_loss])


history=model_2.fit([np.array(train_X_1D),np.array(train_X_2D)], train_y_5, epochs=30, batch_size=64, validation_data=([X_validation_1D,\
                                                                      X_validation_2D], Y_validation), verbose=2, shuffle=False)

filename = '/home/sanzidpr/DeepBBWAE_Net/subject 1/model_Conv2D.h5'
model_2.save(filename)
print('>Saved %s' % filename)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()

gc.collect()
gc.collect()
gc.collect()
gc.collect()

yhat_4=model_2.predict([test_X_1D,test_X_2D])


 ### Present ###
yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))
test_y_r=test_y.reshape((test_y.shape[0]*w,6))

print(yhat_4.shape)


### Unpack ###
yhat_up=unpack_dataset_present(np.array(yhat_5),80)
test_y_up=unpack_dataset_present(np.array(test_y_r),80)

print(yhat_up.shape,test_y_up.shape)

### Present ###

yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)
test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)

print(yhat_up.shape,test_y_up.shape)

### Present ###

rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))

print(rmse[0])
print(rmse[1])
print(rmse[2])

m=np.mean(rmse)

print('\n')
print(m)

print('\n')

print(p[0])
print(p[1])
print(p[2])
print('\n')

print(np.mean(p))

RMSE_Conv2D=rmse
PCC_Conv2D=p

Ablation_3=np.hstack([RMSE_Conv2D,PCC_Conv2D])

"""# 4. GRU-Conv1D-Net"""

#### Main Model ####

inputs_1D = tf.keras.layers.Input( shape=(w,12) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output=GRU_Conv1D_Net(inputs_1D_N,inputs_2D_N)

model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)
model_2.compile(loss=correlation_coefficient_loss, optimizer='Adam', metrics=[correlation_coefficient_loss])


history=model_2.fit([np.array(train_X_1D),np.array(train_X_2D)], train_y_5, epochs=30, batch_size=64, validation_data=([X_validation_1D,\
                                                                      X_validation_2D], Y_validation), verbose=2, shuffle=False)

filename = '/home/sanzidpr/DeepBBWAE_Net/subject 1/model_GRU_Conv1D.h5'
model_2.save(filename)
print('>Saved %s' % filename)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()

gc.collect()
gc.collect()
gc.collect()
gc.collect()

yhat_4=model_2.predict([test_X_1D,test_X_2D])


 ### Present ###
yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))
test_y_r=test_y.reshape((test_y.shape[0]*w,6))

print(yhat_4.shape)


### Unpack ###
yhat_up=unpack_dataset_present(np.array(yhat_5),80)
test_y_up=unpack_dataset_present(np.array(test_y_r),80)

print(yhat_up.shape,test_y_up.shape)

### Present ###

yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)
test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)

print(yhat_up.shape,test_y_up.shape)

### Present ###

rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))

print(rmse[0])
print(rmse[1])
print(rmse[2])

m=np.mean(rmse)

print('\n')
print(m)

print('\n')

print(p[0])
print(p[1])
print(p[2])
print('\n')

print(np.mean(p))

RMSE_GRU_Conv1D=rmse
PCC_GRU_Conv1D=p

Ablation_4=np.hstack([RMSE_GRU_Conv1D,PCC_GRU_Conv1D])

"""# 5. GRU-Conv2D-Net"""

#### Main Model ####

inputs_1D = tf.keras.layers.Input( shape=(w,12) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output=GRU_Conv2D_Net(inputs_1D_N,inputs_2D_N)

model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)
model_2.compile(loss=correlation_coefficient_loss, optimizer='Adam', metrics=[correlation_coefficient_loss])


history=model_2.fit([np.array(train_X_1D),np.array(train_X_2D)], train_y_5, epochs=30, batch_size=64, validation_data=([X_validation_1D,\
                                                                      X_validation_2D], Y_validation), verbose=2, shuffle=False)

filename = '/home/sanzidpr/DeepBBWAE_Net/subject 1/model_GRU_Conv2D.h5'
model_2.save(filename)
print('>Saved %s' % filename)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()

gc.collect()
gc.collect()
gc.collect()
gc.collect()

yhat_4=model_2.predict([test_X_1D,test_X_2D])


 ### Present ###
yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))
test_y_r=test_y.reshape((test_y.shape[0]*w,6))

print(yhat_4.shape)


### Unpack ###
yhat_up=unpack_dataset_present(np.array(yhat_5),80)
test_y_up=unpack_dataset_present(np.array(test_y_r),80)

print(yhat_up.shape,test_y_up.shape)

### Present ###

yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)
test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)

print(yhat_up.shape,test_y_up.shape)

### Present ###

rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))

print(rmse[0])
print(rmse[1])
print(rmse[2])

m=np.mean(rmse)

print('\n')
print(m)

print('\n')

print(p[0])
print(p[1])
print(p[2])
print('\n')

print(np.mean(p))

RMSE_GRU_Conv2D=rmse
PCC_GRU_Conv2D=p

Ablation_5=np.hstack([RMSE_GRU_Conv2D,PCC_GRU_Conv2D])

"""# 5. Conv1D-Conv2D-Net"""

#### Main Model ####

inputs_1D = tf.keras.layers.Input( shape=(w,12) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output=Conv1D_Conv2D_Net(inputs_1D_N,inputs_2D_N)

model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=output)
model_2.compile(loss=correlation_coefficient_loss, optimizer='Adam', metrics=[correlation_coefficient_loss])


history=model_2.fit([np.array(train_X_1D),np.array(train_X_2D)], train_y_5, epochs=30, batch_size=64, validation_data=([X_validation_1D,\
                                                                      X_validation_2D], Y_validation), verbose=2, shuffle=False)

filename = '/home/sanzidpr/DeepBBWAE_Net/subject 1/model_Conv1D_Conv2D.h5'
model_2.save(filename)
print('>Saved %s' % filename)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()

gc.collect()
gc.collect()
gc.collect()
gc.collect()

yhat_4=model_2.predict([test_X_1D,test_X_2D])


 ### Present ###
yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))
test_y_r=test_y.reshape((test_y.shape[0]*w,6))

print(yhat_4.shape)


### Unpack ###
yhat_up=unpack_dataset_present(np.array(yhat_5),80)
test_y_up=unpack_dataset_present(np.array(test_y_r),80)

print(yhat_up.shape,test_y_up.shape)

### Present ###

yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)
test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)

print(yhat_up.shape,test_y_up.shape)

### Present ###

rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))

print(rmse[0])
print(rmse[1])
print(rmse[2])

m=np.mean(rmse)

print('\n')
print(m)

print('\n')

print(p[0])
print(p[1])
print(p[2])
print('\n')

print(np.mean(p))

RMSE_Conv1D_Conv2D=rmse
PCC_Conv1D_Conv2D=p

Ablation_6=np.hstack([RMSE_Conv1D_Conv2D,PCC_Conv1D_Conv2D])

"""# 7. Gait-SubNet-1"""

#### Main Model ####

inputs_1D = tf.keras.layers.Input( shape=(w,12) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output=Gait_SubNet_1(inputs_1D_N,inputs_2D_N)

model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output])
model_2.compile(loss=correlation_coefficient_loss, optimizer='Adam', metrics=[correlation_coefficient_loss])


history=model_2.fit([np.array(train_X_1D),np.array(train_X_2D)], [train_y_5,train_y_5,train_y_5], epochs=30, batch_size=64, validation_data=([X_validation_1D,\
                                                                      X_validation_2D], [Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

filename = '/home/sanzidpr/DeepBBWAE_Net/subject 1/model_Gait_subNet_1.h5'
model_2.save(filename)
print('>Saved %s' % filename)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()

gc.collect()
gc.collect()
gc.collect()
gc.collect()

yhat_1,yhat_2,yhat_4=model_2.predict([test_X_1D,test_X_2D])

### Present ###
yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))
test_y_r=test_y.reshape((test_y.shape[0]*w,6))

print(yhat_4.shape)


### Unpack ###
yhat_up=unpack_dataset_present(np.array(yhat_5),80)
test_y_up=unpack_dataset_present(np.array(test_y_r),80)

print(yhat_up.shape,test_y_up.shape)

### Present ###

yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)
test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)

print(yhat_up.shape,test_y_up.shape)

### Present ###

rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))

print(rmse[0])
print(rmse[1])
print(rmse[2])

m=np.mean(rmse)

print('\n')
print(m)

print('\n')

print(p[0])
print(p[1])
print(p[2])
print('\n')

print(np.mean(p))

RMSE_Gait_subNet_1=rmse
PCC_Gait_subNet_1=p

Ablation_7=np.hstack([RMSE_Gait_subNet_1,PCC_Gait_subNet_1])

"""# 7. Gait-SubNet-2"""

#### Main Model ####

inputs_1D = tf.keras.layers.Input( shape=(w,12) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output=Gait_SubNet_2(inputs_1D_N,inputs_2D_N)

model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output])
model_2.compile(loss=correlation_coefficient_loss, optimizer='Adam', metrics=[correlation_coefficient_loss])


history=model_2.fit([np.array(train_X_1D),np.array(train_X_2D)], [train_y_5,train_y_5,train_y_5], epochs=30, batch_size=64, validation_data=([X_validation_1D,\
                                                                      X_validation_2D], [Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

filename = '/home/sanzidpr/DeepBBWAE_Net/subject 1/model_Gait_subNet_2.h5'
model_2.save(filename)
print('>Saved %s' % filename)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()

gc.collect()
gc.collect()
gc.collect()
gc.collect()

yhat_1,yhat_2,yhat_4=model_2.predict([test_X_1D,test_X_2D])

### Present ###
yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))
test_y_r=test_y.reshape((test_y.shape[0]*w,6))

print(yhat_4.shape)


### Unpack ###
yhat_up=unpack_dataset_present(np.array(yhat_5),80)
test_y_up=unpack_dataset_present(np.array(test_y_r),80)

print(yhat_up.shape,test_y_up.shape)

### Present ###

yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)
test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)

print(yhat_up.shape,test_y_up.shape)

### Present ###

rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))

print(rmse[0])
print(rmse[1])
print(rmse[2])

m=np.mean(rmse)

print('\n')
print(m)

print('\n')

print(p[0])
print(p[1])
print(p[2])
print('\n')

print(np.mean(p))

RMSE_Gait_subNet_2=rmse
PCC_Gait_subNet_2=p

Ablation_8=np.hstack([RMSE_Gait_subNet_2,PCC_Gait_subNet_2])

"""# 7. Gait-SubNet-3"""

#### Main Model ####

inputs_1D = tf.keras.layers.Input( shape=(w,12) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output=Gait_SubNet_3(inputs_1D_N,inputs_2D_N)

model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output])
model_2.compile(loss=correlation_coefficient_loss, optimizer='Adam', metrics=[correlation_coefficient_loss])


history=model_2.fit([np.array(train_X_1D),np.array(train_X_2D)], [train_y_5,train_y_5,train_y_5], epochs=30, batch_size=64, validation_data=([X_validation_1D,\
                                                                      X_validation_2D], [Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

filename = '/home/sanzidpr/DeepBBWAE_Net/subject 1/model_Gait_subNet_3.h5'
model_2.save(filename)
print('>Saved %s' % filename)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()

gc.collect()
gc.collect()
gc.collect()
gc.collect()

yhat_1,yhat_2,yhat_4=model_2.predict([test_X_1D,test_X_2D])

### Present ###
yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))
test_y_r=test_y.reshape((test_y.shape[0]*w,6))

print(yhat_4.shape)


### Unpack ###
yhat_up=unpack_dataset_present(np.array(yhat_5),80)
test_y_up=unpack_dataset_present(np.array(test_y_r),80)

print(yhat_up.shape,test_y_up.shape)

### Present ###

yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)
test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)

print(yhat_up.shape,test_y_up.shape)

### Present ###

rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))

print(rmse[0])
print(rmse[1])
print(rmse[2])

m=np.mean(rmse)

print('\n')
print(m)

print('\n')

print(p[0])
print(p[1])
print(p[2])
print('\n')

print(np.mean(p))

RMSE_Gait_subNet_3=rmse
PCC_Gait_subNet_3=p

Ablation_9=np.hstack([RMSE_Gait_subNet_3,PCC_Gait_subNet_3])

"""# 8. Gait-Net"""

#### Main Model ####

inputs_1D = tf.keras.layers.Input( shape=(w,12) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output_3,output=Gait_Net(inputs_1D_N,inputs_2D_N)

model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])
model_2.compile(loss=correlation_coefficient_loss, optimizer='Adam', metrics=[correlation_coefficient_loss])


history=model_2.fit([np.array(train_X_1D),np.array(train_X_2D)], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=30, batch_size=64, validation_data=([X_validation_1D,\
                                                                      X_validation_2D], [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

filename = '/home/sanzidpr/DeepBBWAE_Net/subject 1/model_Gait_Net.h5'
model_2.save(filename)
print('>Saved %s' % filename)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()

gc.collect()
gc.collect()
gc.collect()
gc.collect()

# # convert an array of values into a dataset matrix
def unpack_dataset_present(dataset_1, window=80):
  dataX= []
  k=1
  w=100
  l=0
  shift=100
  for i in range(10000):
    j=shift*k
    a = dataset_1[l:j,:]
    # print(a.shape)
    l=0
    l=j+50
    dataX=np.append(dataX,a)
    k=k+1
    j=0
  return np.array(dataX)

[yhat_1,yhat_2,yhat_3,yhat_4]=model_2.predict([test_X_1D,test_X_2D])



# [yhat_1,yhat_2,yhat_3,yhat_4,yhat_f,yhat_6,yhat_7,yhat_8]=model_2.predict([test_X_1D,test_X_2D])


# yhat_4=model_2.predict([test_X_1D,test_X_2D])

# yhat_4=yhat_bag_p
# yhat_f=yhat_bag_f

# print(test_y_f.shape)

 ### Present ###
yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))
test_y_r=test_y.reshape((test_y.shape[0]*w,6))

print(yhat_4.shape)
### Future ###

# yhat_f=yhat_f.reshape((yhat_f.shape[0]*w_f,6))
# test_y_f_1=test_y_f.reshape((test_y_f.shape[0]*w_f,6))

### Unpack ###
yhat_up=unpack_dataset_present(np.array(yhat_5),80)
test_y_up=unpack_dataset_present(np.array(test_y_r),80)



print(yhat_up.shape,test_y_up.shape)


### Present ###

yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)
test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)

print(yhat_up.shape,test_y_up.shape)

### Present ###

rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))

print(rmse[0])
print(rmse[1])
print(rmse[2])

m=np.mean(rmse)

print('\n')
print(m)

print('\n')

print(p[0])
print(p[1])
print(p[2])
print('\n')

print(np.mean(p))

RMSE_Gait_Net=rmse
PCC_Gait_Net=p

Ablation_10=np.hstack([RMSE_Gait_Net,PCC_Gait_Net])

"""# 9. Gait-Net--Joint Loss"""

#### Main Model ####

inputs_1D = tf.keras.layers.Input( shape=(w,12) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output_3,output=Gait_Net(inputs_1D_N,inputs_2D_N)

model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])
model_2.compile(loss=correlation_coefficient_loss_joint, optimizer='Adam', metrics=[correlation_coefficient_loss_joint])


history=model_2.fit([np.array(train_X_1D),np.array(train_X_2D)], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=30, batch_size=64, validation_data=([X_validation_1D,\
                                                                      X_validation_2D], [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

filename = '/home/sanzidpr/DeepBBWAE_Net/subject 1/model_Gait_JL_Net.h5'
model_2.save(filename)
print('>Saved %s' % filename)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()

gc.collect()
gc.collect()
gc.collect()
gc.collect()

# # convert an array of values into a dataset matrix
def unpack_dataset_present(dataset_1, window=80):
  dataX= []
  k=1
  w=100
  l=0
  shift=100
  for i in range(10000):
    j=shift*k
    a = dataset_1[l:j,:]
    # print(a.shape)
    l=0
    l=j+50
    dataX=np.append(dataX,a)
    k=k+1
    j=0
  return np.array(dataX)

[yhat_1,yhat_2,yhat_3,yhat_4]=model_2.predict([test_X_1D,test_X_2D])



# [yhat_1,yhat_2,yhat_3,yhat_4,yhat_f,yhat_6,yhat_7,yhat_8]=model_2.predict([test_X_1D,test_X_2D])


# yhat_4=model_2.predict([test_X_1D,test_X_2D])

# yhat_4=yhat_bag_p
# yhat_f=yhat_bag_f

# print(test_y_f.shape)

 ### Present ###
yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))
test_y_r=test_y.reshape((test_y.shape[0]*w,6))

print(yhat_4.shape)
### Future ###

# yhat_f=yhat_f.reshape((yhat_f.shape[0]*w_f,6))
# test_y_f_1=test_y_f.reshape((test_y_f.shape[0]*w_f,6))

### Unpack ###
yhat_up=unpack_dataset_present(np.array(yhat_5),80)
test_y_up=unpack_dataset_present(np.array(test_y_r),80)



print(yhat_up.shape,test_y_up.shape)


### Present ###

yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)
test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)

print(yhat_up.shape,test_y_up.shape)

### Present ###

rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))

print(rmse[0])
print(rmse[1])
print(rmse[2])

m=np.mean(rmse)

print('\n')
print(m)

print('\n')

print(p[0])
print(p[1])
print(p[2])
print('\n')

print(np.mean(p))

RMSE_Gait_JL_Net=rmse
PCC_Gait_JL_Net=p

Ablation_11=np.hstack([RMSE_Gait_JL_Net,PCC_Gait_JL_Net])

treadmill_dataset=np.concatenate((subject_2_treadmill,subject_3_treadmill,subject_4_treadmill,subject_5_treadmill,subject_6_treadmill,subject_7_treadmill,subject_8_treadmill,subject_9_treadmill,subject_10_treadmill,),axis=0)
overground_dataset=np.concatenate((subject_2_overground,subject_3_overground,subject_4_overground,subject_5_overground,subject_6_overground,subject_7_overground,subject_8_overground,subject_9_overground,subject_10_overground,),axis=0)
stair_slope_dataset=np.concatenate((subject_2_stair_slope,subject_3_stair_slope,subject_4_stair_slope,subject_5_stair_slope,subject_6_stair_slope,subject_7_stair_slope,subject_8_stair_slope,subject_9_stair_slope,subject_10_stair_slope,),axis=0)

train_dataset=np.concatenate((treadmill_dataset,overground_dataset,overground_dataset,overground_dataset,stair_slope_dataset,stair_slope_dataset,stair_slope_dataset,stair_slope_dataset),axis=0)
#train_dataset=np.concatenate((treadmill_dataset,overground_dataset,stair_slope_dataset),axis=0)

test_dataset=subject_1

print(test_dataset.shape)

"""# Augmented Dataset"""

print(test_dataset.shape)

# Sensor 1- Sternum
# Sensor 2-Sacrum
# Sensor 3-R_thigh
# Sensor 4-L_thigh
# Sensor 5-R_shank
# Sensor 6-L_shank
# Sensor 7-R_dorsal
# Sensor 8-L_dorsal
 
 
# Train features #
train_1=train_dataset[:,0:8]
train_2=train_dataset[:,17:25]
train_3=train_dataset[:,34:42]
train_4=train_dataset[:,51:59]
train_5=train_dataset[:,68:76]
train_6=train_dataset[:,85:93]
train_7=train_dataset[:,102:110]
train_8=train_dataset[:,119:127]




train_7_1=train_dataset[:,102:105]
train_7_2=train_dataset[:,106:109]

train_8_1=train_dataset[:,119:122]
train_8_2=train_dataset[:,123:126]


## Extra Features

train_extra=train_dataset[:,160:176]
 
#x_1=train_4
 
x_train=np.concatenate((train_7_1,train_7_2,train_8_1,train_8_2),axis=1)
 
# x_train=np.concatenate((train_7,train_8,train_extra),axis=1)
# x_train=np.concatenate((train_2,train_3,train_4,train_5,train_6,train_7,train_8,train_extra),axis=1)
 
#x_train=train_extra
 
 
 
scaler = MinMaxScaler(feature_range=(0, 1))
 
 
#train_X_1=scaler.fit_transform(x_train)
 
 
train_X_1=x_train
 
 
 
 
# Test features #
 
test_1=test_dataset[:,0:8]
test_2=test_dataset[:,17:25]
test_3=test_dataset[:,34:42]
test_4=test_dataset[:,51:59]
test_5=test_dataset[:,68:76]
test_6=test_dataset[:,85:93]
test_7=test_dataset[:,102:110]
test_8=test_dataset[:,119:127]
 



test_7_1=test_dataset[:,102:105]
test_7_2=test_dataset[:,106:109]

test_8_1=test_dataset[:,119:122]
test_8_2=test_dataset[:,123:126]

   ### Extra features  ###
  
test_extra=test_dataset[:,160:176]
 
x_test=np.concatenate((test_7_1,test_7_2,test_8_1,test_8_2),axis=1)

 
# x_test=np.concatenate((test_7,test_8,test_extra),axis=1)
# x_test=np.concatenate((test_2,test_3,test_4,test_5,test_6,test_7,test_8,test_extra),axis=1)
 
#x_test=test_extra
 
 
#test_X_1=scaler.fit_transform(x_test)
 
test_X_1=x_test
 
 
 
    
f=88
 
y_1_1=train_dataset[:,(f+55):(f+56)]
y_1_2=train_dataset[:,(f+58):(f+60)]
y_1_3=train_dataset[:,(f+62):(f+63)]
y_1_4=train_dataset[:,(f+65):(f+67)]
 
 
 
#train_y_1= np.concatenate((y_1_R,y_1_R_1,y_1_L,y_1_L_1),axis=1)     # 1-horizontal, 0-vertical---main
 

train_y_1=np.concatenate((y_1_1,y_1_2,y_1_3,y_1_4),axis=1)
 

 
y_2_1=test_dataset[:,(f+55):(f+56)]
y_2_2=test_dataset[:,(f+58):(f+60)]
y_2_3=test_dataset[:,(f+62):(f+63)]
y_2_4=test_dataset[:,(f+65):(f+67)]


 
test_y_1= np.concatenate((y_2_1,y_2_2,y_2_3,y_2_4),axis=1)

#test_y_1= np.concatenate((y_2_1,y_2_2),axis=1)
# test_y_1=y_2_1
 
 
print(train_X_1.shape,test_X_1.shape,train_y_1.shape,test_y_1.shape)

print(test_extra.shape)

"""# Data creation"""

# # convert an array of values into a dataset matrix
def create_dataset_present(dataset_1, window=100):
  dataX= []
  k=0
  w=100
  shift=50
  for i in range(int(len(dataset_1)/shift-8)):
    j=shift*k
    a = dataset_1[j:j+window,:]
    # print(a.shape)
    dataX.append(a)
    k=k+1
  return np.array(dataX)

# # convert an array of values into a dataset matrix
def create_dataset_future(dataset_1, window=100):
  dataX= []
  k=0
  w=100
  shift=50
  for i in range(int(len(dataset_1)/shift-9)):
      j=shift*k
      a = dataset_1[j+window:j+window+shift,:]
      dataX.append(a)
      k=k+1
  return np.array(dataX)

import gc
gc.collect()

### Reconstruction/Present Dataset ###
w=100

train_X_3=create_dataset_present(train_X_1,w)
train_y_3=create_dataset_present(train_y_1,w)

test_X_1D=create_dataset_present(test_X_1,w)
test_y=create_dataset_present(test_y_1,w)

print(test_y.shape)

### Future Prediction Dataset ###


train_y_3_f=create_dataset_future(train_y_1,w)
test_y_f=create_dataset_future(test_y_1,w)

print(train_y_3_f.shape)

train_y_3_f=np.concatenate((train_y_3_f[0:len(train_y_3_f),:,:],train_y_3_f[len(train_y_3_f)-1:len(train_y_3_f),:,:]),axis=0)
test_y_f=np.concatenate((test_y_f[0:len(test_y_f),:,:],test_y_f[len(test_y_f)-1:len(test_y_f),:,:]),axis=0)

print(train_y_3.shape,train_y_3_f.shape)

w=100
w_f=50

train_y_3=train_y_3.reshape(train_y_3.shape[0],w*6)
test_y=test_y.reshape(test_y.shape[0],w*6)


train_y_3_f=train_y_3_f.reshape(train_y_3_f.shape[0],w_f*6)
test_y_f=test_y_f.reshape(test_y_f.shape[0],w_f*6)

print(train_X_1.shape,test_X_1.shape,train_y_1.shape,test_y_1.shape)

# train_X_1D, X_validation_1D, train_y_5, Y_validation = train_test_split(train_X_3,train_y_3, test_size=0.20, random_state=123)
# train_y_5_f, Y_validation_f = train_test_split(train_y_3_f, test_size=0.20, random_state=123)

# print(train_X_1D.shape,train_y_5.shape,X_validation_1D.shape,Y_validation.shape)

train_X_1D, X_validation_1D, train_y_5, Y_validation = train_test_split(train_X_3,train_y_3, test_size=0.20, random_state=True)

# train_X_extra=train_X_1D[:,0,12:28]
# X_validation_extra=X_validation_1D[:,0,12:28]
# test_X_extra=test_X_1D[:,0,12:28]

train_X_1D=train_X_1D[:,:,0:12]
X_validation_1D=X_validation_1D[:,:,0:12]
test_X_1D=test_X_1D[:,:,0:12]
train_y_5_f, Y_validation_f = train_test_split(train_y_3_f, test_size=0.20, random_state=True)

print(train_X_1D.shape,train_y_5.shape,X_validation_1D.shape,Y_validation.shape)
# print(train_X_extra.shape,X_validation_extra.shape,test_X_extra.shape)

# train_X_1D, X_validation_1D, train_y_5, Y_validation = train_test_split(train_X_3,train_y_3, test_size=0.20, random_state=123)

# train_X_extra=train_X_1D[:,0,56:62]
# X_validation_extra=X_validation_1D[:,0,56:62]
# test_X_extra=test_X_1D[:,0,56:62]

# train_X_1D=train_X_1D[:,:,0:56]
# X_validation_1D=X_validation_1D[:,:,0:56]
# test_X_1D=test_X_1D[:,:,0:56]
# train_y_5_f, Y_validation_f = train_test_split(train_y_3_f, test_size=0.20, random_state=123)

# print(train_X_1D.shape,train_y_5.shape,X_validation_1D.shape,Y_validation.shape)
# print(train_X_extra.shape,X_validation_extra.shape,test_X_extra.shape)

features=6

train_X_2D=train_X_1D[:,:,0:12].reshape(train_X_1D.shape[0],train_X_1D.shape[1],features,2)
test_X_2D=test_X_1D[:,:,0:12].reshape(test_X_1D.shape[0],test_X_1D.shape[1],features,2)
X_validation_2D= X_validation_1D[:,:,0:12].reshape(X_validation_1D.shape[0],
                                                   X_validation_1D.shape[1],features,2)
#X_validation_2D_ridge= X_validation_1D_ridge.reshape(X_validation_1D_ridge.shape[0],X_validation_1D_ridge.shape[1],8,2)


print(train_X_2D.shape,test_X_2D.shape,X_validation_2D.shape)

# features=6

# train_X_2D=train_X_1D.reshape(train_X_1D.shape[0],train_X_1D.shape[1],features,2)
# test_X_2D=test_X_1D.reshape(test_X_1D.shape[0],test_X_1D.shape[1],features,2)
# X_validation_2D= X_validation_1D.reshape(X_validation_1D.shape[0],X_validation_1D.shape[1],features,2)
# #X_validation_2D_ridge= X_validation_1D_ridge.reshape(X_validation_1D_ridge.shape[0],X_validation_1D_ridge.shape[1],8,2)


# print(train_X_2D.shape,test_X_2D.shape,X_validation_2D.shape)

Bag_samples=train_X_2D.shape[0]
print(Bag_samples)

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

Bag_samples=train_X_2D.shape[0]
print(Bag_samples)

import gc
gc.collect()
gc.collect()
gc.collect()

"""# 12. Gait-Net+Augmentation"""

#### Main Model ####

inputs_1D = tf.keras.layers.Input( shape=(w,12) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output_3,output=Gait_Net(inputs_1D_N,inputs_2D_N)

model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])
model_2.compile(loss=correlation_coefficient_loss, optimizer='Adam', metrics=[correlation_coefficient_loss])


history=model_2.fit([np.array(train_X_1D),np.array(train_X_2D)], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=30, batch_size=64, validation_data=([X_validation_1D,\
                                                                      X_validation_2D], [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

filename = '/home/sanzidpr/DeepBBWAE_Net/subject 1/model_Gait_A_Net.h5'
model_2.save(filename)
print('>Saved %s' % filename)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()

gc.collect()
gc.collect()
gc.collect()
gc.collect()

# # convert an array of values into a dataset matrix
def unpack_dataset_present(dataset_1, window=80):
  dataX= []
  k=1
  w=100
  l=0
  shift=100
  for i in range(10000):
    j=shift*k
    a = dataset_1[l:j,:]
    # print(a.shape)
    l=0
    l=j+50
    dataX=np.append(dataX,a)
    k=k+1
    j=0
  return np.array(dataX)

[yhat_1,yhat_2,yhat_3,yhat_4]=model_2.predict([test_X_1D,test_X_2D])



# [yhat_1,yhat_2,yhat_3,yhat_4,yhat_f,yhat_6,yhat_7,yhat_8]=model_2.predict([test_X_1D,test_X_2D])


# yhat_4=model_2.predict([test_X_1D,test_X_2D])

# yhat_4=yhat_bag_p
# yhat_f=yhat_bag_f

# print(test_y_f.shape)

 ### Present ###
yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))
test_y_r=test_y.reshape((test_y.shape[0]*w,6))

print(yhat_4.shape)
### Future ###

# yhat_f=yhat_f.reshape((yhat_f.shape[0]*w_f,6))
# test_y_f_1=test_y_f.reshape((test_y_f.shape[0]*w_f,6))

### Unpack ###
yhat_up=unpack_dataset_present(np.array(yhat_5),80)
test_y_up=unpack_dataset_present(np.array(test_y_r),80)



print(yhat_up.shape,test_y_up.shape)


### Present ###

yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)
test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)

print(yhat_up.shape,test_y_up.shape)

### Present ###

rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))

print(rmse[0])
print(rmse[1])
print(rmse[2])

m=np.mean(rmse)

print('\n')
print(m)

print('\n')

print(p[0])
print(p[1])
print(p[2])
print('\n')

print(np.mean(p))

RMSE_Gait_A_Net=rmse
PCC_Gait_A_Net=p

Ablation_12=np.hstack([RMSE_Gait_A_Net,PCC_Gait_A_Net])

"""# 13. Gait-JL-Net+Augmentation"""

#### Main Model ####

inputs_1D = tf.keras.layers.Input( shape=(w,12) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output_3,output=Gait_Net(inputs_1D_N,inputs_2D_N)

model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])
model_2.compile(loss=correlation_coefficient_loss_joint, optimizer='Adam', metrics=[correlation_coefficient_loss_joint])


history=model_2.fit([np.array(train_X_1D),np.array(train_X_2D)], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=30, batch_size=64, validation_data=([X_validation_1D,\
                                                                      X_validation_2D], [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

filename = '/home/sanzidpr/DeepBBWAE_Net/subject 1/model_Gait_JL_A_Net.h5'
model_2.save(filename)
print('>Saved %s' % filename)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()

gc.collect()
gc.collect()
gc.collect()
gc.collect()

# # convert an array of values into a dataset matrix
def unpack_dataset_present(dataset_1, window=80):
  dataX= []
  k=1
  w=100
  l=0
  shift=100
  for i in range(10000):
    j=shift*k
    a = dataset_1[l:j,:]
    # print(a.shape)
    l=0
    l=j+50
    dataX=np.append(dataX,a)
    k=k+1
    j=0
  return np.array(dataX)

[yhat_1,yhat_2,yhat_3,yhat_4]=model_2.predict([test_X_1D,test_X_2D])



# [yhat_1,yhat_2,yhat_3,yhat_4,yhat_f,yhat_6,yhat_7,yhat_8]=model_2.predict([test_X_1D,test_X_2D])


# yhat_4=model_2.predict([test_X_1D,test_X_2D])

# yhat_4=yhat_bag_p
# yhat_f=yhat_bag_f

# print(test_y_f.shape)

 ### Present ###
yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))
test_y_r=test_y.reshape((test_y.shape[0]*w,6))

print(yhat_4.shape)
### Future ###

# yhat_f=yhat_f.reshape((yhat_f.shape[0]*w_f,6))
# test_y_f_1=test_y_f.reshape((test_y_f.shape[0]*w_f,6))

### Unpack ###
yhat_up=unpack_dataset_present(np.array(yhat_5),80)
test_y_up=unpack_dataset_present(np.array(test_y_r),80)



print(yhat_up.shape,test_y_up.shape)


### Present ###

yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)
test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)

print(yhat_up.shape,test_y_up.shape)

### Present ###

rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))

print(rmse[0])
print(rmse[1])
print(rmse[2])

m=np.mean(rmse)

print('\n')
print(m)

print('\n')

print(p[0])
print(p[1])
print(p[2])
print('\n')

print(np.mean(p))

RMSE_Gait_JL_A_Net=rmse
PCC_Gait_JL_A_Net=p

Ablation_13=np.hstack([RMSE_Gait_JL_A_Net,PCC_Gait_JL_A_Net])

"""# Loss function with respect to weight"""

from keras import backend as K
def correlation_coefficient_loss_joint_2(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(tf.multiply(xm,ym))
    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
    r = r_num / r_den

    r = K.maximum(K.minimum(r, 1.0), -1.0)

    l1=K.sqrt(K.mean(K.square(y - x))) 
    l2=1-K.square(r)

    l=l1+2*l2
    return l

from keras import backend as K
def correlation_coefficient_loss_joint_4(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(tf.multiply(xm,ym))
    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
    r = r_num / r_den

    r = K.maximum(K.minimum(r, 1.0), -1.0)

    l1=K.sqrt(K.mean(K.square(y - x))) 
    l2=1-K.square(r)

    l=l1+4*l2
    return l

from keras import backend as K
def correlation_coefficient_loss_joint_6(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(tf.multiply(xm,ym))
    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
    r = r_num / r_den

    r = K.maximum(K.minimum(r, 1.0), -1.0)

    l1=K.sqrt(K.mean(K.square(y - x))) 
    l2=1-K.square(r)

    l=l1+6*l2
    return l

from keras import backend as K
def correlation_coefficient_loss_joint_8(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(tf.multiply(xm,ym))
    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
    r = r_num / r_den

    r = K.maximum(K.minimum(r, 1.0), -1.0)

    l1=K.sqrt(K.mean(K.square(y - x))) 
    l2=1-K.square(r)

    l=l1+8*l2
    return l

from keras import backend as K
def correlation_coefficient_loss_joint_10(y_true, y_pred):
    x = y_true
    y = y_pred
    mx = K.mean(x)
    my = K.mean(y)
    xm, ym = x-mx, y-my
    r_num = K.sum(tf.multiply(xm,ym))
    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))
    r = r_num / r_den

    r = K.maximum(K.minimum(r, 1.0), -1.0)

    l1=K.sqrt(K.mean(K.square(y - x))) 
    l2=1-K.square(r)

    l=l1+10*l2
    return l

"""# 14. Gait-JL-Net_2+Augmentation"""

#### Main Model ####

inputs_1D = tf.keras.layers.Input( shape=(w,12) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output_3,output=Gait_Net(inputs_1D_N,inputs_2D_N)

model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])
model_2.compile(loss=correlation_coefficient_loss_joint_2, optimizer='Adam', metrics=[correlation_coefficient_loss_joint_2])


history=model_2.fit([np.array(train_X_1D),np.array(train_X_2D)], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=30, batch_size=64, validation_data=([X_validation_1D,\
                                                                      X_validation_2D], [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

filename = '/home/sanzidpr/DeepBBWAE_Net/subject 1/model_Gait_JL_A_Net_2.h5'
model_2.save(filename)
print('>Saved %s' % filename)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()

gc.collect()
gc.collect()
gc.collect()
gc.collect()

# # convert an array of values into a dataset matrix
def unpack_dataset_present(dataset_1, window=80):
  dataX= []
  k=1
  w=100
  l=0
  shift=100
  for i in range(10000):
    j=shift*k
    a = dataset_1[l:j,:]
    # print(a.shape)
    l=0
    l=j+50
    dataX=np.append(dataX,a)
    k=k+1
    j=0
  return np.array(dataX)

[yhat_1,yhat_2,yhat_3,yhat_4]=model_2.predict([test_X_1D,test_X_2D])



# [yhat_1,yhat_2,yhat_3,yhat_4,yhat_f,yhat_6,yhat_7,yhat_8]=model_2.predict([test_X_1D,test_X_2D])


# yhat_4=model_2.predict([test_X_1D,test_X_2D])

# yhat_4=yhat_bag_p
# yhat_f=yhat_bag_f

# print(test_y_f.shape)

 ### Present ###
yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))
test_y_r=test_y.reshape((test_y.shape[0]*w,6))

print(yhat_4.shape)
### Future ###

# yhat_f=yhat_f.reshape((yhat_f.shape[0]*w_f,6))
# test_y_f_1=test_y_f.reshape((test_y_f.shape[0]*w_f,6))

### Unpack ###
yhat_up=unpack_dataset_present(np.array(yhat_5),80)
test_y_up=unpack_dataset_present(np.array(test_y_r),80)



print(yhat_up.shape,test_y_up.shape)


### Present ###

yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)
test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)

print(yhat_up.shape,test_y_up.shape)

### Present ###

rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))

print(rmse[0])
print(rmse[1])
print(rmse[2])

m=np.mean(rmse)

print('\n')
print(m)

print('\n')

print(p[0])
print(p[1])
print(p[2])
print('\n')

print(np.mean(p))

RMSE_Gait_JL_A_Net_2=rmse
PCC_Gait_JL_A_Net_2=p

Ablation_14=np.hstack([RMSE_Gait_JL_A_Net_2,PCC_Gait_JL_A_Net_2])

"""# 14. Gait-JL-Net_4+Augmentation"""

#### Main Model ####

inputs_1D = tf.keras.layers.Input( shape=(w,12) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output_3,output=Gait_Net(inputs_1D_N,inputs_2D_N)

model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])
model_2.compile(loss=correlation_coefficient_loss_joint_4, optimizer='Adam', metrics=[correlation_coefficient_loss_joint_4])


history=model_2.fit([np.array(train_X_1D),np.array(train_X_2D)], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=30, batch_size=64, validation_data=([X_validation_1D,\
                                                                      X_validation_2D], [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

filename = '/home/sanzidpr/DeepBBWAE_Net/subject 1/model_Gait_JL_A_Net_4.h5'
model_2.save(filename)
print('>Saved %s' % filename)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()

gc.collect()
gc.collect()
gc.collect()
gc.collect()

# # convert an array of values into a dataset matrix
def unpack_dataset_present(dataset_1, window=80):
  dataX= []
  k=1
  w=100
  l=0
  shift=100
  for i in range(10000):
    j=shift*k
    a = dataset_1[l:j,:]
    # print(a.shape)
    l=0
    l=j+50
    dataX=np.append(dataX,a)
    k=k+1
    j=0
  return np.array(dataX)

[yhat_1,yhat_2,yhat_3,yhat_4]=model_2.predict([test_X_1D,test_X_2D])



# [yhat_1,yhat_2,yhat_3,yhat_4,yhat_f,yhat_6,yhat_7,yhat_8]=model_2.predict([test_X_1D,test_X_2D])


# yhat_4=model_2.predict([test_X_1D,test_X_2D])

# yhat_4=yhat_bag_p
# yhat_f=yhat_bag_f

# print(test_y_f.shape)

 ### Present ###
yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))
test_y_r=test_y.reshape((test_y.shape[0]*w,6))

print(yhat_4.shape)
### Future ###

# yhat_f=yhat_f.reshape((yhat_f.shape[0]*w_f,6))
# test_y_f_1=test_y_f.reshape((test_y_f.shape[0]*w_f,6))

### Unpack ###
yhat_up=unpack_dataset_present(np.array(yhat_5),80)
test_y_up=unpack_dataset_present(np.array(test_y_r),80)



print(yhat_up.shape,test_y_up.shape)


### Present ###

yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)
test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)

print(yhat_up.shape,test_y_up.shape)

### Present ###

rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))

print(rmse[0])
print(rmse[1])
print(rmse[2])

m=np.mean(rmse)

print('\n')
print(m)

print('\n')

print(p[0])
print(p[1])
print(p[2])
print('\n')

print(np.mean(p))

RMSE_Gait_JL_A_Net_4=rmse
PCC_Gait_JL_A_Net_4=p

Ablation_15=np.hstack([RMSE_Gait_JL_A_Net_4,PCC_Gait_JL_A_Net_4])

"""# 16. Gait-JL-Net_6+Augmentation"""

#### Main Model ####

inputs_1D = tf.keras.layers.Input( shape=(w,12) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output_3,output=Gait_Net(inputs_1D_N,inputs_2D_N)

model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])
model_2.compile(loss=correlation_coefficient_loss_joint_6, optimizer='Adam', metrics=[correlation_coefficient_loss_joint_6])


history=model_2.fit([np.array(train_X_1D),np.array(train_X_2D)], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=30, batch_size=64, validation_data=([X_validation_1D,\
                                                                      X_validation_2D], [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

filename = '/home/sanzidpr/DeepBBWAE_Net/subject 1/model_Gait_JL_A_Net_6.h5'
model_2.save(filename)
print('>Saved %s' % filename)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()

gc.collect()
gc.collect()
gc.collect()
gc.collect()

# # convert an array of values into a dataset matrix
def unpack_dataset_present(dataset_1, window=80):
  dataX= []
  k=1
  w=100
  l=0
  shift=100
  for i in range(10000):
    j=shift*k
    a = dataset_1[l:j,:]
    # print(a.shape)
    l=0
    l=j+50
    dataX=np.append(dataX,a)
    k=k+1
    j=0
  return np.array(dataX)

[yhat_1,yhat_2,yhat_3,yhat_4]=model_2.predict([test_X_1D,test_X_2D])



# [yhat_1,yhat_2,yhat_3,yhat_4,yhat_f,yhat_6,yhat_7,yhat_8]=model_2.predict([test_X_1D,test_X_2D])


# yhat_4=model_2.predict([test_X_1D,test_X_2D])

# yhat_4=yhat_bag_p
# yhat_f=yhat_bag_f

# print(test_y_f.shape)

 ### Present ###
yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))
test_y_r=test_y.reshape((test_y.shape[0]*w,6))

print(yhat_4.shape)
### Future ###

# yhat_f=yhat_f.reshape((yhat_f.shape[0]*w_f,6))
# test_y_f_1=test_y_f.reshape((test_y_f.shape[0]*w_f,6))

### Unpack ###
yhat_up=unpack_dataset_present(np.array(yhat_5),80)
test_y_up=unpack_dataset_present(np.array(test_y_r),80)



print(yhat_up.shape,test_y_up.shape)


### Present ###

yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)
test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)

print(yhat_up.shape,test_y_up.shape)

### Present ###

rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))

print(rmse[0])
print(rmse[1])
print(rmse[2])

m=np.mean(rmse)

print('\n')
print(m)

print('\n')

print(p[0])
print(p[1])
print(p[2])
print('\n')

print(np.mean(p))

RMSE_Gait_JL_A_Net_6=rmse
PCC_Gait_JL_A_Net_6=p

Ablation_16=np.hstack([RMSE_Gait_JL_A_Net_6,PCC_Gait_JL_A_Net_6])

"""# 17. Gait-JL-Net_8+Augmentation"""

#### Main Model ####

inputs_1D = tf.keras.layers.Input( shape=(w,12) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output_3,output=Gait_Net(inputs_1D_N,inputs_2D_N)

model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])
model_2.compile(loss=correlation_coefficient_loss_joint_8, optimizer='Adam', metrics=[correlation_coefficient_loss_joint_8])


history=model_2.fit([np.array(train_X_1D),np.array(train_X_2D)], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=30, batch_size=64, validation_data=([X_validation_1D,\
                                                                      X_validation_2D], [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

filename = '/home/sanzidpr/DeepBBWAE_Net/subject 1/model_Gait_JL_A_Net_8.h5'
model_2.save(filename)
print('>Saved %s' % filename)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()

gc.collect()
gc.collect()
gc.collect()
gc.collect()

# # convert an array of values into a dataset matrix
def unpack_dataset_present(dataset_1, window=80):
  dataX= []
  k=1
  w=100
  l=0
  shift=100
  for i in range(10000):
    j=shift*k
    a = dataset_1[l:j,:]
    # print(a.shape)
    l=0
    l=j+50
    dataX=np.append(dataX,a)
    k=k+1
    j=0
  return np.array(dataX)

[yhat_1,yhat_2,yhat_3,yhat_4]=model_2.predict([test_X_1D,test_X_2D])



# [yhat_1,yhat_2,yhat_3,yhat_4,yhat_f,yhat_6,yhat_7,yhat_8]=model_2.predict([test_X_1D,test_X_2D])


# yhat_4=model_2.predict([test_X_1D,test_X_2D])

# yhat_4=yhat_bag_p
# yhat_f=yhat_bag_f

# print(test_y_f.shape)

 ### Present ###
yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))
test_y_r=test_y.reshape((test_y.shape[0]*w,6))

print(yhat_4.shape)
### Future ###

# yhat_f=yhat_f.reshape((yhat_f.shape[0]*w_f,6))
# test_y_f_1=test_y_f.reshape((test_y_f.shape[0]*w_f,6))

### Unpack ###
yhat_up=unpack_dataset_present(np.array(yhat_5),80)
test_y_up=unpack_dataset_present(np.array(test_y_r),80)



print(yhat_up.shape,test_y_up.shape)


### Present ###

yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)
test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)

print(yhat_up.shape,test_y_up.shape)

### Present ###

rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))

print(rmse[0])
print(rmse[1])
print(rmse[2])

m=np.mean(rmse)

print('\n')
print(m)

print('\n')

print(p[0])
print(p[1])
print(p[2])
print('\n')

print(np.mean(p))

RMSE_Gait_JL_A_Net_8=rmse
PCC_Gait_JL_A_Net_8=p

Ablation_17=np.hstack([RMSE_Gait_JL_A_Net_8,PCC_Gait_JL_A_Net_8])

"""# 18. Gait-JL-Net_10+Augmentation"""

#### Main Model ####

inputs_1D = tf.keras.layers.Input( shape=(w,12) )
inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )


inputs_1D_N=BatchNormalization()(inputs_1D)
inputs_2D_N=BatchNormalization()(inputs_2D)


output_1,output_2,output_3,output=Gait_Net(inputs_1D_N,inputs_2D_N)

model_2 = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])
model_2.compile(loss=correlation_coefficient_loss_joint_10, optimizer='Adam', metrics=[correlation_coefficient_loss_joint_10])


history=model_2.fit([np.array(train_X_1D),np.array(train_X_2D)], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=30, batch_size=64, validation_data=([X_validation_1D,\
                                                                      X_validation_2D], [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)

filename = '/home/sanzidpr/DeepBBWAE_Net/subject 1/model_Gait_JL_A_Net_10.h5'
model_2.save(filename)
print('>Saved %s' % filename)

# # # summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()
model_2.summary()

gc.collect()
gc.collect()
gc.collect()
gc.collect()

# # convert an array of values into a dataset matrix
def unpack_dataset_present(dataset_1, window=80):
  dataX= []
  k=1
  w=100
  l=0
  shift=100
  for i in range(10000):
    j=shift*k
    a = dataset_1[l:j,:]
    # print(a.shape)
    l=0
    l=j+50
    dataX=np.append(dataX,a)
    k=k+1
    j=0
  return np.array(dataX)

[yhat_1,yhat_2,yhat_3,yhat_4]=model_2.predict([test_X_1D,test_X_2D])



# [yhat_1,yhat_2,yhat_3,yhat_4,yhat_f,yhat_6,yhat_7,yhat_8]=model_2.predict([test_X_1D,test_X_2D])


# yhat_4=model_2.predict([test_X_1D,test_X_2D])

# yhat_4=yhat_bag_p
# yhat_f=yhat_bag_f

# print(test_y_f.shape)

 ### Present ###
yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))
test_y_r=test_y.reshape((test_y.shape[0]*w,6))

print(yhat_4.shape)
### Future ###

# yhat_f=yhat_f.reshape((yhat_f.shape[0]*w_f,6))
# test_y_f_1=test_y_f.reshape((test_y_f.shape[0]*w_f,6))

### Unpack ###
yhat_up=unpack_dataset_present(np.array(yhat_5),80)
test_y_up=unpack_dataset_present(np.array(test_y_r),80)



print(yhat_up.shape,test_y_up.shape)


### Present ###

yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)
test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)

print(yhat_up.shape,test_y_up.shape)

### Present ###

rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))

print(rmse[0])
print(rmse[1])
print(rmse[2])

m=np.mean(rmse)

print('\n')
print(m)

print('\n')

print(p[0])
print(p[1])
print(p[2])
print('\n')

print(np.mean(p))

RMSE_Gait_JL_A_Net_10=rmse
PCC_Gait_JL_A_Net_10=p

Ablation_18=np.hstack([RMSE_Gait_JL_A_Net_10,PCC_Gait_JL_A_Net_10])

"""# Results"""

all_results=np.vstack([Ablation_1,Ablation_2,Ablation_3,Ablation_4,Ablation_5,Ablation_6,Ablation_7,Ablation_8,Ablation_9,Ablation_10,Ablation_11,Ablation_12,Ablation_13,Ablation_14,Ablation_15,Ablation_16,Ablation_17,Ablation_18])
from numpy import savetxt

savetxt('/home/sanzidpr/DeepBBWAE_Net/Results/subject_1_results.csv', all_results, delimiter=',')

"""# Ensemble Model (Bagging)"""

#         ### bagboost model 1 ###



# from sklearn.model_selection import KFold
# from sklearn.ensemble import GradientBoostingRegressor
# from sklearn.multioutput import MultiOutputRegressor
# import pickle
# from sklearn.linear_model import Ridge
# import xgboost as xg 

# n_splits=10
# i=0

# for _ in range(n_splits):
#   ix = [i for i in range(len(train_X_2D))]
#   train_ix = resample(ix, replace=True, n_samples=Bag_samples)
#   test_ix = [x for x in ix if x not in train_ix]
#   train_X_1D_bagged=train_X_1D[train_ix]
#   train_X_2D_bagged=train_X_2D[train_ix]
#   train_X_extra_bag=train_X_extra[train_ix]
#   train_y=train_y_5[train_ix]
#   train_y_f=train_y_5_f[train_ix]


  
#   inputs_1D = tf.keras.layers.Input( shape=(w,12) )
#   inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )
#   input_extra=tf.keras.layers.Input(shape=6,dtype=tf.float32)


#   inputs_1D_N=BatchNormalization()(inputs_1D)
#   inputs_2D_N=BatchNormalization()(inputs_2D)


#   Feat_f,output_1,output_2,output_3,output=Gait_Net(inputs_1D_N,inputs_2D_N,input_extra)


#   output_4=Reshape(target_shape=(w,6))(output)

#   # future_features=concatenate([inputs_1D_N,output_4])

#   model_3=Bidirectional(GRU(128,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.000001),return_sequences=True))(output_4)
#   model_3=Dropout(0.25)(model_3)
#   model_3=Bidirectional(GRU(64,kernel_regularizer=l2(0.000001), recurrent_regularizer=l2(0.000001), bias_regularizer=l2(0.0000001),return_sequences=True))(model_3)
#   model_3=Dropout(0.25)(model_3)
#   model_3=Flatten()(model_3)


#   future_features=concatenate([Feat_f,model_3])
#   output_f=Dense(6*w_f,bias_regularizer=l2(0.001), activation='linear')(future_features)
#   model_2 = Model(inputs=[inputs_1D, inputs_2D,input_extra], outputs=[output_1,output_2,output_3,output,output_f])
#   model_2.compile(loss=correlation_coefficient_loss_joint, optimizer='Adam', metrics=[correlation_coefficient_loss_joint])


#   history=model_2.fit([np.array(train_X_1D_bagged),np.array(train_X_2D_bagged),train_X_extra_bag], [train_y,train_y,train_y,train_y,train_y_f], epochs=30, batch_size=64, validation_data=([X_validation_1D,\
#                                                                         X_validation_2D,X_validation_extra], [Y_validation,Y_validation,Y_validation,Y_validation,Y_validation_f]), verbose=2, shuffle=False)

#   # # # summarize history for loss
#   plt.plot(history.history['loss'])
#   plt.plot(history.history['val_loss'])
#   plt.title('model loss')
#   plt.ylabel('loss')
#   plt.xlabel('epoch')
#   plt.legend(['train', 'test'], loc='upper right')
#   plt.show()
#   model_2.summary()
  
#   filename_1 = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 10/conv2D/bagging/model_' + str(i + 1) + '.h5'
#   model_2.save(filename_1)
#   print('>Saved %s' % filename_1)

#   i=i+1

# print(w)

# ###################################################################################################################################################################

# # load models from file
# def load_all_models(n_models):
#   all_models = list()
#   for i in range(n_models):
#       # define filename for this ensemble
#       filename = '/content/drive/My Drive/data for machine learning_stair_slope/ridge/subject 10/conv2D/bagging/model_' + str(i + 1) + '.h5'
#       # load model from file
#       model = tf.keras.models.load_model(filename, custom_objects={'correlation_coefficient_loss_joint': correlation_coefficient_loss_joint})
#       # model = model_from_json(open(filename).read())
#       # model.load_weights(os.path.join(os.path.dirname(filename), 'model_weights.h5'))
#       # add to list of members
#       all_models.append(model)
#       print('>loaded %s' % filename)
#   return all_models


# ###################################################################################################################################################################

# ### Present Prediction####
#       ### Load model and do prediction ###

# n_members =10
# members = load_all_models(n_members)

# print('Loaded %d models' % len(members))


# # create stacked model input dataset as outputs from the ensemble
# def stacked_dataset(members, inputX, inputX_lstm):
#   stackX = None
#   l=1
#   for model in members:
#       # make prediction
#       [yhat_1,yhat_2,yhat_3,yhat,yhat_f]= model.predict([inputX_lstm,inputX], verbose=0)           
#       # stack predictions into [rows, members, probabilities]
#       if stackX is None:
#           stackX = yhat
#       else:
#           stackX = np.dstack((stackX, yhat))
#   # flatten predictions to [rows, members x probabilities]
#   stackX = stackX.reshape((stackX.shape[0], stackX.shape[1],stackX.shape[2]))
#   return stackX



#       ### Fit test data on baggging ensemble ###

# # fit a model based on the outputs from the ensemble members
# def fit_stacked_model_test(members, inputX,inputX_lstm, inputy):
#   # create dataset using ensemble
#   from sklearn.linear_model import Ridge
#   from sklearn.linear_model import LogisticRegression
#   stackedX = stacked_dataset(members, inputX,inputX_lstm)
#   return stackedX   


# ### Test data ###

# stackedX_test= fit_stacked_model_test(members, test_X_2D,test_X_1D, test_y)
# print(stackedX_test.shape) 


# ###################################################################################################################################################################

# ### Future Prediction ###

# # create stacked model input dataset as outputs from the ensemble
# def stacked_dataset(members, inputX, inputX_lstm):
#   stackX = None
#   l=1
#   for model in members:
#       # make prediction
#       [yhat_1,yhat_2,yhat_3,yhat_4,yhat] = model.predict([inputX_lstm,inputX], verbose=0)           
#       # stack predictions into [rows, members, probabilities]
#       if stackX is None:
#           stackX = yhat
#       else:
#           stackX = np.dstack((stackX, yhat))
#   # flatten predictions to [rows, members x probabilities]
#   stackX = stackX.reshape((stackX.shape[0], stackX.shape[1],stackX.shape[2]))
#   return stackX



#       ### Fit test data on baggging ensemble ###

# # fit a model based on the outputs from the ensemble members
# def fit_stacked_model_test(members, inputX,inputX_lstm, inputy):
#   # create dataset using ensemble
#   from sklearn.linear_model import Ridge
#   from sklearn.linear_model import LogisticRegression
#   stackedX = stacked_dataset(members, inputX,inputX_lstm)
#   return stackedX

#       #### Bagging Results ###


# A_1_1=stackedX_test[:,:,0:1]
# A_1=A_1_1.reshape(A_1_1.shape[0]*A_1_1.shape[1])

# A_2_1=stackedX_test[:,:,1:2]
# A_2=A_2_1.reshape(A_2_1.shape[0]*A_2_1.shape[1])

# A_3_1=stackedX_test[:,:,2:3]
# A_3=A_3_1.reshape(A_3_1.shape[0]*A_3_1.shape[1])

# A_4_1=stackedX_test[:,:,3:4]
# A_4=A_4_1.reshape(A_4_1.shape[0]*A_4_1.shape[1])

# A_5_1=stackedX_test[:,:,4:5]
# A_5=A_5_1.reshape(A_5_1.shape[0]*A_5_1.shape[1])

# A_6_1=stackedX_test[:,:,5:6]
# A_6=A_6_1.reshape(A_6_1.shape[0]*A_6_1.shape[1])

# A_7_1=stackedX_test[:,:,6:7]
# A_7=A_7_1.reshape(A_7_1.shape[0]*A_7_1.shape[1])

# A_8_1=stackedX_test[:,:,7:8]
# A_8=A_8_1.reshape(A_8_1.shape[0]*A_8_1.shape[1])

# A_9_1=stackedX_test[:,:,8:9]
# A_9=A_9_1.reshape(A_9_1.shape[0]*A_9_1.shape[1])

# A_10_1=stackedX_test[:,:,9:10]
# A_10=A_10_1.reshape(A_10_1.shape[0]*A_10_1.shape[1])


# B_1=np.array([A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10])
# B=np.average(B_1, weights = [1,1,1,0,0,0,0,0,0,0],axis=0)
# yhat_bag_p=B.reshape(test_y.shape[0],test_y.shape[1])
# print(yhat_bag_p.shape)



# ### Test data ###

# stackedX_test= fit_stacked_model_test(members, test_X_2D,test_X_1D, test_y)
# print(stackedX_test.shape) 


#       #### Bagging Results ###


# A_1_1=stackedX_test[:,:,0:1]
# A_1=A_1_1.reshape(A_1_1.shape[0]*A_1_1.shape[1])

# A_2_1=stackedX_test[:,:,1:2]
# A_2=A_2_1.reshape(A_2_1.shape[0]*A_2_1.shape[1])

# A_3_1=stackedX_test[:,:,2:3]
# A_3=A_3_1.reshape(A_3_1.shape[0]*A_3_1.shape[1])

# A_4_1=stackedX_test[:,:,3:4]
# A_4=A_4_1.reshape(A_4_1.shape[0]*A_4_1.shape[1])

# A_5_1=stackedX_test[:,:,4:5]
# A_5=A_5_1.reshape(A_5_1.shape[0]*A_5_1.shape[1])

# A_6_1=stackedX_test[:,:,5:6]
# A_6=A_6_1.reshape(A_6_1.shape[0]*A_6_1.shape[1])

# A_7_1=stackedX_test[:,:,6:7]
# A_7=A_7_1.reshape(A_7_1.shape[0]*A_7_1.shape[1])

# A_8_1=stackedX_test[:,:,7:8]
# A_8=A_8_1.reshape(A_8_1.shape[0]*A_8_1.shape[1])

# A_9_1=stackedX_test[:,:,8:9]
# A_9=A_9_1.reshape(A_9_1.shape[0]*A_9_1.shape[1])

# A_10_1=stackedX_test[:,:,9:10]
# A_10=A_10_1.reshape(A_10_1.shape[0]*A_10_1.shape[1])


# B_1=np.array([A_1,A_2,A_3,A_4,A_5,A_6,A_7,A_8,A_9,A_10])
# B=np.average(B_1, weights = [1,1,1,0,0,0,0,0,0,0],axis=0)
# yhat_bag_f=B.reshape(test_y_f.shape[0],test_y_f.shape[1])
# print(yhat_bag_f.shape)

# print(yhat_bag_p.shape)
