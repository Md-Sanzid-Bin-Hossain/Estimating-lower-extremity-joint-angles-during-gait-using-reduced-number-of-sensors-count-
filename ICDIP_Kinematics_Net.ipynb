{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "W17jUoA2pwjm",
        "8A4Eoxgop1qX",
        "dbbpf3z7eCAf",
        "bRf9hcyFgZZ1",
        "ROjfjLaBfcBU",
        "NQrV3BIMTZuj",
        "PS3pYaNPgR8g",
        "RjpGdluPLZka",
        "ByxjLTWCQTE7",
        "liw2QcajQxGM",
        "zqNwcGyvLcPx",
        "h_t-B6-pLg8Q",
        "icfDiBX_cqpG",
        "sELQp-gpL-Ww",
        "6CXxNBRuZynf",
        "fpDJxaP4Zqtn",
        "jpfRUIikLxQ4",
        "j6XTj55a9gcU",
        "ZK4fvmjkg7fV",
        "qbYWJ3DyLDSK",
        "PK3E_9xxLDSN",
        "794r5doCLDSP",
        "Kilx5p9SLDSQ",
        "U3A_iQBTX297",
        "IQARpaZR-g-k",
        "zTUsqxDmYdkg",
        "y3DO5r8ZY-qd",
        "6VvB8kwrZWzO",
        "5oAUHymMZgob",
        "XoHyJTRaaC2F",
        "JGOEXFkLaYsc",
        "NM2Lwa-vY9Fs"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanzid-Priam/Estimating-lower-extremity-joint-angles-during-gait-using-reduced-number-of-sensors-count-/blob/main/ICDIP_Kinematics_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ_iJPPKh7zX"
      },
      "source": [
        "# Let`s import all packages that we may need:\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "import statistics\n",
        "from numpy import loadtxt\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import math\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import GRU,LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from statistics import stdev\n",
        "import math\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy.signal import butter,filtfilt\n",
        "\n",
        "import sys\n",
        "import numpy as np # linear algebra\n",
        "from scipy.stats import randint\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL\n",
        "import matplotlib.pyplot as plt # this is used for the plot the graph\n",
        "import seaborn as sns # used for plot interactive graph.\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## for Deep-learing:\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "to_categorical([0, 1, 2, 3], num_classes=4)\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "# from tensorflow.keras.utils import np_utils\n",
        "import itertools\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Bidirectional\n",
        "#import constraint\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.regularizers import l2\n",
        "\n",
        "\n",
        "###  Library for attention layers\n",
        "\n",
        "import pandas as pd\n",
        "#import pyarrow.parquet as pq # Used to read the data\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import * # Keras is the most friendly Neural Network library, this Kernel use a lot of layers classes\n",
        "from tensorflow.keras.models import Model\n",
        "from tqdm import tqdm # Processing time measurement\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import backend as K # The backend give us access to tensorflow operations and allow us to create the Attention class\n",
        "from tensorflow.keras import optimizers # Allow us to access the Adam class to modify some parameters\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold # Used to use Kfold to train our model\n",
        "from tensorflow.keras.callbacks import * # This object helps the model to train in a smarter way, avoiding overfitting\n",
        "\n",
        "from tensorflow.keras.layers import Layer\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import regularizers\n",
        "import statistics\n",
        "import gc\n",
        "import h5py\n",
        "\n",
        "### Early stopping\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W17jUoA2pwjm"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1qJ-_XVjY3z"
      },
      "outputs": [],
      "source": [
        "##IMU, Kin-Kinematics, HoF-Histogram of Features\n",
        "\n",
        "def data_loader(subject):\n",
        "  with h5py.File('/content/drive/My Drive/Kinematics Prediction/All_subjects_data_kinetics.h5', 'r') as hf:\n",
        "    All_subjects = hf['All_subjects']\n",
        "    Subject = All_subjects[subject]\n",
        "\n",
        "    HOF=Subject['hof']\n",
        "    IMU_KIN=Subject['IMU_Kin']\n",
        "\n",
        "    treadmill_hof = HOF['Treadmill']\n",
        "    levelground_hof = HOF['Levelground']\n",
        "    slope_hof = HOF['Slope']\n",
        "    stair_hof = HOF['Stair']\n",
        "    round_hof = HOF['Round']\n",
        "    obstacles_hof = HOF['Obstacles']\n",
        "\n",
        "    treadmill_IMU_kin = IMU_KIN['Treadmill']\n",
        "    levelground_IMU_kin = IMU_KIN['Levelground']\n",
        "    slope_IMU_kin = IMU_KIN['Slope']\n",
        "    stair_IMU_kin = IMU_KIN['Stair']\n",
        "    round_IMU_kin= IMU_KIN['Round']\n",
        "    obstacles_IMU_kin = IMU_KIN['Obstacles']\n",
        "\n",
        "\n",
        "    hof_data=np.concatenate((treadmill_hof,levelground_hof,slope_hof,stair_hof,round_hof,obstacles_hof),axis=0)\n",
        "    IMU_kin_data=np.concatenate((treadmill_IMU_kin,levelground_IMU_kin,slope_IMU_kin,stair_IMU_kin,round_IMU_kin,obstacles_IMU_kin),axis=0)\n",
        "\n",
        "    return np.array(hof_data), np.array(IMU_kin_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53dWXPgXkt4w"
      },
      "outputs": [],
      "source": [
        "subject_1_data_hof, subject_1_data_IMU_Kin=data_loader('Subject_1')\n",
        "gc.collect()\n",
        "subject_2_data_hof, subject_2_data_IMU_Kin=data_loader('Subject_2')\n",
        "gc.collect()\n",
        "subject_3_data_hof, subject_3_data_IMU_Kin=data_loader('Subject_3')\n",
        "gc.collect()\n",
        "subject_4_data_hof, subject_4_data_IMU_Kin=data_loader('Subject_4')\n",
        "gc.collect()\n",
        "subject_5_data_hof, subject_5_data_IMU_Kin=data_loader('Subject_5')\n",
        "gc.collect()\n",
        "subject_6_data_hof, subject_6_data_IMU_Kin=data_loader('Subject_6')\n",
        "gc.collect()\n",
        "subject_7_data_hof, subject_7_data_IMU_Kin=data_loader('Subject_7')\n",
        "gc.collect()\n",
        "subject_8_data_hof, subject_8_data_IMU_Kin=data_loader('Subject_8')\n",
        "gc.collect()\n",
        "subject_9_data_hof, subject_9_data_IMU_Kin=data_loader('Subject_9')\n",
        "gc.collect()\n",
        "subject_10_data_hof, subject_10_data_IMU_Kin=data_loader('Subject_10')\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A4Eoxgop1qX"
      },
      "source": [
        "#Subject Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUDPyqjRoDoZ"
      },
      "outputs": [],
      "source": [
        "main_dir = \"/content/drive/My Drive/public dataset/Public_dataset_2/Subject01\"\n",
        "os.mkdir(main_dir)\n",
        "path=\"/content/\"\n",
        "subject='Subject_01'\n",
        "encoder='lstm'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iMx41lxp4Jp"
      },
      "outputs": [],
      "source": [
        "train_data_hof=np.concatenate((subject_1_data_hof,subject_2_data_hof,subject_4_data_hof,subject_5_data_hof,subject_6_data_hof,\n",
        "                               subject_7_data_hof,subject_8_data_hof,subject_9_data_hof,subject_10_data_hof),axis=0)\n",
        "\n",
        "train_data_IMU_Kin=np.concatenate((subject_1_data_IMU_Kin,subject_2_data_IMU_Kin,subject_4_data_IMU_Kin,subject_5_data_IMU_Kin,subject_6_data_IMU_Kin,\n",
        "                               subject_7_data_IMU_Kin,subject_8_data_IMU_Kin,subject_9_data_IMU_Kin,subject_10_data_IMU_Kin),axis=0)\n",
        "\n",
        "test_data_hof=subject_3_data_hof\n",
        "test_data_IMU_Kin=subject_3_data_IMU_Kin"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing"
      ],
      "metadata": {
        "id": "dbbpf3z7eCAf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGYu4d7pqFe3"
      },
      "outputs": [],
      "source": [
        "##### IMUs-0:48\n",
        "# Sensor 1- Sternum\n",
        "# Sensor 2-Sacrum\n",
        "# Sensor 3-R_thigh\n",
        "# Sensor 4-L_thigh\n",
        "# Sensor 5-R_shank\n",
        "# Sensor 6-L_shank\n",
        "# Sensor 7-R_dorsal\n",
        "# Sensor 8-L_dorsal\n",
        "\n",
        "train_dataset_IMU=train_data_IMU_Kin[:,36:48]\n",
        "train_dataset_hof=train_data_hof\n",
        "train_dataset_target=np.concatenate((train_data_IMU_Kin[:,55:56],train_data_IMU_Kin[:,58:60],train_data_IMU_Kin[:,62:63],train_data_IMU_Kin[:,65:67]),axis=1) ## Left and right leg hip, knee,ankle angle\n",
        "\n",
        "\n",
        "test_dataset_IMU=test_data_IMU_Kin[:,36:48]\n",
        "test_dataset_hof=test_data_hof\n",
        "test_dataset_target=np.concatenate((test_data_IMU_Kin[:,55:56],test_data_IMU_Kin[:,58:60],test_data_IMU_Kin[:,62:63],test_data_IMU_Kin[:,65:67]),axis=1)\n",
        "\n",
        "print(train_dataset_IMU.shape)\n",
        "print(train_dataset_hof.shape)\n",
        "print(train_dataset_target.shape)\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRf9hcyFgZZ1"
      },
      "source": [
        "# Data creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF05NJYvUfuB"
      },
      "source": [
        "## Creating a dataset with overlapping window of 100 samples with overlap of 50 samples ##\n",
        "\n",
        "# # convert an array of values into a dataset matrix\n",
        "def create_dataset_present(dataset_1, window=100):\n",
        "  dataX= []\n",
        "  k=0\n",
        "  shift=50\n",
        "  for i in range(int(len(dataset_1)/shift)-1):\n",
        "    j=shift*k\n",
        "    a = dataset_1[j:j+window,:]\n",
        "    dataX.append(a)\n",
        "    k=k+1\n",
        "  return np.array(dataX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNc_iwMpXkL4"
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWYwyhYhVImc"
      },
      "source": [
        "### Reconstruction/Present Dataset ###\n",
        "w=100\n",
        "\n",
        "train_X_3=create_dataset_present(train_dataset_IMU,w)\n",
        "train_y_3=create_dataset_present(train_dataset_target,w)\n",
        "\n",
        "test_X_1D=create_dataset_present(test_dataset_IMU,w)\n",
        "test_y=create_dataset_present(test_dataset_target,w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7R8lC06V537"
      },
      "source": [
        "train_y_3=train_y_3.reshape(train_y_3.shape[0],w*6)\n",
        "test_y=test_y.reshape(test_y.shape[0],w*6)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pUXjTNPG2LT"
      },
      "source": [
        "train_X_1D, X_validation_1D, train_y_5, Y_validation = train_test_split(train_X_3,train_y_3, test_size=0.20, random_state=True)\n",
        "\n",
        "\n",
        "train_X_1D=train_X_1D\n",
        "X_validation_1D=X_validation_1D\n",
        "test_X_1D=test_X_1D\n",
        "\n",
        "print(train_X_1D.shape,train_y_5.shape,X_validation_1D.shape,Y_validation.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBEpgkovHByF"
      },
      "source": [
        "features=6\n",
        "\n",
        "train_X_2D=train_X_1D[:,:,0:12].reshape(train_X_1D.shape[0],train_X_1D.shape[1],features,2)\n",
        "test_X_2D=test_X_1D[:,:,0:12].reshape(test_X_1D.shape[0],test_X_1D.shape[1],features,2)\n",
        "X_validation_2D= X_validation_1D[:,:,0:12].reshape(X_validation_1D.shape[0],\n",
        "                                                   X_validation_1D.shape[1],features,2)\n",
        "\n",
        "\n",
        "print(train_X_2D.shape,test_X_2D.shape,X_validation_2D.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROjfjLaBfcBU"
      },
      "source": [
        "# Different Function for models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92kuqtl9yfap"
      },
      "source": [
        "def prediction_test(yhat,test_y_up):\n",
        "\n",
        "    test_o=test_y_up\n",
        "    yhat=yhat\n",
        "\n",
        "    y_1=yhat[:,0]\n",
        "    y_2=yhat[:,1]\n",
        "    y_3=yhat[:,2]\n",
        "    y_4=yhat[:,3]\n",
        "    y_5=yhat[:,4]\n",
        "    y_6=yhat[:,5]\n",
        "\n",
        "\n",
        "    y_test_1=test_o[:,0]\n",
        "    y_test_2=test_o[:,1]\n",
        "    y_test_3=test_o[:,2]\n",
        "    y_test_4=test_o[:,3]\n",
        "    y_test_5=test_o[:,4]\n",
        "    y_test_6=test_o[:,5]\n",
        "\n",
        "\n",
        "    ###calculate RMSE\n",
        "\n",
        "    rmse_1 =np.sqrt(mean_squared_error(y_test_1,y_1))\n",
        "    rmse_2 =np.sqrt(mean_squared_error(y_test_2,y_2))\n",
        "    rmse_3 =np.sqrt(mean_squared_error(y_test_3,y_3))\n",
        "    rmse_4 =np.sqrt(mean_squared_error(y_test_4,y_4))\n",
        "    rmse_5 =np.sqrt(mean_squared_error(y_test_5,y_5))\n",
        "    rmse_6 =np.sqrt(mean_squared_error(y_test_6,y_6))\n",
        "\n",
        "\n",
        "    p_1=np.corrcoef(y_1, y_test_1)[0, 1]\n",
        "    p_2=np.corrcoef(y_2, y_test_2)[0, 1]\n",
        "    p_3=np.corrcoef(y_3, y_test_3)[0, 1]\n",
        "    p_4=np.corrcoef(y_4, y_test_4)[0, 1]\n",
        "    p_5=np.corrcoef(y_5, y_test_5)[0, 1]\n",
        "    p_6=np.corrcoef(y_6, y_test_6)[0, 1]\n",
        "\n",
        "    ### Getiing single RMSE and PCC value for a joint\n",
        "    p=np.array([(p_1+p_4)/2,(p_2+p_5)/2,(p_3+p_6)/2])\n",
        "\n",
        "    rmse=np.array([(rmse_1+rmse_4)/2,(rmse_2+rmse_5)/2,(rmse_3+rmse_6)/2])\n",
        "\n",
        "\n",
        "\n",
        "    return rmse,p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAW6trUjyt8F"
      },
      "source": [
        "## Evaluting data with the original form without any overlaps\n",
        "\n",
        "# # convert an array of values into a dataset matrix\n",
        "def unpack_dataset_present(dataset_1):\n",
        "  dataX= []\n",
        "  k=1\n",
        "  l=0\n",
        "  shift=100\n",
        "  for i in range(int(len(dataset_1)/shift)-1):\n",
        "    j=shift*k\n",
        "    a = dataset_1[l:j,:]\n",
        "    l=0\n",
        "    l=j+50\n",
        "    dataX=np.append(dataX,a)\n",
        "    k=k+1\n",
        "    j=0\n",
        "  return np.array(dataX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQrV3BIMTZuj"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tIqJaVFwpqx"
      },
      "source": [
        "def correlation_coefficient_loss_RMSE(y_true, y_pred):\n",
        "    x = y_true\n",
        "    y = y_pred\n",
        "    mx = K.mean(x)\n",
        "    l1=K.sqrt(K.mean(K.square(y - x)))\n",
        "\n",
        "    return l1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-yCSvJfJ5RG"
      },
      "source": [
        "def correlation_coefficient_loss_joint(y_true, y_pred):\n",
        "    # Calculate mean values\n",
        "    mx = tf.reduce_mean(y_true)\n",
        "    my = tf.reduce_mean(y_pred)\n",
        "\n",
        "    # Calculate differences from mean\n",
        "    xm, ym = y_true - mx, y_pred - my\n",
        "\n",
        "    # Calculate numerator and denominator of Pearson correlation coefficient\n",
        "    r_num = tf.reduce_sum(tf.multiply(xm, ym))\n",
        "    r_den = tf.sqrt(tf.multiply(tf.reduce_sum(tf.square(xm)), tf.reduce_sum(tf.square(ym))))\n",
        "\n",
        "    # Calculate Pearson correlation coefficient\n",
        "    r = r_num / r_den\n",
        "\n",
        "    # Clamp r between -1 and 1\n",
        "    r = tf.maximum(tf.minimum(r, 1.0), -1.0)\n",
        "\n",
        "    # Calculate l2 loss\n",
        "    l2 = 1 - tf.square(r)\n",
        "\n",
        "    l1=K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
        "\n",
        "\n",
        "    return l1+l2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Ablation"
      ],
      "metadata": {
        "id": "PS3pYaNPgR8g"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjpGdluPLZka"
      },
      "source": [
        "##1. GRU-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34E-9Az5HA2T"
      },
      "source": [
        "def GRU_Net(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  model_1=Bidirectional(GRU(128,return_sequences=True))(inputs_1D_N)\n",
        "  model_1=Dropout(0.4)(model_1)\n",
        "  model_1=Bidirectional(GRU(64,return_sequences=True))(model_1)\n",
        "  model_1=Dropout(0.4)(model_1)\n",
        "  model_1=Flatten()(model_1)\n",
        "\n",
        "  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)\n",
        "\n",
        "\n",
        "  return output_GRU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Main Model ####\n",
        "\n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,12) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output=GRU_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "model.compile(loss=correlation_coefficient_loss_RMSE, optimizer='Adam')\n",
        "\n",
        "\n",
        "history=model.fit([np.array(train_X_1D),np.array(train_X_2D)], train_y_5, epochs=30, batch_size=64, validation_data=([X_validation_1D,\\\n",
        "                                                                      X_validation_2D], Y_validation), verbose=2, shuffle=False)\n",
        "\n",
        "filename = path+'model_GRU.h5'\n",
        "model.save(filename)\n",
        "print('>Saved %s' % filename)\n",
        "\n",
        "# # # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "KQgazOljbAu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIqRgq5zbD-w"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0M318omQbD-x"
      },
      "outputs": [],
      "source": [
        "model_path = path+'model_GRU.h5'\n",
        "\n",
        "model=load_model(model_path, custom_objects={'correlation_coefficient_loss_RMSE': correlation_coefficient_loss_RMSE})\n",
        "\n",
        "yhat_4=model.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "\n",
        " ### Present ###\n",
        "yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))\n",
        "test_y_r=test_y.reshape((test_y.shape[0]*w,6))\n",
        "\n",
        "print(yhat_4.shape)\n",
        "\n",
        "\n",
        "### Unpack ###\n",
        "yhat_up=unpack_dataset_present(np.array(yhat_5))\n",
        "test_y_up=unpack_dataset_present(np.array(test_y_r))\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)\n",
        "\n",
        "### Present ###\n",
        "\n",
        "yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)\n",
        "test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TK8a1NPztYDH"
      },
      "outputs": [],
      "source": [
        "### Present ###\n",
        "\n",
        "rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))\n",
        "\n",
        "print(rmse[0])\n",
        "print(rmse[1])\n",
        "print(rmse[2])\n",
        "\n",
        "m=np.mean(rmse)\n",
        "\n",
        "print('\\n')\n",
        "print(m)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(p[0])\n",
        "print(p[1])\n",
        "print(p[2])\n",
        "print('\\n')\n",
        "\n",
        "print(np.mean(p))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s__F7s8yy6aq"
      },
      "outputs": [],
      "source": [
        "RMSE_GRU=rmse\n",
        "PCC_GRU=p\n",
        "\n",
        "Ablation_1=np.hstack([RMSE_GRU,PCC_GRU])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByxjLTWCQTE7"
      },
      "source": [
        "## 2. Conv1D-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Wo6lmnHQWYB"
      },
      "source": [
        "def Conv1D_Net(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)\n",
        "  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "\n",
        "  CNN=Dense(64, activation='relu')(CNN)\n",
        "  CNN=Dropout(0.2)(CNN)\n",
        "  CNN=Dense(32, activation='relu')(CNN)\n",
        "  CNN=Dropout(0.2)(CNN)\n",
        "\n",
        "  CNN=Flatten()(CNN)\n",
        "\n",
        "  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)\n",
        "\n",
        "\n",
        "  return output_C1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whrXofFhfjK1"
      },
      "outputs": [],
      "source": [
        "#### Main Model ####\n",
        "\n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,12) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "output=Conv1D_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "model.compile(loss=correlation_coefficient_loss_RMSE, optimizer='Adam')\n",
        "\n",
        "\n",
        "history=model.fit([np.array(train_X_1D),np.array(train_X_2D)], train_y_5, epochs=30, batch_size=64, validation_data=([X_validation_1D,\\\n",
        "                                                                      X_validation_2D], Y_validation), verbose=2, shuffle=False)\n",
        "\n",
        "filename = path+'model_Conv1D.h5'\n",
        "model.save(filename)\n",
        "print('>Saved %s' % filename)\n",
        "\n",
        "# # # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCIgxziNfjK7"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFU8RltzfjK7"
      },
      "outputs": [],
      "source": [
        "model_path = path+'model_Conv1D.h5'\n",
        "\n",
        "model=load_model(model_path, custom_objects={'correlation_coefficient_loss_RMSE': correlation_coefficient_loss_RMSE})\n",
        "\n",
        "yhat_4=model.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "\n",
        " ### Present ###\n",
        "yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))\n",
        "test_y_r=test_y.reshape((test_y.shape[0]*w,6))\n",
        "\n",
        "print(yhat_4.shape)\n",
        "\n",
        "\n",
        "### Unpack ###\n",
        "yhat_up=unpack_dataset_present(np.array(yhat_5))\n",
        "test_y_up=unpack_dataset_present(np.array(test_y_r))\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)\n",
        "\n",
        "### Present ###\n",
        "\n",
        "yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)\n",
        "test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pux5Bu47fjK7"
      },
      "outputs": [],
      "source": [
        "### Present ###\n",
        "\n",
        "rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))\n",
        "\n",
        "print(rmse[0])\n",
        "print(rmse[1])\n",
        "print(rmse[2])\n",
        "\n",
        "m=np.mean(rmse)\n",
        "\n",
        "print('\\n')\n",
        "print(m)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(p[0])\n",
        "print(p[1])\n",
        "print(p[2])\n",
        "print('\\n')\n",
        "\n",
        "print(np.mean(p))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOdSRn6tfjK7"
      },
      "outputs": [],
      "source": [
        "RMSE_Conv1D=rmse\n",
        "PCC_Conv1D=p\n",
        "\n",
        "Ablation_2=np.hstack([RMSE_Conv1D,PCC_Conv1D])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liw2QcajQxGM"
      },
      "source": [
        "## 3. Conv2D-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ERSM-dfQ0bZ"
      },
      "source": [
        "def Conv2D_Net(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  X=Conv2D(64, (3, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(128, (3, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Dense(64, activation='relu')(X)\n",
        "\n",
        "  X=Dropout(0.2)(X)\n",
        "  X=Dense(32,activation='relu')(X)\n",
        "  X=Dropout(0.2)(X)\n",
        "\n",
        "  X=Flatten()(X)\n",
        "  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)\n",
        "\n",
        "\n",
        "  return output_C2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bOXNjk73ZbD"
      },
      "outputs": [],
      "source": [
        "#### Main Model ####\n",
        "\n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,12) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output=Conv2D_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "model.compile(loss=correlation_coefficient_loss_RMSE, optimizer='Adam')\n",
        "\n",
        "\n",
        "history=model.fit([np.array(train_X_1D),np.array(train_X_2D)], train_y_5, epochs=30, batch_size=64, validation_data=([X_validation_1D,\\\n",
        "                                                                      X_validation_2D], Y_validation), verbose=2, shuffle=False)\n",
        "\n",
        "filename = path+'model_Conv2D.h5'\n",
        "model.save(filename)\n",
        "print('>Saved %s' % filename)\n",
        "\n",
        "# # # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvJb3tKa3ZbD"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNQJkWzR3ZbD"
      },
      "outputs": [],
      "source": [
        "model_path = path+'model_Conv2D.h5'\n",
        "\n",
        "model=load_model(model_path, custom_objects={'correlation_coefficient_loss_RMSE': correlation_coefficient_loss_RMSE})\n",
        "\n",
        "yhat_4=model.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "\n",
        " ### Present ###\n",
        "yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))\n",
        "test_y_r=test_y.reshape((test_y.shape[0]*w,6))\n",
        "\n",
        "print(yhat_4.shape)\n",
        "\n",
        "\n",
        "### Unpack ###\n",
        "yhat_up=unpack_dataset_present(np.array(yhat_5))\n",
        "test_y_up=unpack_dataset_present(np.array(test_y_r))\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)\n",
        "\n",
        "### Present ###\n",
        "\n",
        "yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)\n",
        "test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJqZAmN53ZbD"
      },
      "outputs": [],
      "source": [
        "### Present ###\n",
        "\n",
        "rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))\n",
        "\n",
        "print(rmse[0])\n",
        "print(rmse[1])\n",
        "print(rmse[2])\n",
        "\n",
        "m=np.mean(rmse)\n",
        "\n",
        "print('\\n')\n",
        "print(m)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(p[0])\n",
        "print(p[1])\n",
        "print(p[2])\n",
        "print('\\n')\n",
        "\n",
        "print(np.mean(p))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KC99sIDT3ZbD"
      },
      "outputs": [],
      "source": [
        "RMSE_Conv2D=rmse\n",
        "PCC_Conv2D=p\n",
        "\n",
        "Ablation_3=np.hstack([RMSE_Conv2D,PCC_Conv2D])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqNwcGyvLcPx"
      },
      "source": [
        "##4. GRU-Conv1D-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCeg7Fm_Io6r"
      },
      "source": [
        "def GRU_Conv1D_Net(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  model_3=Bidirectional(GRU(128,return_sequences=True))(inputs_1D_N)\n",
        "  model_3=Dropout(0.4)(model_3)\n",
        "  model_3=Bidirectional(GRU(64,return_sequences=True))(model_3)\n",
        "  model_3=Dropout(0.4)(model_3)\n",
        "  model_3=Flatten()(model_3)\n",
        "\n",
        "  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)\n",
        "  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "\n",
        "\n",
        "  CNN=Dense(64, activation='relu')(CNN)\n",
        "  CNN=Dropout(0.2)(CNN)\n",
        "  CNN=Dense(32, activation='relu')(CNN)\n",
        "  CNN=Dropout(0.2)(CNN)\n",
        "\n",
        "  CNN_1=Flatten()(CNN)\n",
        "  CNN= concatenate([model_3,CNN_1])\n",
        "\n",
        "  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)\n",
        "\n",
        "\n",
        "  return output_C1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Main Model ####\n",
        "\n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,12) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "output=GRU_Conv1D_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "model.compile(loss=correlation_coefficient_loss_RMSE, optimizer='Adam')\n",
        "\n",
        "\n",
        "history=model.fit([np.array(train_X_1D),np.array(train_X_2D)], train_y_5, epochs=30, batch_size=64, validation_data=([X_validation_1D,\\\n",
        "                                                                      X_validation_2D], Y_validation), verbose=2, shuffle=False)\n",
        "\n",
        "filename = path+'model_GRU_Conv1D.h5'\n",
        "model.save(filename)\n",
        "print('>Saved %s' % filename)\n",
        "\n",
        "# # # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "X8Jn9dOplvEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZtTMrJJ5_vJ"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7s7ifLf55_vJ"
      },
      "outputs": [],
      "source": [
        "model_path = path+'model_GRU_Conv1D.h5'\n",
        "model=load_model(model_path, custom_objects={'correlation_coefficient_loss_RMSE': correlation_coefficient_loss_RMSE})\n",
        "\n",
        "\n",
        "yhat_4=model.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "\n",
        " ### Present ###\n",
        "yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))\n",
        "test_y_r=test_y.reshape((test_y.shape[0]*w,6))\n",
        "\n",
        "print(yhat_4.shape)\n",
        "\n",
        "\n",
        "### Unpack ###\n",
        "yhat_up=unpack_dataset_present(np.array(yhat_5))\n",
        "test_y_up=unpack_dataset_present(np.array(test_y_r))\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)\n",
        "\n",
        "### Present ###\n",
        "\n",
        "yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)\n",
        "test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdsED4O-5_vK"
      },
      "outputs": [],
      "source": [
        "### Present ###\n",
        "\n",
        "rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))\n",
        "\n",
        "print(rmse[0])\n",
        "print(rmse[1])\n",
        "print(rmse[2])\n",
        "\n",
        "m=np.mean(rmse)\n",
        "\n",
        "print('\\n')\n",
        "print(m)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(p[0])\n",
        "print(p[1])\n",
        "print(p[2])\n",
        "print('\\n')\n",
        "\n",
        "print(np.mean(p))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6g0Koh-5_vK"
      },
      "outputs": [],
      "source": [
        "RMSE_GRU_Conv1D=rmse\n",
        "PCC_GRU_Conv1D=p\n",
        "\n",
        "Ablation_4=np.hstack([RMSE_GRU_Conv1D,PCC_GRU_Conv1D])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_t-B6-pLg8Q"
      },
      "source": [
        "##5. GRU-Conv2D-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHtVi07VLjJo"
      },
      "source": [
        "def GRU_Conv2D_Net(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  model_2=Bidirectional(GRU(128,return_sequences=True))(inputs_1D_N)\n",
        "  model_2=Dropout(0.4)(model_2)\n",
        "  model_2=Bidirectional(GRU(64,return_sequences=True))(model_2)\n",
        "  model_2=Dropout(0.4)(model_2)\n",
        "  model_2=Flatten()(model_2)\n",
        "\n",
        "  X=Conv2D(64, (3, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(128, (3, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "\n",
        "  X=Dense(64, activation='relu')(X)\n",
        "  X=Dropout(0.2)(X)\n",
        "  X=Dense(32,activation='relu')(X)\n",
        "  X=Dropout(0.2)(X)\n",
        "\n",
        "  X_1=Flatten()(X)\n",
        "  X= concatenate([model_2,X_1])\n",
        "\n",
        "  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)\n",
        "\n",
        "\n",
        "  return output_C2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOYFllxg7EmP"
      },
      "outputs": [],
      "source": [
        "#### Main Model ####\n",
        "\n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,12) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output=GRU_Conv2D_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "model.compile(loss=correlation_coefficient_loss_RMSE, optimizer='Adam')\n",
        "\n",
        "\n",
        "history=model.fit([np.array(train_X_1D),np.array(train_X_2D)], train_y_5, epochs=30, batch_size=64, validation_data=([X_validation_1D,\\\n",
        "                                                                      X_validation_2D], Y_validation), verbose=2, shuffle=False)\n",
        "\n",
        "filename = path+'model_GRU_Conv2D.h5'\n",
        "model.save(filename)\n",
        "print('>Saved %s' % filename)\n",
        "\n",
        "# # # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKaXjZ_I7EmQ"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1ONq4xM7EmQ"
      },
      "outputs": [],
      "source": [
        "model_path = path+'model_GRU_Conv2D.h5'\n",
        "model=load_model(model_path, custom_objects={'correlation_coefficient_loss_RMSE': correlation_coefficient_loss_RMSE})\n",
        "\n",
        "yhat_4=model.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "\n",
        " ### Present ###\n",
        "yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))\n",
        "test_y_r=test_y.reshape((test_y.shape[0]*w,6))\n",
        "\n",
        "print(yhat_4.shape)\n",
        "\n",
        "\n",
        "### Unpack ###\n",
        "yhat_up=unpack_dataset_present(np.array(yhat_5))\n",
        "test_y_up=unpack_dataset_present(np.array(test_y_r))\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)\n",
        "\n",
        "### Present ###\n",
        "\n",
        "yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)\n",
        "test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixDOV2Ux7EmQ"
      },
      "outputs": [],
      "source": [
        "### Present ###\n",
        "\n",
        "rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))\n",
        "\n",
        "print(rmse[0])\n",
        "print(rmse[1])\n",
        "print(rmse[2])\n",
        "\n",
        "m=np.mean(rmse)\n",
        "\n",
        "print('\\n')\n",
        "print(m)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(p[0])\n",
        "print(p[1])\n",
        "print(p[2])\n",
        "print('\\n')\n",
        "\n",
        "print(np.mean(p))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5p1zOvBl7EmQ"
      },
      "outputs": [],
      "source": [
        "RMSE_GRU_Conv2D=rmse\n",
        "PCC_GRU_Conv2D=p\n",
        "\n",
        "Ablation_5=np.hstack([RMSE_GRU_Conv2D,PCC_GRU_Conv2D])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icfDiBX_cqpG"
      },
      "source": [
        "##6. Conv1D-Conv2D-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3jlZlyNcqpK"
      },
      "source": [
        "def Conv1D_Conv2D_Net(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  X=Conv2D(64, (3, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(128, (3, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "\n",
        "  X=Dense(64, activation='relu')(X)\n",
        "  X=Dropout(0.2)(X)\n",
        "  X=Dense(32,activation='relu')(X)\n",
        "  X=Dropout(0.2)(X)\n",
        "  X_1=Flatten()(X)\n",
        "\n",
        "  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)\n",
        "  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Dense(64, activation='relu')(CNN)\n",
        "  CNN=Dropout(0.2)(CNN)\n",
        "  CNN=Dense(32, activation='relu')(CNN)\n",
        "  CNN=Dropout(0.2)(CNN)\n",
        "  CNN_1=Flatten()(CNN)\n",
        "\n",
        "  CNN= concatenate([X_1,CNN_1])\n",
        "  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)\n",
        "\n",
        "\n",
        "  return output_C1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDRmk-nb98pB"
      },
      "outputs": [],
      "source": [
        "#### Main Model ####\n",
        "\n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,12) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output=Conv1D_Conv2D_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model = Model(inputs=[inputs_1D, inputs_2D], outputs=output)\n",
        "model.compile(loss=correlation_coefficient_loss_RMSE, optimizer='Adam')\n",
        "\n",
        "\n",
        "history=model.fit([np.array(train_X_1D),np.array(train_X_2D)], train_y_5, epochs=30, batch_size=64, validation_data=([X_validation_1D,\\\n",
        "                                                                      X_validation_2D], Y_validation), verbose=2, shuffle=False)\n",
        "\n",
        "filename = path+'model_Conv1D_Conv2D.h5'\n",
        "model.save(filename)\n",
        "print('>Saved %s' % filename)\n",
        "\n",
        "# # # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyGfXdG698pB"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uag-Kjcr98pB"
      },
      "outputs": [],
      "source": [
        "model_path = path+'model_Conv1D_Conv2D.h5'\n",
        "model=load_model(model_path, custom_objects={'correlation_coefficient_loss_RMSE': correlation_coefficient_loss_RMSE})\n",
        "\n",
        "yhat_4=model.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "\n",
        " ### Present ###\n",
        "yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))\n",
        "test_y_r=test_y.reshape((test_y.shape[0]*w,6))\n",
        "\n",
        "print(yhat_4.shape)\n",
        "\n",
        "\n",
        "### Unpack ###\n",
        "yhat_up=unpack_dataset_present(np.array(yhat_5))\n",
        "test_y_up=unpack_dataset_present(np.array(test_y_r))\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)\n",
        "\n",
        "### Present ###\n",
        "\n",
        "yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)\n",
        "test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8oj2kd998pB"
      },
      "outputs": [],
      "source": [
        "### Present ###\n",
        "\n",
        "rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))\n",
        "\n",
        "print(rmse[0])\n",
        "print(rmse[1])\n",
        "print(rmse[2])\n",
        "\n",
        "m=np.mean(rmse)\n",
        "\n",
        "print('\\n')\n",
        "print(m)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(p[0])\n",
        "print(p[1])\n",
        "print(p[2])\n",
        "print('\\n')\n",
        "\n",
        "print(np.mean(p))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zURZeTd098pB"
      },
      "outputs": [],
      "source": [
        "RMSE_Conv1D_Conv2D=rmse\n",
        "PCC_Conv1D_Conv2D=p\n",
        "\n",
        "Ablation_6=np.hstack([RMSE_Conv1D_Conv2D,PCC_Conv1D_Conv2D])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sELQp-gpL-Ww"
      },
      "source": [
        "##7. Gait-SubNet-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4E8_7qSZY-R"
      },
      "source": [
        "def Gait_SubNet_1(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  model_1=Bidirectional(GRU(128,return_sequences=True))(inputs_1D_N)\n",
        "  model_1=Dropout(0.4)(model_1)\n",
        "  model_1=Bidirectional(GRU(64,return_sequences=True))(model_1)\n",
        "  model_1=Dropout(0.4)(model_1)\n",
        "  model_1=Flatten()(model_1)\n",
        "\n",
        "  model_3=Bidirectional(GRU(128,return_sequences=True))(inputs_1D_N)\n",
        "  model_3=Dropout(0.4)(model_3)\n",
        "  model_3=Bidirectional(GRU(64,return_sequences=True))(model_3)\n",
        "  model_3=Dropout(0.4)(model_3)\n",
        "  model_3=Flatten()(model_3)\n",
        "\n",
        "  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)\n",
        "  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "\n",
        "  CNN=Dense(64, activation='relu')(CNN)\n",
        "  CNN=Dropout(0.2)(CNN)\n",
        "  CNN=Dense(32, activation='relu')(CNN)\n",
        "  CNN=Dropout(0.2)(CNN)\n",
        "  CNN_1=Flatten()(CNN)\n",
        "  CNN= concatenate([model_3,CNN_1])\n",
        "\n",
        "  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)\n",
        "  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)\n",
        "\n",
        "  output = Average()([output_GRU,output_C1])\n",
        "\n",
        "\n",
        "  return (output_GRU,output_C1,output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQEm_kHl-Of3"
      },
      "outputs": [],
      "source": [
        "#### Main Model ####\n",
        "\n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,12) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output_1,output_2,output=Gait_SubNet_1(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output])\n",
        "model.compile(loss=correlation_coefficient_loss_RMSE, optimizer='Adam')\n",
        "\n",
        "\n",
        "history=model.fit([np.array(train_X_1D),np.array(train_X_2D)], [train_y_5,train_y_5,train_y_5], epochs=30, batch_size=64, validation_data=([X_validation_1D,\\\n",
        "                                                                      X_validation_2D], [Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)\n",
        "\n",
        "filename = path+'model_Gait_subNet_1.h5'\n",
        "model.save(filename)\n",
        "print('>Saved %s' % filename)\n",
        "\n",
        "# # # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ey_7MA9B-Of3"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrjrpFzs-Of3"
      },
      "outputs": [],
      "source": [
        "model_path = path+'model_Gait_subNet_1.h5'\n",
        "model=load_model(model_path, custom_objects={'correlation_coefficient_loss_RMSE': correlation_coefficient_loss_RMSE})\n",
        "\n",
        "yhat_1,yhat_2,yhat_4=model.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "### Present ###\n",
        "yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))\n",
        "test_y_r=test_y.reshape((test_y.shape[0]*w,6))\n",
        "\n",
        "print(yhat_4.shape)\n",
        "\n",
        "### Unpack ###\n",
        "yhat_up=unpack_dataset_present(np.array(yhat_5))\n",
        "test_y_up=unpack_dataset_present(np.array(test_y_r))\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)\n",
        "\n",
        "### Present ###\n",
        "\n",
        "yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)\n",
        "test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u51ePtJ8-Of4"
      },
      "outputs": [],
      "source": [
        "### Present ###\n",
        "\n",
        "rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))\n",
        "\n",
        "print(rmse[0])\n",
        "print(rmse[1])\n",
        "print(rmse[2])\n",
        "\n",
        "m=np.mean(rmse)\n",
        "\n",
        "print('\\n')\n",
        "print(m)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(p[0])\n",
        "print(p[1])\n",
        "print(p[2])\n",
        "print('\\n')\n",
        "\n",
        "print(np.mean(p))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMzitUWT-Of4"
      },
      "outputs": [],
      "source": [
        "RMSE_Gait_subNet_1=rmse\n",
        "PCC_Gait_subNet_1=p\n",
        "\n",
        "Ablation_7=np.hstack([RMSE_Gait_subNet_1,PCC_Gait_subNet_1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CXxNBRuZynf"
      },
      "source": [
        "##8. Gait-SubNet-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkgxLz6pZynf"
      },
      "source": [
        "def Gait_SubNet_2(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  model_1=Bidirectional(GRU(128,return_sequences=True))(inputs_1D_N)\n",
        "  model_1=Dropout(0.4)(model_1)\n",
        "  model_1=Bidirectional(GRU(64,return_sequences=True))(model_1)\n",
        "  model_1=Dropout(0.4)(model_1)\n",
        "  model_1=Flatten()(model_1)\n",
        "\n",
        "  model_2=Bidirectional(GRU(128,return_sequences=True))(inputs_1D_N)\n",
        "  model_2=Dropout(0.4)(model_2)\n",
        "  model_2=Bidirectional(GRU(64,return_sequences=True))(model_2)\n",
        "  model_2=Dropout(0.4)(model_2)\n",
        "  model_2=Flatten()(model_2)\n",
        "\n",
        "  X=Conv2D(64, (3, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(128, (3, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Dense(64, activation='relu')(X)\n",
        "  X=Dropout(0.2)(X)\n",
        "  X=Dense(32,activation='relu')(X)\n",
        "  X=Dropout(0.2)(X)\n",
        "\n",
        "  X_1=Flatten()(X)\n",
        "  X= concatenate([model_2,X_1])\n",
        "\n",
        "  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)\n",
        "  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)\n",
        "\n",
        "  output = Average()([output_GRU,output_C2])\n",
        "\n",
        "  return (output_GRU,output_C2,output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-1elo0bB5gB"
      },
      "outputs": [],
      "source": [
        "#### Main Model ####\n",
        "\n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,12) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output_1,output_2,output=Gait_SubNet_2(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output])\n",
        "model.compile(loss=correlation_coefficient_loss_RMSE, optimizer='Adam')\n",
        "\n",
        "\n",
        "history=model.fit([np.array(train_X_1D),np.array(train_X_2D)], [train_y_5,train_y_5,train_y_5], epochs=30, batch_size=64, validation_data=([X_validation_1D,\\\n",
        "                                                                      X_validation_2D], [Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)\n",
        "\n",
        "filename = path+'model_Gait_subNet_2.h5'\n",
        "model.save(filename)\n",
        "print('>Saved %s' % filename)\n",
        "\n",
        "# # # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F13g30mtB5gC"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUJjWnx0B5gC"
      },
      "outputs": [],
      "source": [
        "model_path = path+'model_Gait_subNet_2.h5'\n",
        "model=load_model(model_path, custom_objects={'correlation_coefficient_loss_RMSE': correlation_coefficient_loss_RMSE})\n",
        "\n",
        "yhat_1,yhat_2,yhat_4=model.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "### Present ###\n",
        "yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))\n",
        "test_y_r=test_y.reshape((test_y.shape[0]*w,6))\n",
        "\n",
        "print(yhat_4.shape)\n",
        "\n",
        "\n",
        "### Unpack ###\n",
        "yhat_up=unpack_dataset_present(np.array(yhat_5))\n",
        "test_y_up=unpack_dataset_present(np.array(test_y_r))\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)\n",
        "\n",
        "### Present ###\n",
        "\n",
        "yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)\n",
        "test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQXgMjm5B5gC"
      },
      "outputs": [],
      "source": [
        "### Present ###\n",
        "\n",
        "rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))\n",
        "\n",
        "print(rmse[0])\n",
        "print(rmse[1])\n",
        "print(rmse[2])\n",
        "\n",
        "m=np.mean(rmse)\n",
        "\n",
        "print('\\n')\n",
        "print(m)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(p[0])\n",
        "print(p[1])\n",
        "print(p[2])\n",
        "print('\\n')\n",
        "\n",
        "print(np.mean(p))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyKs_jLvB5gC"
      },
      "outputs": [],
      "source": [
        "RMSE_Gait_subNet_2=rmse\n",
        "PCC_Gait_subNet_2=p\n",
        "\n",
        "Ablation_8=np.hstack([RMSE_Gait_subNet_2,PCC_Gait_subNet_2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpDJxaP4Zqtn"
      },
      "source": [
        "##9. Gait-SubNet-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XnX_gDpZqto"
      },
      "source": [
        "def Gait_SubNet_3(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  model_2=Bidirectional(GRU(128,return_sequences=True))(inputs_1D_N)\n",
        "  model_2=Dropout(0.4)(model_2)\n",
        "  model_2=Bidirectional(GRU(64,return_sequences=True))(model_2)\n",
        "  model_2=Dropout(0.4)(model_2)\n",
        "  model_2=Flatten()(model_2)\n",
        "\n",
        "  X=Conv2D(64, (3, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(128, (3, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "\n",
        "  X=Dense(64, activation='relu')(X)\n",
        "  X=Dropout(0.2)(X)\n",
        "  X=Dense(32,activation='relu')(X)\n",
        "  X=Dropout(0.2)(X)\n",
        "  X_1=Flatten()(X)\n",
        "  X= concatenate([model_2,X_1])\n",
        "\n",
        "  model_3=Bidirectional(GRU(128,return_sequences=True))(inputs_1D_N)\n",
        "  model_3=Dropout(0.4)(model_3)\n",
        "  model_3=Bidirectional(GRU(64,return_sequences=True))(model_3)\n",
        "  model_3=Dropout(0.4)(model_3)\n",
        "  model_3=Flatten()(model_3)\n",
        "\n",
        "  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)\n",
        "  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "\n",
        "  CNN=Dense(64, activation='relu')(CNN)\n",
        "  CNN=Dropout(0.2)(CNN)\n",
        "  CNN=Dense(32, activation='relu')(CNN)\n",
        "  CNN=Dropout(0.2)(CNN)\n",
        "\n",
        "  CNN_1=Flatten()(CNN)\n",
        "\n",
        "\n",
        "  CNN= concatenate([model_3,CNN_1])\n",
        "\n",
        "  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)\n",
        "  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)\n",
        "\n",
        "  output = Average()([output_C2,output_C1])\n",
        "\n",
        "\n",
        "  return (output_C1,output_C2,output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3OwmvdyCG45"
      },
      "outputs": [],
      "source": [
        "#### Main Model ####\n",
        "\n",
        "inputs_1D = tf.keras.layers.Input(shape=(w,12))\n",
        "inputs_2D = tf.keras.layers.Input(shape=(w,6,2))\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "output_1,output_2,output=Gait_SubNet_3(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output])\n",
        "model.compile(loss=correlation_coefficient_loss_RMSE, optimizer='Adam')\n",
        "\n",
        "\n",
        "history=model.fit([np.array(train_X_1D),np.array(train_X_2D)], [train_y_5,train_y_5,train_y_5], epochs=30, batch_size=64, validation_data=([X_validation_1D,\\\n",
        "                                                                      X_validation_2D], [Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)\n",
        "\n",
        "filename = path+'model_Gait_subNet_3.h5'\n",
        "model.save(filename)\n",
        "print('>Saved %s' % filename)\n",
        "\n",
        "# # # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpfUfImCCG46"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5yFMnptCG46"
      },
      "outputs": [],
      "source": [
        "model_path = path+'model_Gait_subNet_3.h5'\n",
        "model=load_model(model_path, custom_objects={'correlation_coefficient_loss_RMSE': correlation_coefficient_loss_RMSE})\n",
        "\n",
        "\n",
        "yhat_1,yhat_2,yhat_4=model.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "### Present ###\n",
        "yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))\n",
        "test_y_r=test_y.reshape((test_y.shape[0]*w,6))\n",
        "\n",
        "print(yhat_4.shape)\n",
        "\n",
        "\n",
        "### Unpack ###\n",
        "yhat_up=unpack_dataset_present(np.array(yhat_5))\n",
        "test_y_up=unpack_dataset_present(np.array(test_y_r))\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)\n",
        "\n",
        "### Present ###\n",
        "\n",
        "yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)\n",
        "test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJX9IYtECG46"
      },
      "outputs": [],
      "source": [
        "### Present ###\n",
        "\n",
        "rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))\n",
        "\n",
        "print(rmse[0])\n",
        "print(rmse[1])\n",
        "print(rmse[2])\n",
        "\n",
        "m=np.mean(rmse)\n",
        "\n",
        "print('\\n')\n",
        "print(m)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(p[0])\n",
        "print(p[1])\n",
        "print(p[2])\n",
        "print('\\n')\n",
        "\n",
        "print(np.mean(p))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eSOYwr1CG46"
      },
      "outputs": [],
      "source": [
        "RMSE_Gait_subNet_3=rmse\n",
        "PCC_Gait_subNet_3=p\n",
        "\n",
        "Ablation_9=np.hstack([RMSE_Gait_subNet_3,PCC_Gait_subNet_3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpfRUIikLxQ4"
      },
      "source": [
        "##10. Gait-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0byrd6rt-VL"
      },
      "source": [
        "def Gait_Net(inputs_1D_N,inputs_2D_N):\n",
        "\n",
        "  model_1=Bidirectional(GRU(128,return_sequences=True))(inputs_1D_N)\n",
        "  model_1=Dropout(0.4)(model_1)\n",
        "  model_1=Bidirectional(GRU(64,return_sequences=True))(model_1)\n",
        "  model_1=Dropout(0.4)(model_1)\n",
        "  model_1=Flatten()(model_1)\n",
        "\n",
        "  model_2=Bidirectional(GRU(128,return_sequences=True))(inputs_1D_N)\n",
        "  model_2=Dropout(0.4)(model_2)\n",
        "  model_2=Bidirectional(GRU(64,return_sequences=True))(model_2)\n",
        "  model_2=Dropout(0.4)(model_2)\n",
        "  model_2=Flatten()(model_2)\n",
        "\n",
        "  X=Conv2D(64, (3, 3), activation='relu',padding='same')(inputs_2D_N)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Conv2D(128, (3, 3), activation='relu', padding='same')(X)\n",
        "  X=BatchNormalization()(X)\n",
        "  X=MaxPooling2D((2, 2))(X)\n",
        "  X=Dense(64, activation='relu')(X)\n",
        "  X=Dropout(0.2)(X)\n",
        "  X=Dense(32,activation='relu')(X)\n",
        "  X=Dropout(0.2)(X)\n",
        "\n",
        "  X_1=Flatten()(X)\n",
        "  X= concatenate([model_2,X_1])\n",
        "\n",
        "\n",
        "  model_3=Bidirectional(GRU(128,return_sequences=True))(inputs_1D_N)\n",
        "  model_3=Dropout(0.4)(model_3)\n",
        "  model_3=Bidirectional(GRU(64,return_sequences=True))(model_3)\n",
        "  model_3=Dropout(0.4)(model_3)\n",
        "  model_3=Flatten()(model_3)\n",
        "\n",
        "  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(inputs_1D_N)\n",
        "  CNN=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=Conv1D(filters=128, kernel_size=3, activation='relu',padding='same')(CNN)\n",
        "  CNN=BatchNormalization()(CNN)\n",
        "  CNN=MaxPooling1D(pool_size=2)(CNN)\n",
        "\n",
        "\n",
        "  CNN=Dense(64, activation='relu')(CNN)\n",
        "  CNN=Dropout(0.2)(CNN)\n",
        "  CNN=Dense(32, activation='relu')(CNN)\n",
        "  CNN=Dropout(0.2)(CNN)\n",
        "\n",
        "  CNN_1=Flatten()(CNN)\n",
        "\n",
        "\n",
        "  CNN= concatenate([model_3,CNN_1])\n",
        "\n",
        "  output_GRU=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(model_1)\n",
        "  output_C2=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(X)\n",
        "  output_C1=Dense(6*w,bias_regularizer=l2(0.001), activation='linear')(CNN)\n",
        "\n",
        "\n",
        "  output = Average()([output_GRU,output_C2,output_C1])\n",
        "\n",
        "  return (output_GRU,output_C2,output_C1,output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1bKl72fJEAu"
      },
      "outputs": [],
      "source": [
        "#### Main Model ####\n",
        "\n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,12) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "output_1,output_2,output_3,output=Gait_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])\n",
        "model.compile(loss=correlation_coefficient_loss_RMSE, optimizer='Adam')\n",
        "\n",
        "\n",
        "history=model.fit([np.array(train_X_1D),np.array(train_X_2D)], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=30, batch_size=64, validation_data=([X_validation_1D,\\\n",
        "                                                                      X_validation_2D], [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)\n",
        "\n",
        "filename = path+'model_Gait_Net.h5'\n",
        "model.save(filename)\n",
        "print('>Saved %s' % filename)\n",
        "\n",
        "# # # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jF3bs7zPYbsl"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwxvOy7Zgu7a"
      },
      "outputs": [],
      "source": [
        "model_path = path+'model_Gait_Net.h5'\n",
        "model=load_model(model_path, custom_objects={'correlation_coefficient_loss_RMSE': correlation_coefficient_loss_RMSE})\n",
        "\n",
        "[yhat_1,yhat_2,yhat_3,yhat_4]=model.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "\n",
        " ### Present ###\n",
        "yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))\n",
        "test_y_r=test_y.reshape((test_y.shape[0]*w,6))\n",
        "\n",
        "print(yhat_4.shape)\n",
        "\n",
        "### Unpack ###\n",
        "yhat_up=unpack_dataset_present(np.array(yhat_5))\n",
        "test_y_up=unpack_dataset_present(np.array(test_y_r))\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)\n",
        "\n",
        "### Present ###\n",
        "\n",
        "yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)\n",
        "test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6GXKuFRWEm0"
      },
      "outputs": [],
      "source": [
        "### Present ###\n",
        "\n",
        "rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))\n",
        "\n",
        "print(rmse[0])\n",
        "print(rmse[1])\n",
        "print(rmse[2])\n",
        "\n",
        "m=np.mean(rmse)\n",
        "\n",
        "print('\\n')\n",
        "print(m)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(p[0])\n",
        "print(p[1])\n",
        "print(p[2])\n",
        "print('\\n')\n",
        "\n",
        "print(np.mean(p))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9IF4ngPCepy"
      },
      "outputs": [],
      "source": [
        "RMSE_Gait_Net=rmse\n",
        "PCC_Gait_Net=p\n",
        "\n",
        "Ablation_10=np.hstack([RMSE_Gait_Net,PCC_Gait_Net])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6XTj55a9gcU"
      },
      "source": [
        "## 11. Gait-Net--Joint Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i98DSfo39gcY"
      },
      "source": [
        "#### Main Model ####\n",
        "\n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,12) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output_1,output_2,output_3,output=Gait_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])\n",
        "model.compile(loss=correlation_coefficient_loss_joint, optimizer='Adam')\n",
        "\n",
        "\n",
        "history=model.fit([np.array(train_X_1D),np.array(train_X_2D)], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=30, batch_size=64, validation_data=([X_validation_1D,\\\n",
        "                                                                      X_validation_2D], [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)\n",
        "\n",
        "filename = path+'model_Gait_JL_Net.h5'\n",
        "model.save(filename)\n",
        "print('>Saved %s' % filename)\n",
        "\n",
        "# # # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j76cjn2i9gcZ"
      },
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGelxx3_9gcZ"
      },
      "source": [
        "model_path = path+'model_GRU_Conv2D.h5'\n",
        "model=load_model(model_path, custom_objects={'correlation_coefficient_loss_RMSE': correlation_coefficient_loss_RMSE})\n",
        "\n",
        "[yhat_1,yhat_2,yhat_3,yhat_4]=model.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "\n",
        " ### Present ###\n",
        "yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))\n",
        "test_y_r=test_y.reshape((test_y.shape[0]*w,6))\n",
        "\n",
        "print(yhat_4.shape)\n",
        "\n",
        "### Unpack ###\n",
        "yhat_up=unpack_dataset_present(np.array(yhat_5),80)\n",
        "test_y_up=unpack_dataset_present(np.array(test_y_r),80)\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)\n",
        "\n",
        "\n",
        "### Present ###\n",
        "\n",
        "yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)\n",
        "test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwdeqdEE9gcZ"
      },
      "source": [
        "### Present ###\n",
        "\n",
        "rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))\n",
        "\n",
        "print(rmse[0])\n",
        "print(rmse[1])\n",
        "print(rmse[2])\n",
        "\n",
        "m=np.mean(rmse)\n",
        "\n",
        "print('\\n')\n",
        "print(m)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(p[0])\n",
        "print(p[1])\n",
        "print(p[2])\n",
        "print('\\n')\n",
        "\n",
        "print(np.mean(p))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDr6w76s9gcZ"
      },
      "source": [
        "RMSE_Gait_JL_Net=rmse\n",
        "PCC_Gait_JL_Net=p\n",
        "\n",
        "Ablation_11=np.hstack([RMSE_Gait_JL_Net,PCC_Gait_JL_Net])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augmentation+ Joint Loss analysis"
      ],
      "metadata": {
        "id": "ZK4fvmjkg7fV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbYWJ3DyLDSK"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgVXeXKsLDSL"
      },
      "outputs": [],
      "source": [
        "def data_loader(subject):\n",
        "  with h5py.File('/content/drive/My Drive/Kinematics Prediction/All_subjects_data_kinetics.h5', 'r') as hf:\n",
        "    All_subjects = hf['All_subjects']\n",
        "    Subject = All_subjects[subject]\n",
        "\n",
        "    HOF=Subject['hof']\n",
        "    IMU_KIN=Subject['IMU_Kin']\n",
        "\n",
        "    treadmill_hof = HOF['Treadmill']\n",
        "    levelground_hof = HOF['Levelground']\n",
        "    slope_hof = HOF['Slope']\n",
        "    stair_hof = HOF['Stair']\n",
        "    round_hof = HOF['Round']\n",
        "    obstacles_hof = HOF['Obstacles']\n",
        "\n",
        "    treadmill_IMU_kin = IMU_KIN['Treadmill']\n",
        "    levelground_IMU_kin = IMU_KIN['Levelground']\n",
        "    slope_IMU_kin = IMU_KIN['Slope']\n",
        "    stair_IMU_kin = IMU_KIN['Stair']\n",
        "    round_IMU_kin= IMU_KIN['Round']\n",
        "    obstacles_IMU_kin = IMU_KIN['Obstacles']\n",
        "\n",
        "\n",
        "    hof_data=np.concatenate((treadmill_hof,levelground_hof,levelground_hof,slope_hof,slope_hof,slope_hof,stair_hof,stair_hof,stair_hof,stair_hof,round_hof,obstacles_hof,round_hof,\\\n",
        "                             obstacles_hof,round_hof,obstacles_hof,round_hof,obstacles_hof),axis=0)\n",
        "    IMU_kin_data=np.concatenate((treadmill_IMU_kin,levelground_IMU_kin,levelground_IMU_kin,slope_IMU_kin,slope_IMU_kin,slope_IMU_kin,stair_IMU_kin,stair_IMU_kin,\\\n",
        "                                 stair_IMU_kin,round_IMU_kin,obstacles_IMU_kin,round_IMU_kin,obstacles_IMU_kin,round_IMU_kin,obstacles_IMU_kin,round_IMU_kin,obstacles_IMU_kin),axis=0)\n",
        "\n",
        "    return np.array(hof_data), np.array(IMU_kin_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5qLE9DXLDSM"
      },
      "outputs": [],
      "source": [
        "subject_1_data_hof_aug, subject_1_data_IMU_Kin_aug=data_loader('Subject_1')\n",
        "gc.collect()\n",
        "subject_2_data_hof_aug, subject_2_data_IMU_Kin_aug=data_loader('Subject_2')\n",
        "gc.collect()\n",
        "subject_3_data_hof_aug, subject_3_data_IMU_Kin_aug=data_loader('Subject_3')\n",
        "gc.collect()\n",
        "subject_4_data_hof_aug, subject_4_data_IMU_Kin_aug=data_loader('Subject_4')\n",
        "gc.collect()\n",
        "subject_5_data_hof_aug, subject_5_data_IMU_Kin_aug=data_loader('Subject_5')\n",
        "gc.collect()\n",
        "subject_6_data_hof_aug, subject_6_data_IMU_Kin_aug=data_loader('Subject_6')\n",
        "gc.collect()\n",
        "subject_7_data_hof_aug, subject_7_data_IMU_Kin_aug=data_loader('Subject_7')\n",
        "gc.collect()\n",
        "subject_8_data_hof_aug, subject_8_data_IMU_Kin_aug=data_loader('Subject_8')\n",
        "gc.collect()\n",
        "subject_9_data_hof_aug, subject_9_data_IMU_Kin_aug=data_loader('Subject_9')\n",
        "gc.collect()\n",
        "subject_10_data_hof_aug, subject_10_data_IMU_Kin_aug=data_loader('Subject_10')\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK3E_9xxLDSN"
      },
      "source": [
        "##Subject Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NdS4h8OLDSN"
      },
      "outputs": [],
      "source": [
        "main_dir = \"/content/drive/My Drive/public dataset/Public_dataset_2/Subject01\"\n",
        "# os.mkdir(main_dir)\n",
        "path=\"/content/\"\n",
        "subject='Subject_01'\n",
        "encoder='lstm'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nSFIN1VLDSO"
      },
      "outputs": [],
      "source": [
        "train_data_hof_aug=np.concatenate((subject_1_data_hof_aug,subject_2_data_hof_aug,subject_4_data_hof_aug,subject_5_data_hof_aug,subject_6_data_hof_aug,\n",
        "                               subject_7_data_hof_aug,subject_8_data_hof_aug,subject_9_data_hof_aug,subject_10_data_hof_aug),axis=0)\n",
        "\n",
        "train_data_IMU_Kin_aug=np.concatenate((subject_1_data_IMU_Kin_aug,subject_2_data_IMU_Kin_aug,subject_4_data_IMU_Kin_aug,subject_5_data_IMU_Kin_aug,subject_6_data_IMU_Kin_aug,\n",
        "                               subject_7_data_IMU_Kin_aug,subject_8_data_IMU_Kin_aug,subject_9_data_IMU_Kin_aug,subject_10_data_IMU_Kin_aug),axis=0)\n",
        "\n",
        "test_data_hof=subject_3_data_hof\n",
        "test_data_IMU_Kin=subject_3_data_IMU_Kin"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing"
      ],
      "metadata": {
        "id": "794r5doCLDSP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AX0qxjgbLDSP"
      },
      "outputs": [],
      "source": [
        "##### IMUs-0:48\n",
        "# Sensor 1- Sternum\n",
        "# Sensor 2-Sacrum\n",
        "# Sensor 3-R_thigh\n",
        "# Sensor 4-L_thigh\n",
        "# Sensor 5-R_shank\n",
        "# Sensor 6-L_shank\n",
        "# Sensor 7-R_dorsal\n",
        "# Sensor 8-L_dorsal\n",
        "\n",
        "train_dataset_IMU=train_data_IMU_Kin_aug[:,36:48]\n",
        "train_dataset_hof=train_data_hof_aug\n",
        "train_dataset_target=np.concatenate((train_data_IMU_Kin_aug[:,55:56],train_data_IMU_Kin_aug[:,58:60],train_data_IMU_Kin_aug[:,62:63],train_data_IMU_Kin_aug[:,65:67]),axis=1)\n",
        "\n",
        "\n",
        "test_dataset_IMU=test_data_IMU_Kin[:,36:48]\n",
        "test_dataset_hof=test_data_hof\n",
        "test_dataset_target=np.concatenate((test_data_IMU_Kin[:,55:56],test_data_IMU_Kin[:,58:60],test_data_IMU_Kin[:,62:63],test_data_IMU_Kin[:,65:67]),axis=1)\n",
        "\n",
        "print(train_dataset_IMU.shape)\n",
        "print(train_dataset_hof.shape)\n",
        "print(train_dataset_target.shape)\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kilx5p9SLDSQ"
      },
      "source": [
        "## Data creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JVp3l-NLDSR"
      },
      "source": [
        "# # convert an array of values into a dataset matrix\n",
        "def create_dataset_present(dataset_1, window=100):\n",
        "  dataX= []\n",
        "  k=0\n",
        "  shift=50\n",
        "  for i in range(int(len(dataset_1)/shift)-1):\n",
        "    j=shift*k\n",
        "    a = dataset_1[j:j+window,:]\n",
        "    dataX.append(a)\n",
        "    k=k+1\n",
        "  return np.array(dataX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXgT28-1LDSR"
      },
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrE26GKQLDSS"
      },
      "source": [
        "### Reconstruction/Present Dataset ###\n",
        "w=100\n",
        "\n",
        "train_X_3=create_dataset_present(train_dataset_IMU,w)\n",
        "train_y_3=create_dataset_present(train_dataset_target,w)\n",
        "\n",
        "test_X_1D=create_dataset_present(test_dataset_IMU,w)\n",
        "test_y=create_dataset_present(test_dataset_target,w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ4n7O9PLDST"
      },
      "source": [
        "train_y_3=train_y_3.reshape(train_y_3.shape[0],w*6)\n",
        "test_y=test_y.reshape(test_y.shape[0],w*6)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu1WUlhWLDST"
      },
      "source": [
        "train_X_1D, X_validation_1D, train_y_5, Y_validation = train_test_split(train_X_3,train_y_3, test_size=0.20, random_state=True)\n",
        "\n",
        "\n",
        "train_X_1D=train_X_1D\n",
        "X_validation_1D=X_validation_1D\n",
        "test_X_1D=test_X_1D\n",
        "\n",
        "print(train_X_1D.shape,train_y_5.shape,X_validation_1D.shape,Y_validation.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebtSzNs0LDSU"
      },
      "source": [
        "features=6\n",
        "\n",
        "train_X_2D=train_X_1D[:,:,0:12].reshape(train_X_1D.shape[0],train_X_1D.shape[1],features,2)\n",
        "test_X_2D=test_X_1D[:,:,0:12].reshape(test_X_1D.shape[0],test_X_1D.shape[1],features,2)\n",
        "X_validation_2D= X_validation_1D[:,:,0:12].reshape(X_validation_1D.shape[0],\n",
        "                                                   X_validation_1D.shape[1],features,2)\n",
        "\n",
        "\n",
        "print(train_X_2D.shape,test_X_2D.shape,X_validation_2D.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3A_iQBTX297"
      },
      "source": [
        "## 12. Gait-Net+Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O6sVEsyX2-A"
      },
      "source": [
        "#### Main Model ####\n",
        "\n",
        "inputs_1D = tf.keras.layers.Input(shape=(w,12))\n",
        "inputs_2D = tf.keras.layers.Input(shape=(w,6,2))\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "output_1,output_2,output_3,output=Gait_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])\n",
        "model.compile(loss=correlation_coefficient_loss_RMSE, optimizer='Adam')\n",
        "\n",
        "\n",
        "history=model.fit([np.array(train_X_1D),np.array(train_X_2D)], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=30, batch_size=64, validation_data=([X_validation_1D,\\\n",
        "                                                                      X_validation_2D], [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)\n",
        "\n",
        "filename = path+'model_Gait_A_Net.h5'\n",
        "model.save(filename)\n",
        "print('>Saved %s' % filename)\n",
        "\n",
        "# # # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc_qxQTGX2-A"
      },
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJPaCQdrX2-B"
      },
      "source": [
        "model_path = path+'model_Gait_A_Net.h5'\n",
        "model=load_model(model_path, custom_objects={'correlation_coefficient_loss_RMSE': correlation_coefficient_loss_RMSE})\n",
        "\n",
        "[yhat_1,yhat_2,yhat_3,yhat_4]=model.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "\n",
        " ### Present ###\n",
        "yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))\n",
        "test_y_r=test_y.reshape((test_y.shape[0]*w,6))\n",
        "\n",
        "print(yhat_4.shape)\n",
        "### Future ###\n",
        "\n",
        "### Unpack ###\n",
        "yhat_up=unpack_dataset_present(np.array(yhat_5))\n",
        "test_y_up=unpack_dataset_present(np.array(test_y_r))\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)\n",
        "\n",
        "\n",
        "### Present ###\n",
        "\n",
        "yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)\n",
        "test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLNZoXpfX2-B"
      },
      "source": [
        "### Present ###\n",
        "\n",
        "rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))\n",
        "\n",
        "print(rmse[0])\n",
        "print(rmse[1])\n",
        "print(rmse[2])\n",
        "\n",
        "m=np.mean(rmse)\n",
        "\n",
        "print('\\n')\n",
        "print(m)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(p[0])\n",
        "print(p[1])\n",
        "print(p[2])\n",
        "print('\\n')\n",
        "\n",
        "print(np.mean(p))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFVIiyG_X2-B"
      },
      "source": [
        "RMSE_Gait_A_Net=rmse\n",
        "PCC_Gait_A_Net=p\n",
        "\n",
        "Ablation_12=np.hstack([RMSE_Gait_A_Net,PCC_Gait_A_Net])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQARpaZR-g-k"
      },
      "source": [
        "## 13. Gait-JL-Net+Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjqAUfwI-ef0"
      },
      "source": [
        "#### Main Model ####\n",
        "\n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,12) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output_1,output_2,output_3,output=Gait_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])\n",
        "model.compile(loss=correlation_coefficient_loss_joint, optimizer='Adam')\n",
        "\n",
        "\n",
        "history=model.fit([np.array(train_X_1D),np.array(train_X_2D)], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=30, batch_size=64, validation_data=([X_validation_1D,\\\n",
        "                                                                      X_validation_2D], [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)\n",
        "\n",
        "filename = path+'model_Gait_JL_A_Net.h5'\n",
        "model.save(filename)\n",
        "print('>Saved %s' % filename)\n",
        "\n",
        "# # # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCbYdJfi-ef1"
      },
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dArhLdf-ef1"
      },
      "source": [
        "model_path = path+'model_Gait_JL_A_Net.h5'\n",
        "model=load_model(model_path, custom_objects={'correlation_coefficient_loss_joint': correlation_coefficient_loss_joint})\n",
        "\n",
        "[yhat_1,yhat_2,yhat_3,yhat_4]=model.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "\n",
        " ### Present ###\n",
        "yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))\n",
        "test_y_r=test_y.reshape((test_y.shape[0]*w,6))\n",
        "\n",
        "print(yhat_4.shape)\n",
        "\n",
        "### Unpack ###\n",
        "yhat_up=unpack_dataset_present(np.array(yhat_5))\n",
        "test_y_up=unpack_dataset_present(np.array(test_y_r))\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)\n",
        "\n",
        "\n",
        "### Present ###\n",
        "\n",
        "yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)\n",
        "test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB_QqO6y-ef1"
      },
      "source": [
        "### Present ###\n",
        "\n",
        "rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))\n",
        "\n",
        "print(rmse[0])\n",
        "print(rmse[1])\n",
        "print(rmse[2])\n",
        "\n",
        "m=np.mean(rmse)\n",
        "\n",
        "print('\\n')\n",
        "print(m)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(p[0])\n",
        "print(p[1])\n",
        "print(p[2])\n",
        "print('\\n')\n",
        "\n",
        "print(np.mean(p))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meIYLUM2-ef1"
      },
      "source": [
        "RMSE_Gait_JL_A_Net=rmse\n",
        "PCC_Gait_JL_A_Net=p\n",
        "\n",
        "Ablation_13=np.hstack([RMSE_Gait_JL_A_Net,PCC_Gait_JL_A_Net])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTUsqxDmYdkg"
      },
      "source": [
        "## Loss function with respect to weight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKgXjH4LOJTB"
      },
      "source": [
        "def correlation_coefficient_loss_joint_2(y_true, y_pred):\n",
        "    # Calculate mean values\n",
        "    mx = tf.reduce_mean(y_true)\n",
        "    my = tf.reduce_mean(y_pred)\n",
        "\n",
        "    # Calculate differences from mean\n",
        "    xm, ym = y_true - mx, y_pred - my\n",
        "\n",
        "    # Calculate numerator and denominator of Pearson correlation coefficient\n",
        "    r_num = tf.reduce_sum(tf.multiply(xm, ym))\n",
        "    r_den = tf.sqrt(tf.multiply(tf.reduce_sum(tf.square(xm)), tf.reduce_sum(tf.square(ym))))\n",
        "\n",
        "    # Calculate Pearson correlation coefficient\n",
        "    r = r_num / r_den\n",
        "\n",
        "    # Clamp r between -1 and 1\n",
        "    r = tf.maximum(tf.minimum(r, 1.0), -1.0)\n",
        "\n",
        "    # Calculate l2 loss\n",
        "    l2 = 1 - tf.square(r)\n",
        "\n",
        "    l1=K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
        "\n",
        "\n",
        "    return l1+2*l2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVqqzu3IOQ6E"
      },
      "source": [
        "def correlation_coefficient_loss_joint_4(y_true, y_pred):\n",
        "    # Calculate mean values\n",
        "    mx = tf.reduce_mean(y_true)\n",
        "    my = tf.reduce_mean(y_pred)\n",
        "\n",
        "    # Calculate differences from mean\n",
        "    xm, ym = y_true - mx, y_pred - my\n",
        "\n",
        "    # Calculate numerator and denominator of Pearson correlation coefficient\n",
        "    r_num = tf.reduce_sum(tf.multiply(xm, ym))\n",
        "    r_den = tf.sqrt(tf.multiply(tf.reduce_sum(tf.square(xm)), tf.reduce_sum(tf.square(ym))))\n",
        "\n",
        "    # Calculate Pearson correlation coefficient\n",
        "    r = r_num / r_den\n",
        "\n",
        "    # Clamp r between -1 and 1\n",
        "    r = tf.maximum(tf.minimum(r, 1.0), -1.0)\n",
        "\n",
        "    # Calculate l2 loss\n",
        "    l2 = 1 - tf.square(r)\n",
        "\n",
        "    l1=K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
        "\n",
        "\n",
        "    return l1+4*l2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O1dQjTqOMG8"
      },
      "source": [
        "def correlation_coefficient_loss_joint_6(y_true, y_pred):\n",
        "    # Calculate mean values\n",
        "    mx = tf.reduce_mean(y_true)\n",
        "    my = tf.reduce_mean(y_pred)\n",
        "\n",
        "    # Calculate differences from mean\n",
        "    xm, ym = y_true - mx, y_pred - my\n",
        "\n",
        "    # Calculate numerator and denominator of Pearson correlation coefficient\n",
        "    r_num = tf.reduce_sum(tf.multiply(xm, ym))\n",
        "    r_den = tf.sqrt(tf.multiply(tf.reduce_sum(tf.square(xm)), tf.reduce_sum(tf.square(ym))))\n",
        "\n",
        "    # Calculate Pearson correlation coefficient\n",
        "    r = r_num / r_den\n",
        "\n",
        "    # Clamp r between -1 and 1\n",
        "    r = tf.maximum(tf.minimum(r, 1.0), -1.0)\n",
        "\n",
        "    # Calculate l2 loss\n",
        "    l2 = 1 - tf.square(r)\n",
        "\n",
        "    l1=K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
        "\n",
        "\n",
        "    return l1+6*l2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFJvh6r_OPRl"
      },
      "source": [
        "def correlation_coefficient_loss_joint_8(y_true, y_pred):\n",
        "    # Calculate mean values\n",
        "    mx = tf.reduce_mean(y_true)\n",
        "    my = tf.reduce_mean(y_pred)\n",
        "\n",
        "    # Calculate differences from mean\n",
        "    xm, ym = y_true - mx, y_pred - my\n",
        "\n",
        "    # Calculate numerator and denominator of Pearson correlation coefficient\n",
        "    r_num = tf.reduce_sum(tf.multiply(xm, ym))\n",
        "    r_den = tf.sqrt(tf.multiply(tf.reduce_sum(tf.square(xm)), tf.reduce_sum(tf.square(ym))))\n",
        "\n",
        "    # Calculate Pearson correlation coefficient\n",
        "    r = r_num / r_den\n",
        "\n",
        "    # Clamp r between -1 and 1\n",
        "    r = tf.maximum(tf.minimum(r, 1.0), -1.0)\n",
        "\n",
        "    # Calculate l2 loss\n",
        "    l2 = 1 - tf.square(r)\n",
        "\n",
        "    l1=K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
        "\n",
        "\n",
        "    return l1+8*l2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcLdlDgNOMfm"
      },
      "source": [
        "def correlation_coefficient_loss_joint_10(y_true, y_pred):\n",
        "    # Calculate mean values\n",
        "    mx = tf.reduce_mean(y_true)\n",
        "    my = tf.reduce_mean(y_pred)\n",
        "\n",
        "    # Calculate differences from mean\n",
        "    xm, ym = y_true - mx, y_pred - my\n",
        "\n",
        "    # Calculate numerator and denominator of Pearson correlation coefficient\n",
        "    r_num = tf.reduce_sum(tf.multiply(xm, ym))\n",
        "    r_den = tf.sqrt(tf.multiply(tf.reduce_sum(tf.square(xm)), tf.reduce_sum(tf.square(ym))))\n",
        "\n",
        "    # Calculate Pearson correlation coefficient\n",
        "    r = r_num / r_den\n",
        "\n",
        "    # Clamp r between -1 and 1\n",
        "    r = tf.maximum(tf.minimum(r, 1.0), -1.0)\n",
        "\n",
        "    # Calculate l2 loss\n",
        "    l2 = 1 - tf.square(r)\n",
        "\n",
        "    l1=K.sqrt(K.mean(K.square(y_true - y_pred)))\n",
        "\n",
        "\n",
        "    return l1+10*l2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3DO5r8ZY-qd"
      },
      "source": [
        "## 14. Gait-JL-Net_2+Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK9jPLkoY-qd"
      },
      "source": [
        "#### Main Model ####\n",
        "\n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,12) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output_1,output_2,output_3,output=Gait_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])\n",
        "model.compile(loss=correlation_coefficient_loss_joint_2, optimizer='Adam')\n",
        "\n",
        "\n",
        "history=model.fit([np.array(train_X_1D),np.array(train_X_2D)], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=30, batch_size=64, validation_data=([X_validation_1D,\\\n",
        "                                                                      X_validation_2D], [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)\n",
        "\n",
        "filename = path+'model_Gait_JL_A_Net_2.h5'\n",
        "model.save(filename)\n",
        "print('>Saved %s' % filename)\n",
        "\n",
        "# # # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LT2gsxTCY-qe"
      },
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP9L_WiPY-qe"
      },
      "source": [
        "model_path = path+'model_Gait_JL_A_Net_2.h5'\n",
        "model=load_model(model_path, custom_objects={'correlation_coefficient_loss_joint_2': correlation_coefficient_loss_joint_2})\n",
        "\n",
        "[yhat_1,yhat_2,yhat_3,yhat_4]=model.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "\n",
        "\n",
        " ### Present ###\n",
        "yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))\n",
        "test_y_r=test_y.reshape((test_y.shape[0]*w,6))\n",
        "\n",
        "print(yhat_4.shape)\n",
        "\n",
        "### Unpack ###\n",
        "yhat_up=unpack_dataset_present(np.array(yhat_5))\n",
        "test_y_up=unpack_dataset_present(np.array(test_y_r))\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)\n",
        "\n",
        "\n",
        "### Present ###\n",
        "\n",
        "yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)\n",
        "test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzEkuIzKY-qe"
      },
      "source": [
        "### Present ###\n",
        "\n",
        "rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))\n",
        "\n",
        "print(rmse[0])\n",
        "print(rmse[1])\n",
        "print(rmse[2])\n",
        "\n",
        "m=np.mean(rmse)\n",
        "\n",
        "print('\\n')\n",
        "print(m)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(p[0])\n",
        "print(p[1])\n",
        "print(p[2])\n",
        "print('\\n')\n",
        "\n",
        "print(np.mean(p))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drNjPLWSY-qe"
      },
      "source": [
        "RMSE_Gait_JL_A_Net_2=rmse\n",
        "PCC_Gait_JL_A_Net_2=p\n",
        "\n",
        "Ablation_14=np.hstack([RMSE_Gait_JL_A_Net_2,PCC_Gait_JL_A_Net_2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VvB8kwrZWzO"
      },
      "source": [
        "## 15. Gait-JL-Net_4+Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuX5yTzQZWzO"
      },
      "source": [
        "#### Main Model ####\n",
        "\n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,12) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output_1,output_2,output_3,output=Gait_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])\n",
        "model.compile(loss=correlation_coefficient_loss_joint_4, optimizer='Adam')\n",
        "\n",
        "history=model.fit([np.array(train_X_1D),np.array(train_X_2D)], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=30, batch_size=64, validation_data=([X_validation_1D,\\\n",
        "                                                                      X_validation_2D], [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)\n",
        "\n",
        "filename = path+'model_Gait_JL_A_Net_4.h5'\n",
        "model.save(filename)\n",
        "print('>Saved %s' % filename)\n",
        "\n",
        "# # # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8okqWI6TZWzO"
      },
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9mRosVfZWzO"
      },
      "source": [
        "model_path = path+'model_Gait_JL_A_Net_4.h5'\n",
        "model=load_model(model_path, custom_objects={'correlation_coefficient_loss_joint_4': correlation_coefficient_loss_joint_4})\n",
        "\n",
        "[yhat_1,yhat_2,yhat_3,yhat_4]=model.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "\n",
        " ### Present ###\n",
        "yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))\n",
        "test_y_r=test_y.reshape((test_y.shape[0]*w,6))\n",
        "\n",
        "print(yhat_4.shape)\n",
        "\n",
        "### Unpack ###\n",
        "yhat_up=unpack_dataset_present(np.array(yhat_5))\n",
        "test_y_up=unpack_dataset_present(np.array(test_y_r))\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)\n",
        "\n",
        "\n",
        "### Present ###\n",
        "\n",
        "yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)\n",
        "test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cz9n9q6ZWzP"
      },
      "source": [
        "### Present ###\n",
        "\n",
        "rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))\n",
        "\n",
        "print(rmse[0])\n",
        "print(rmse[1])\n",
        "print(rmse[2])\n",
        "\n",
        "m=np.mean(rmse)\n",
        "\n",
        "print('\\n')\n",
        "print(m)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(p[0])\n",
        "print(p[1])\n",
        "print(p[2])\n",
        "print('\\n')\n",
        "\n",
        "print(np.mean(p))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlYhY32lZWzP"
      },
      "source": [
        "RMSE_Gait_JL_A_Net_4=rmse\n",
        "PCC_Gait_JL_A_Net_4=p\n",
        "\n",
        "Ablation_15=np.hstack([RMSE_Gait_JL_A_Net_4,PCC_Gait_JL_A_Net_4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oAUHymMZgob"
      },
      "source": [
        "## 16. Gait-JL-Net_6+Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9YADRp4Zgob"
      },
      "source": [
        "#### Main Model ####\n",
        "\n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,12) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output_1,output_2,output_3,output=Gait_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])\n",
        "model.compile(loss=correlation_coefficient_loss_joint_6, optimizer='Adam')\n",
        "\n",
        "\n",
        "history=model.fit([np.array(train_X_1D),np.array(train_X_2D)], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=30, batch_size=64, validation_data=([X_validation_1D,\\\n",
        "                                                                      X_validation_2D], [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)\n",
        "\n",
        "filename = path+'model_Gait_JL_A_Net_6.h5'\n",
        "model.save(filename)\n",
        "print('>Saved %s' % filename)\n",
        "\n",
        "# # # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY_iSJt9Zgob"
      },
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgT17N-hZgoc"
      },
      "source": [
        "model_path = path+'model_Gait_JL_A_Net_6.h5'\n",
        "model=load_model(model_path, custom_objects={'correlation_coefficient_loss_joint_6': correlation_coefficient_loss_joint_6})\n",
        "\n",
        "[yhat_1,yhat_2,yhat_3,yhat_4]=model.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "\n",
        " ### Present ###\n",
        "yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))\n",
        "test_y_r=test_y.reshape((test_y.shape[0]*w,6))\n",
        "\n",
        "print(yhat_4.shape)\n",
        "\n",
        "### Unpack ###\n",
        "yhat_up=unpack_dataset_present(np.array(yhat_5))\n",
        "test_y_up=unpack_dataset_present(np.array(test_y_r))\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)\n",
        "\n",
        "### Present ###\n",
        "\n",
        "yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)\n",
        "test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VUnpDPuZgoc"
      },
      "source": [
        "### Present ###\n",
        "\n",
        "rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))\n",
        "\n",
        "print(rmse[0])\n",
        "print(rmse[1])\n",
        "print(rmse[2])\n",
        "\n",
        "m=np.mean(rmse)\n",
        "\n",
        "print('\\n')\n",
        "print(m)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(p[0])\n",
        "print(p[1])\n",
        "print(p[2])\n",
        "print('\\n')\n",
        "\n",
        "print(np.mean(p))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADSxBSGZZgoc"
      },
      "source": [
        "RMSE_Gait_JL_A_Net_6=rmse\n",
        "PCC_Gait_JL_A_Net_6=p\n",
        "\n",
        "Ablation_16=np.hstack([RMSE_Gait_JL_A_Net_6,PCC_Gait_JL_A_Net_6])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoHyJTRaaC2F"
      },
      "source": [
        "## 17. Gait-JL-Net_8+Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R9LTibyaC2F"
      },
      "source": [
        "#### Main Model ####\n",
        "\n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,12) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output_1,output_2,output_3,output=Gait_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])\n",
        "model.compile(loss=correlation_coefficient_loss_joint_8, optimizer='Adam')\n",
        "\n",
        "\n",
        "history=model.fit([np.array(train_X_1D),np.array(train_X_2D)], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=30, batch_size=64, validation_data=([X_validation_1D,\\\n",
        "                                                                      X_validation_2D], [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)\n",
        "\n",
        "filename = path+'model_Gait_JL_A_Net_8.h5'\n",
        "model.save(filename)\n",
        "print('>Saved %s' % filename)\n",
        "\n",
        "# # # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSh6oAmVaC2F"
      },
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amrwxrYFaC2G"
      },
      "source": [
        "model_path = path+'model_Gait_JL_A_Net_8.h5'\n",
        "model=load_model(model_path, custom_objects={'correlation_coefficient_loss_joint_8': correlation_coefficient_loss_joint_8})\n",
        "\n",
        "[yhat_1,yhat_2,yhat_3,yhat_4]=model.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "\n",
        " ### Present ###\n",
        "yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))\n",
        "test_y_r=test_y.reshape((test_y.shape[0]*w,6))\n",
        "\n",
        "print(yhat_4.shape)\n",
        "\n",
        "### Unpack ###\n",
        "yhat_up=unpack_dataset_present(np.array(yhat_5))\n",
        "test_y_up=unpack_dataset_present(np.array(test_y_r))\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)\n",
        "\n",
        "### Present ###\n",
        "\n",
        "yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)\n",
        "test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eGLFQngaC2G"
      },
      "source": [
        "### Present ###\n",
        "\n",
        "rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))\n",
        "\n",
        "print(rmse[0])\n",
        "print(rmse[1])\n",
        "print(rmse[2])\n",
        "\n",
        "m=np.mean(rmse)\n",
        "\n",
        "print('\\n')\n",
        "print(m)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(p[0])\n",
        "print(p[1])\n",
        "print(p[2])\n",
        "print('\\n')\n",
        "\n",
        "print(np.mean(p))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02W2smx5aC2G"
      },
      "source": [
        "RMSE_Gait_JL_A_Net_8=rmse\n",
        "PCC_Gait_JL_A_Net_8=p\n",
        "\n",
        "Ablation_17=np.hstack([RMSE_Gait_JL_A_Net_8,PCC_Gait_JL_A_Net_8])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGOEXFkLaYsc"
      },
      "source": [
        "## 18. Gait-JL-Net_10+Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80nBZNNPaYsd"
      },
      "source": [
        "#### Main Model ####\n",
        "\n",
        "inputs_1D = tf.keras.layers.Input( shape=(w,12) )\n",
        "inputs_2D = tf.keras.layers.Input( shape=(w,6,2) )\n",
        "\n",
        "\n",
        "inputs_1D_N=BatchNormalization()(inputs_1D)\n",
        "inputs_2D_N=BatchNormalization()(inputs_2D)\n",
        "\n",
        "\n",
        "output_1,output_2,output_3,output=Gait_Net(inputs_1D_N,inputs_2D_N)\n",
        "\n",
        "model = Model(inputs=[inputs_1D, inputs_2D], outputs=[output_1,output_2,output_3,output])\n",
        "model.compile(loss=correlation_coefficient_loss_joint_10, optimizer='Adam')\n",
        "\n",
        "\n",
        "history=model.fit([np.array(train_X_1D),np.array(train_X_2D)], [train_y_5,train_y_5,train_y_5,train_y_5], epochs=30, batch_size=64, validation_data=([X_validation_1D,\\\n",
        "                                                                      X_validation_2D], [Y_validation,Y_validation,Y_validation,Y_validation]), verbose=2, shuffle=False)\n",
        "\n",
        "filename = path+'model_Gait_JL_A_Net_10.h5'\n",
        "model.save(filename)\n",
        "print('>Saved %s' % filename)\n",
        "\n",
        "# # # summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper right')\n",
        "plt.show()\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgeoqqK_aYsd"
      },
      "source": [
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3TzXbv3aYsd"
      },
      "source": [
        "model_path = path+'model_Gait_JL_A_Net_10.h5'\n",
        "model=load_model(model_path, custom_objects={'correlation_coefficient_loss_joint_10': correlation_coefficient_loss_joint_10})\n",
        "\n",
        "[yhat_1,yhat_2,yhat_3,yhat_4]=model.predict([test_X_1D,test_X_2D])\n",
        "\n",
        "\n",
        " ### Present ###\n",
        "yhat_5=yhat_4.reshape((yhat_4.shape[0]*w,6))\n",
        "test_y_r=test_y.reshape((test_y.shape[0]*w,6))\n",
        "\n",
        "print(yhat_4.shape)\n",
        "\n",
        "\n",
        "### Unpack ###\n",
        "yhat_up=unpack_dataset_present(np.array(yhat_5))\n",
        "test_y_up=unpack_dataset_present(np.array(test_y_r))\n",
        "\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)\n",
        "\n",
        "### Present ###\n",
        "\n",
        "yhat_up=yhat_up.reshape(int(len(yhat_up)/6),6)\n",
        "test_y_up=test_y_up.reshape(int(len(test_y_up)/6),6)\n",
        "\n",
        "print(yhat_up.shape,test_y_up.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6bzyXLdaYsd"
      },
      "source": [
        "### Present ###\n",
        "\n",
        "rmse,p= prediction_test(np.array(yhat_up),np.array(test_y_up))\n",
        "\n",
        "print(rmse[0])\n",
        "print(rmse[1])\n",
        "print(rmse[2])\n",
        "\n",
        "m=np.mean(rmse)\n",
        "\n",
        "print('\\n')\n",
        "print(m)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(p[0])\n",
        "print(p[1])\n",
        "print(p[2])\n",
        "print('\\n')\n",
        "\n",
        "print(np.mean(p))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hLVMtg6aYsd"
      },
      "source": [
        "RMSE_Gait_JL_A_Net_10=rmse\n",
        "PCC_Gait_JL_A_Net_10=p\n",
        "\n",
        "Ablation_18=np.hstack([RMSE_Gait_JL_A_Net_10,PCC_Gait_JL_A_Net_10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM2Lwa-vY9Fs"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDrtuwba-u1O"
      },
      "source": [
        "all_results=np.vstack([Ablation_1,Ablation_2,Ablation_3,Ablation_4,Ablation_5,Ablation_6,Ablation_7,Ablation_8,Ablation_9,Ablation_10,Ablation_11,Ablation_12,Ablation_13,Ablation_14,Ablation_15,Ablation_16,Ablation_17,Ablation_18])\n",
        "from numpy import savetxt\n",
        "\n",
        "savetxt('/content/drive/My Drive/Conference paper/subject 2/subject_2_results.csv', all_results, delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}